* crash
*** 进程crash时会发送signal（SIGSEGV，SIGART etc），提前注册sigaction处理目标signal，然后clone进程，使用ptrace syscall获取进程mem reg信息，同时获取procfs信息得到thread stacktrace
*** breakpad   ---> crashDump --> addr2line
*** fork使用fork syscall实现, pthread_create都使用clone syscall实现，可以指定共享哪些数据区域
*** thread specific stack is on heap(mmap) with guard page of guardsize protected by mprotect
*** core dump file
**** with systemd systems cat /proc/sys/kernel/core_pattern -> |/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e
**** core dump files stored in directory /var/lib/systemd/coredump
**** core dump file managed by coredumpctl
**** list all core dump files: coredumpctl list | tail -5
**** extract core dump file: coredumpctl dump ${pid} -o core
* Finalizer
*** object.finalize() run in FinalizerDaemon thread
*** FinalizerWatchdogDaemon thread 监控FinalizerDaemon thread的执行，若FinalizerDaemon执行某个object的finalize方法超过10s，
则发出SIGQUIT signal
*** finalize timeout 可以通过反射停止FinalizerWatchdogDaemon，Android P限制反射调用失效

* generate code at runtime
*** u1 code[PAGE_SIZE]; mprotect(code, PAGE_SIZE, PROT_READ | PROT_WRITE | PROT_EXEC); memcpy(code, "\xC3", 1);
* RecyclerView
*** Recycler.tryGetViewHolderForPositionByDeadline缓存逻辑:
    # 根据position查找mAttachedScrap-->mHiddenViews-->mCachedViews，
    # 根据type查找mAttachedScrap-->mCachedViews
    # if mViewCacheExtension != null, 通过mViewCacheExtension查找
    # mRecyclerPool.mScrap中查找
    # mAdapter.createViewHolder

* android gradle plugin
*** 方法的最后一个参数是闭包时可以将{}移到()外，同时()可以省略，所以常见的android {...}
*** android包含属性：compileSdkVersion、buildToolsVersion、defaultConfig、buildTypes、signingConfigs、lintOptions、
compileOptions etc
*** defaultConfig是默认的productFlavor类型，包含applicationId、minSdkVersion、versionCode、versionName、
testInstrumentationRunner、ndk etc
*** buildType属性：applicationSuffix、debuggable、jniDebuggable、minifyEnabled、multiDexEnabled、proguardFile、shrinkResources、
signingConfig、zipAlignEnabled
*** variant 是productFlavors和buildTypes组合产物，variant有applicationVariants、libraryVariants、testVariants
*** 使用exec可以执行命令行操作
*** 可以通过环境变量System.getenv()获取签名信息，从而隐藏签名信息
*** manifestPlaceHolders buildConfigField resValue
* Java Thread start
*** State
    #+begin_src java
        public enum State {
        /**
         * Thread state for a thread which has not yet started.
         */
        NEW,

        /**
         * Thread state for a runnable thread.  A thread in the runnable
         * state is executing in the Java virtual machine but it may
         * be waiting for other resources from the operating system
         * such as processor.
         */
        RUNNABLE,

        /**
         * Thread state for a thread blocked waiting for a monitor lock.
         * A thread in the blocked state is waiting for a monitor lock
         * to enter a synchronized block/method or
         * reenter a synchronized block/method after calling
         * {@link Object#wait() Object.wait}.
         */
        BLOCKED,

        /**
         * Thread state for a waiting thread.
         * A thread is in the waiting state due to calling one of the
         * following methods:
         * <ul>
         *   <li>{@link Object#wait() Object.wait} with no timeout</li>
         *   <li>{@link #join() Thread.join} with no timeout</li>
         *   <li>{@link LockSupport#park() LockSupport.park}</li>
         * </ul>
         *
         * <p>A thread in the waiting state is waiting for another thread to
         * perform a particular action.
         *
         * For example, a thread that has called <tt>Object.wait()</tt>
         * on an object is waiting for another thread to call
         * <tt>Object.notify()</tt> or <tt>Object.notifyAll()</tt> on
         * that object. A thread that has called <tt>Thread.join()</tt>
         * is waiting for a specified thread to terminate.
         */
        WAITING,

        /**
         * Thread state for a waiting thread with a specified waiting time.
         * A thread is in the timed waiting state due to calling one of
         * the following methods with a specified positive waiting time:
         * <ul>
         *   <li>{@link #sleep Thread.sleep}</li>
         *   <li>{@link Object#wait(long) Object.wait} with timeout</li>
         *   <li>{@link #join(long) Thread.join} with timeout</li>
         *   <li>{@link LockSupport#parkNanos LockSupport.parkNanos}</li>
         *   <li>{@link LockSupport#parkUntil LockSupport.parkUntil}</li>
         * </ul>
         */
        TIMED_WAITING,

        /**
         * Thread state for a terminated thread.
         * The thread has completed execution.
         */
        TERMINATED;
    }
    #+end_src

*** android-source/art/runtime/thread_state.h
enum ThreadState {
  //                                   Thread.State   JDWP state
  kTerminated = 66,                 // TERMINATED     TS_ZOMBIE    Thread.run has returned, but Thread* still around
  kRunnable,                        // RUNNABLE       TS_RUNNING   runnable
  kTimedWaiting,                    // TIMED_WAITING  TS_WAIT      in Object.wait() with a timeout
  kSleeping,                        // TIMED_WAITING  TS_SLEEPING  in Thread.sleep()
  kBlocked,                         // BLOCKED        TS_MONITOR   blocked on a monitor
  kWaiting,                         // WAITING        TS_WAIT      in Object.wait()
  kWaitingForLockInflation,         // WAITING        TS_WAIT      blocked inflating a thin-lock
  kWaitingForTaskProcessor,         // WAITING        TS_WAIT      blocked waiting for taskProcessor
  kWaitingForGcToComplete,          // WAITING        TS_WAIT      blocked waiting for GC
  kWaitingForCheckPointsToRun,      // WAITING        TS_WAIT      GC waiting for checkpoints to run
  kWaitingPerformingGc,             // WAITING        TS_WAIT      performing GC
  kWaitingForDebuggerSend,          // WAITING        TS_WAIT      blocked waiting for events to be sent
  kWaitingForDebuggerToAttach,      // WAITING        TS_WAIT      blocked waiting for debugger to attach
  kWaitingInMainDebuggerLoop,       // WAITING        TS_WAIT      blocking/reading/processing debugger events
  kWaitingForDebuggerSuspension,    // WAITING        TS_WAIT      waiting for debugger suspend all
  kWaitingForJniOnLoad,             // WAITING        TS_WAIT      waiting for execution of dlopen and JNI on load code
  kWaitingForSignalCatcherOutput,   // WAITING        TS_WAIT      waiting for signal catcher IO to complete
  kWaitingInMainSignalCatcherLoop,  // WAITING        TS_WAIT      blocking/reading/processing signals
  kWaitingForDeoptimization,        // WAITING        TS_WAIT      waiting for deoptimization suspend all
  kWaitingForMethodTracingStart,    // WAITING        TS_WAIT      waiting for method tracing to start
  kWaitingForVisitObjects,          // WAITING        TS_WAIT      waiting for visiting objects
  kWaitingForGetObjectsAllocated,   // WAITING        TS_WAIT      waiting for getting the number of allocated objects
  kWaitingWeakGcRootRead,           // WAITING        TS_WAIT      waiting on the GC to read a weak root
  kWaitingForGcThreadFlip,          // WAITING        TS_WAIT      waiting on the GC thread flip (CC collector) to finish
  kStarting,                        // NEW            TS_WAIT      native thread started, not yet ready to run managed code
  kNative,                          // RUNNABLE       TS_RUNNING   running in a JNI native method
  kSuspended,                       // RUNNABLE       TS_RUNNING   suspended by GC or debugger
};
*** android-source/art/runtime/native/java_lang_Thread.cc
Thread.start()-->nativeCreate()-->Thread.CreateNativeThread()-->JNIEnvExt::Creat()-->pthread_create()-->child_thread invoke java Thread.run() method
*** JNIEnv implemented in android-source/art/runtime/jni_internal.cc
* x86_64
*** 函数传参寄存器顺序rdi, rsi, rdx, rcx, r8, r9，超过6个则压栈
*** callq会push %rip(return address aka.下一条指令)
*** %rax保存返回值
* gdb
*** p foo print foo
*** set foo = 123 set variable foo = 123
*** $sp $pc $fp 别名适用所有平台
*** x/i $pc 当前指令
*** x/32x $sp 显示stack内存信息
*** x/32x addr 显示addr开始的32个dword
*** objdump -dS elf显示汇编
*** disas/m 反汇编code
* lag analyze tool
*** TraceView
  - Debug.startMethodTracing("sample") Debug.startMethodTracingSampling() Debug.stopMethodTracing()
*** Systrace
  - ./systrace.py sched freq idle am wm gfx view sync binder_driver irq workq input -b 96000
  - java framework: Trace.traceBegin(long traceTag, String methodName) Trace.traceEnd(long traceTag)
  - app: Trace.beginSection(String sectionName) Trace.endSection()
  - native: ATRACE_CALL();
*** 获取GC统计信息
  - // GC 使用的总耗时，单位是毫秒
    Debug.getRuntimeStat("art.gc.gc-time");
    // 阻塞式 GC 的总耗时
    Debug.getRuntimeStat("art.gc.blocking-gc-time");

* C++ mangle/demangle tool
*** c++filt -n _ZN7android6Tracer12sEnabledTagsE
* Hook
** inline hook
*** Substrate
**** MSHookFunction(void *symbol, void *replace, void **result)
 - 作用：symbol：原函数地址，replace：hook函数地址，result：返回动态生成的代替原函数的指针，用于在hook函数中调用原来的逻辑
 - 替换symbol的前几个指令，将其跳转到replace的首地址，replace中调用*result(mmap新的buffer，保存old function头部被替换的字节，尾部跳转到原函数未被替换字节的首地址)
** PLT/GOT hook
*** PLT(procedure linkage table) GOT(global offset table)
*** example:
callq <printf@PLT> 查找GOT中相应记录，若没有加载printf地址，加载so库，修改GOT中printf记录地址为真实地址，后续调用直接调用GOT中真实地址
*** dl_iterate_phdr
walk through list of shared objects
*** facebook profilo iqiyi xHook
* vcpkg
*** visual stduio管理第三方libs
* unit test
*** 安全的重构代码
*** cmake && google test
*** gtest
**** assertions
| assertions | fatal | intercept |
|------------+-------+-----------|
| ASSERT_*   | YES   | YES       |
| EXPECT_*   | NO    | NO        |
**** fixtures
***** Using the Same Data Configuration for Multiple Tests
#+begin_src cpp
  // class Queue wanted to be tested
  template <typename E>  // E is the element type.
  class Queue {
   public:
    Queue();
    void Enqueue(const E& element);
    E* Dequeue();  // Returns NULL if the queue is empty.
    size_t size() const;
    ...
  };

  // test case fixture class
  class QueueTest : public ::testing::Test {
   protected:
    void SetUp() override {
       q1_.Enqueue(1);
       q2_.Enqueue(2);
       q2_.Enqueue(3);
    }

    // void TearDown() override {}

    Queue<int> q0_;
    Queue<int> q1_;
    Queue<int> q2_;
  };

  // tests
  TEST_F(QueueTest, IsEmptyInitially) {
    EXPECT_EQ(q0_.size(), 0);
  }

  TEST_F(QueueTest, DequeueWorks) {
    int* n = q0_.Dequeue();
    EXPECT_EQ(n, nullptr);

    n = q1_.Dequeue();
    ASSERT_NE(n, nullptr);
    EXPECT_EQ(*n, 1);
    EXPECT_EQ(q1_.size(), 0);
    delete n;

    n = q2_.Dequeue();
    ASSERT_NE(n, nullptr);
    EXPECT_EQ(*n, 2);
    EXPECT_EQ(q2_.size(), 1);
    delete n;
  }
#+end_src
**** running tests
***** TEST TEST_F 隐式注册到googletest，不需要显示指定需要运行哪些测试
***** RUN_ALL_TESTS()
* ASM
** event-based and tree-based api
** Parsing Generating Transforming class
** ClassReader ClassVisitor ClassWriter
** ClassWriter implemented ClassVisitor
*** visitXXX方法调用时会写入字节码数据
*** toByteArray返回记录的字节码数据
** ASMifier class -> java (generating class bytecode with ASM ClassWriter)
** visitor pattern
*** the visitor design pattern is a way of separating an algorithm from an object structure on which it operates
*** [[https://en.wikipedia.org/wiki/Visitor_pattern][wiki]]
* clojure
** cider
*** M-x cider-jack-in C-c M-j
* FPS tracer
** Choreographer.FrameCallback
* tracing activity startup
** reflect android.app.ActivityThread -> sCurrentActivityThread -> mH -> mCallback(hook with new one)
* ClassLoader
** locate or generate data that constitutes a definition for the class
** Class object contains a reference to the ClassLoader that defined it
** 数组对象的Class由JVM创建，非ClassLoader，且与其元素类型Class的ClassLoader相同；基本类型数据数组的Class无ClassLoader
** 代理加载机制，即先向父ClassLoader请求加载类，未找到则自己加载
** defineClass 将字节数组转换成Class对象
* gradle
** gradle init --type java-application
** gradle jar
#+begin_src groovy
  jar {
      manifest {
          attributes("Main-Class": "App")
      }
  }

  task uberJar(type: Jar) {
      classifier = "all"
      from sourceSets.main.output
      manifest {
          attributes("Main-Class": "App")
      }

      dependsOn configurations.runtimeClasspath
      from {
          configurations.runtimeClasspath.findAll { it.name.endsWith('jar') }.collect { zipTree(it) }
      }

      with jar
  }
#+end_src

* PhontomReference
** 必须与引用队列一起使用，提供在finalize执行之后得到通知的机会，比如执行post-mortem清理机制
* line-oriented search tools
** the silver searcher
** git grep
** ripgrep
* linux process group and session group
** process group
*** 一组进程，具有相同的进程组id，用于向这个进程组发送信号，fork pipe创建的进程属于一个进程组
** session group
*** 多个进程组组成会话
*** 一个进程组不能从一个会话迁移到另外一个会话
*** 一个进程组只能属于一个会话
*** 一个进程不能创建属于其他会话的进程组
* daemon process
** fork()
*** 子进程不是一个进程组的组长进程,这为下面执行setsid创建新会话创建条件
** setsid()
*** 成为新会话的首进程
*** 成为新进程组的组长进程
*** 没有控制终端与之相连
** umask(0)
*** 防止继承得来的文件模式创建屏蔽字在创建文件时会拒绝设置某些权限
** close fds: STDIN_FILENO STDOUT_FILENO STDERR_FILENO
#+begin_src c
  int fd = open("/dev/null", O_RDWR);
  dup2(fd, STDIN_FILENO);
  dup2(fd, STDOUT_FILENO);
#+end_src

* UI
** smallestWidth适配
*** [https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ==&mid=2650826034&idx=1&sn=5e86768d7abc1850b057941cdd003927&chksm=80b7b1acb7c038ba8912b9a09f7e0d41eef13ec0cea19462e47c4e4fe6a08ab760fec864c777&scene=21#wechat_redirect]
*** dp = px / density density = DPI/160
** 今日头条计算density
* APK size
** proguard
*** Shrink、Optimize 和 Obfuscate，也就是裁剪、优化、混淆
** dex
*** facebook redex byte code optimizer
*** so file 7-zip XZ
** shrinkresources
*** Lint 提示无用的资源
*** shrinkResources true in gradle
**** 没有处理resources.arsc文件
**** 没有删除资源文件
**** R.java文件需要提前准备好，所有资源都分配了一个常量ID，编译Java代码过程，将代码中的资源引用替换成常量
* find duplicated number in array
** [http://keithschwarz.com/interesting/code/?dir=find-duplicate]
** 此问题等价于链表找环问题
* linked list cycle
** 判断是否有环
*** h t两个指针从起点S出发，t每前进1步，h前进2步，只要二者都可以前进而且没有相遇，就保持二者推进。
*** 当h无法前进，即到达某个没有后继节点时，可以确定从S出发没有环，反之当t和h再次相遇时，就可以确定从S出发一定会进入某个环，设其为环C
** 环的长度
*** 判断出存在环C时，t和h位于同一点，设其为节点M。显然，仅需令h不动，而t不断推进，最终又会回到节点M，统计这一次t推进的步数，即得到环的长度
** 环的起点
*** t从起点S到相遇点M走过的距离是环C长度的整数倍，因为h走过的距离比t走过的距离多环长度的整数倍，而h的速度是t的2倍
*** 令t回到起点S，同时让h从节点M共同推进，h和t都一次前进一步，当h和t再次相遇时，设此次相遇时位于同一节点P，则P即为从起点S出发所到达环C的第一个节点
* git submodule
** git submodule add ${url}
** git clone -> git submodule init -> git submodule update
* bookmark optimization
** #+DESCRIPTION: emacs lisp multibyte string
   #+BEGIN_SRC emacs-lisp
     (defun compare (string-a string-b)
       (cl-loop for a being the elements of string-a
                for b being the elements of string-b
                unless (eql a b)
                return (cons a b)))

     (benchmark-run
         (let ((a (make-string 100000 0))
               (b (make-string 100000 0)))
           (compare a b)))
     ;; => (0.012568031 0 0.0)

     (benchmark-run
         (let ((a (make-string 100000 0))
               (b (make-string 100000 0)))
           (setf (aref a (1- (length a))) 256)
           (compare a b)))
     ;; => (0.012680513 0 0.0)

     (benchmark-run
         (let ((a (make-string 100000 0))
               (b (make-string 100000 0)))
           (setf (aref a (1- (length a))) 256
                 (aref b (1- (length b))) 256)
           (compare a b)))
     ;; => (2.327959762 0 0.0)
   #+END_SRC
** To avoid the O(n) cost on this common indexing operating, Emacs keeps a “bookmark” for the last indexing location into a multibyte string. If the next access is nearby, it can starting looking from this bookmark, forwards or backwards.
* application binary interface
** an interface between two binary program modules, often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user.
** a common aspect of an ABI is the calling convention
** X86 calling convention
*** The order in which atomic parameters, or individual parts of a complex parameter, are allocated
*** How parameters are passed (pushed on the stack, placed in registers, or a mix of both)
*** Which rigisters the called function must preserve for the caller
*** How the task of preparing the stack for, and restoring after, a function call is divided between the caller and the callee
* sqlite3
** shell
*** dot command
**** .help
**** .width
**** .mode
**** .echo
**** .headers
**** .open
**** .cd
*** sql statement
**** Think of each SQL statement as a separate computer program.  The
**** original SQL text is source code.  A prepared statement object
**** is the compiled object code.  All SQL must be converted into a
**** prepared statement before it can be run.
*** The life-cycle of a prepared statement object usually goes like this:
**** 1. Create the prepared statement object using [sqlite3_prepare_v2()].
**** 2. Bind values to [parameters] using the sqlite3_bind_*() interfaces.
**** 3. Run the SQL by calling [sqlite3_step()] one or more times.
**** 4. Reset the prepared statement using [sqlite3_reset()] then go back
**** 5. to step 2.  Do this zero or more times.
**** 6. Destroy the object using [sqlite3_finalize()].
** Hash table
*** code snippet
     #+begin_src c
       struct Hash {
	 unsigned int htsize;      /* Number of buckets in the hash table */
	 unsigned int count;       /* Number of entries in this table */
	 HashElem *first;          /* The first element of the array */
	 struct _ht {              /* the hash table */
	   int count;                 /* Number of entries with this hash */
	   HashElem *chain;           /* Pointer to first entry with this hash */
	 } *ht;
       };

       struct HashElem {
	 HashElem *next, *prev;       /* Next and previous elements in the table */
	 void *data;                  /* Data associated with this element */
	 const char *pKey;            /* Key associated with this element */
       };
       
       /*
	** The hashing function.
	*/
       static unsigned int strHash(const char *z){
	 unsigned int h = 0;
	 unsigned char c;
	 while( (c = (unsigned char)*z++)!=0 ){     /*OPTIMIZATION-IF-TRUE*/
	   /* Knuth multiplicative hashing.  (Sorting & Searching, p. 510).
	   ** 0x9e3779b1 is 2654435761 which is the closest prime number to
	   ** (2**32)*golden_ratio, where golden_ratio = (sqrt(5) - 1)/2. */
	   h += sqlite3UpperToLower[c];
	   h *= 0x9e3779b1;
	 }
	 return h;
	}
    #+end_src
*** All elements of the hash table are on a single doubly-linked list.
*** Hash.first points to the head of this list.
*** There are Hash.htsize buckets.  Each bucket points to a spot in the global doubly-linked list.
*** The contents of the bucket are the element pointed to plus the next _ht.count-1 elements in the list.
*** Hash.htsize and Hash.ht may be zero.  In that case lookup is done by a linear search of the global list. 
*** For small tables, the Hash.ht table is never allocated because if there are few elements in the table, it is faster to do a linear search than to manage the hash table.
** lemon parser
*** similar to bison yacc
*** grammr file parse.y
*** token(sqlte3GetToken()) -> parse(sqlite3Parser()) -> prepared Vdbe(in Parse context)
*** sqlite3_stmt == Vdbe
** prepare
*** -> sqlite3_prepare_v2
*** -> sqlite3LockAndPrepare
*** -> sqlite3Prepare
*** -> sqlite3RunParser
*** -> while(1) { sqlite3GetToken; sqlite3Parser; }
** step
*** -> sqlite3_step
*** -> sqlite3VdbeExec
**** big switch( pOp->opcode )
** atomic commit
*** rollback journal file
**** single file commit
***** acquiring a shared lock
****** allows two or more database connections read at the same time, prevent another connection from writing while we are reading it
***** reading information out of the database
****** reading from mass storage into os cache, then transferred from os cache into user space
***** obtaining a reserved lock
****** allows to read, but there can only be a single reserved lock on the database file
****** it signals that a process intends to modify the database file in the near future but has not yet started to make the modifications
***** creating a rollback journal file
****** write the original content of the database pages that are to altered into a rollback journal file
****** it contains all the information needed to restore the database back to its original state before the transaction
***** changing database pages in user space
****** each connection has its own private copy of user space, so the changes are only visible to the database connection that is making the changes
***** flushing the rollback journal file to mass storage
****** this is a critical step in ensuring that the database can survive an unexpected power loss
***** obtaining an exclusive lock
****** first obtains a pending lock, then it escalates the pending lock to an exclusive lock
****** a pending lock allows other processes that already have a shared lock to continue reading the database file, but it prevents new share lock from being established
****** the idea behind the pending lock is to prevent writer starvation caused by a large pool of readers
****** evetually all shared locks will be clear and the pending lock will then be able to escalate into an exclusive lock
***** writing changes to database file
****** changes only go as far as the system cache
***** flushing changes to mass storage
***** deleting the rollback journal file
****** SQLite gives the apprearance of having made no changes to the database file or having made the complete set of changes to the database file depending on whether or not the rollback journal file exists
***** releasing the lock
**** rollback
***** hot rollback journals
****** The rollback journal exists.
****** The rollback journal is not an empty file.
****** There is no reserved lock on the main database file.
****** The header of the rollback journal is well-formed and in particular has not been zeroed out.
****** The rollback journal does not contain the name of a master journal file (see section 5.5 below) or if does contain the name of a master journal, then that master journal file exists.
***** obtaining an exlusive lock
***** rolling back incomplete changes
***** deleting the hot journal
***** continue as if the uncompleted writes has never happened 
*** write-ahead logging(wal) mode
**** journal approach
***** The traditional rollback journal works by writing a copy of the original database content into a separate rollback journal file and then writing
***** changes directly into the original database file. In the event of a crash or ROLLBACK, the original content contained in the rollback journal is
***** played back into the database file to revert the database file to its original state. The COMMIT occurs when the rollback journal is deleted. 
**** journal approach
***** The WAL approach inverts this. The original content is preserved in the database file and the changes are appended into a separate WAL file. A 
***** COMMIT occurs when a special record indicating a commit is appended to the WAL. Thus a COMMIT can happen without ever writing to the original database
***** file, which allows readers to continue operating from the original unaltered database while changes are simultaneously being committed into the WAL.
***** Multiple transactions can be appened to the end of a single WAL file.
** sql tips
*** a single column with type (INTEGER PRIMARY KEY) is an alias for rowid(all rows within SQLite tables have a 64-bit signed integer key that identifies the row within its table)
*** column with INTEGER PRIMARY KEY is used as the rowid, and Table.iPKey is set to be the index of the column, -1 by default
*** if the key is not an INTEGER PRIMARY KEY, then create a UNIQUE index for the key, No index is created for INTEGER PRIMARY KEYs 
*** foreign key requires parent key columns must be subject to a UNIQUE constraint or have a UNIQUE index
*** An index should be created on the child key columns of each foreign key constraint, because each time an application deletes a row from the parent table, it performs a searching for referencing rows in the child table
*** ON UPDATE CASCADE or ON DELETE CASCADE means doing the same action on child key columns which is similar to trigger
*** any column in an SQLite3 database, except an INTEGER PRIMARY KEY column, may be used to store a value of any storage class, it is just that some columns, given a choice, will prefer to use one storage over another(aka. type affinity) 
*** INSERT OR IGNORE == INSERT ON CONFICT IGNORE
*** COLLATE NOCASE means ignore case when used in select or where statements
*** a default value of a column may be CURRENT_TIME, CURRENT_DATE, CURRENT_TIMESTAMP
** misc
*** db at index 0 is "main", db at index 1 is "temp"
*** column count limit in a table is 2000 by default
** Robson proof
*** N	The amount of raw memory needed by the memory allocation system in order to guarantee that no memory allocation will ever fail.
*** M	The maximum amount of memory that the application ever has checked out at any point in time.
*** n	The ratio of the largest memory allocation to the smallest. We assume that every memory allocation size is an integer multiple of the smallest memory allocation size.
*** N = M*(1 + (log2 n)/2) - n + 1
* c/c++ tips
** assert(argv[argc] == null)
** -DNDEBUG disable assert
** oop in c
*** define a strcut of class which contains constructor, destructor, etc, describing the class infomation 
*** a object is void *obj which is created using constructor in struct class
*** object has a pointer points to the struct class
*** analogy to Java
*** code snippets
    #+begin_src c
    void * new (const void * _class, ...)
    { 
      const struct Class * class = _class;
      void * p = calloc(1, class —> size);
      assert(p);
      *(const struct Class **) p = class;
      if (class —> ctor)
      { 
	va_list ap;
	va_start(ap, _class);
	p = class —> ctor(p, & ap);
	va_end(ap);
      }
      return p;
    }
    #+end_src
** we need to pass size param to malloc, then why not to call free?
*** malloc allocate a bit more memory than you asked for, this extra memory is used to store information such as the size of the allocated block
*** and a link to the next free block in a chain of blocks
*** and sometimes the "guard data" that helps the system to detect if you past the end of the allocated block
*** usually, most allocators will round up the size and/or the start of the block to a multiple of bytes such as 64bit in a 64-bit system
** c struct alignment rules
*** address of each member = 0 (mod sizeof(each member))
*** sizeof(struct) = 0 (mod sizeof(largest member))
*** char and char[] have no padding between them
** LD_PRELOAD environment variable could load your library before any other ones aka. program -> your library -> destination library
** Explicitly call the 64-bit version of lseek() on Android. Otherwise, lseek() is the 32-bit version, even if _FILE_OFFSET_BITS=64 is defined.
** fstat obtain information about an open file, such as owner, permission, size, file type symbolic directory socket character etc
** fork vs clone
*** fork create a new child process with 'copy-on-write' machanism, which executes in the child process from the point of the fork call
*** clone allows the child process to share parts of its execution context with the calling process, such as the virtual address space, the table of file descriptors, and the table of signal handlers
** /dev/random /dev/urandom are character files provide interface to system random generator
** memory barrier
*** asm volatile("" ::: "memory") compile-time memory barrier
*** __sync_synchronize runtime(HW) memory barrier
** new operator and operator new
*** operator new can be called explicitly as a regular function, but in C++, new is an operator with a very specific behavior: An expression with the new operator, first calls function operator new (i.e., this function) with the size of its type specifier as first argument, and if this is successful, it then automatically initializes or constructs the object (if needed). Finally, the expression evaluates as a pointer to the appropriate type.
*** placement new is construct new object in a known address
** valgrind
*** leak checks benchmarks
** readelf and objdump
*** objdump -dC main.out
** smart pointer
*** unique_ptr create a object which take over the destruction of the other object
** delete is null-pointer safe
** malloc
*** block = mem_control_block + data
*** sbrk to expand head space
*** code
    #+begin_src c
    /**
  * @brief Dynamic distribute memory function
  * @param numbytes: what size you need   
  * @retval a void pointer to the distribute first address
  */ 
void * malloc(unsigned int numbytes)
{
    unsigned int current_location,otherbck_location;
    /* This is the same as current_location, but cast to a memory_control_block */
    mem_control_block * current_location_mcb = NULL,* other_location_mcb = NULL;
    /* varialbe for saving return value and be set to 0 until we find something suitable */
    void * memory_location = NULL;
    /* current dividing block size */
    unsigned int process_blocksize;
    
    /* Initialize if we haven't already done so */
    if(! has_initialized) {
        malloc_init();
    }
    
    /* Begin searching at the start of managed memory */
    current_location = managed_memory_start;
    /* Keep going until we have searched all allocated space */
    while(current_location != managed_memory_end){
        /* current_location and current_location_mcb point to the same address.  However, 
         * current_location_mcb is of the correct type, so we can use it as a struct. current_location 
         * is a void pointer so we can use it to calculate addresses.
         */
        current_location_mcb = (mem_control_block *)current_location;
        /* judge whether current block is avaiable */
        if(current_location_mcb->is_available){
            /* judge whether current block size exactly fit for the need */
            if((current_location_mcb->current_blocksize == numbytes)){
                /* It is no longer available */ 
                current_location_mcb->is_available = 0;            
                /* We own it */
                memory_location = (void *)(current_location + sizeof(mem_control_block));
                /* Leave the loop */
                break;
            /* judge whether current block size is enough for dividing a new block */
            }else if(current_location_mcb->current_blocksize >= numbytes + sizeof(mem_control_block)){
                /* It is no longer available */ 
                current_location_mcb->is_available = 0;
                /* because we will divide current blcok,before we changed current block size,we should
                 * save the integral size.
                 */
                process_blocksize = current_location_mcb->current_blocksize;
                /* Now blcok size could be changed */
                current_location_mcb->current_blocksize = numbytes;
                
                /* find the memory_control_block's head of remaining block and set parameter,block of no
                 * parameter can't be managed. 
                 */
                other_location_mcb = (mem_control_block *)(current_location + numbytes \
                                                + sizeof(mem_control_block));
                /* the remaining block is still avaiable */
                other_location_mcb->is_available = 1;
                /* of course,its prior block size is numbytes */
                other_location_mcb->prior_blocksize = numbytes;
                /* its size should get small */
                other_location_mcb->current_blocksize = process_blocksize - numbytes \
                                                - sizeof(mem_control_block);
                
                /* find the memory_control_block's head of block after current block and \
                 * set parameter--prior_blocksize. 
                 */
                otherbck_location = current_location + process_blocksize \
                                            + sizeof(mem_control_block);                
                /* We need check wehter this block is on the edge of managed memeory! */
                if(otherbck_location != managed_memory_end){
                    other_location_mcb = (mem_control_block *)(otherbck_location);
                    /*  its prior block size has changed! */
                    other_location_mcb->prior_blocksize = process_blocksize\
                        - numbytes - sizeof(mem_control_block);
                }
                /*We own the occupied block ,not remaining block */ 
                memory_location = (void *)(current_location + sizeof(mem_control_block));
                /* Leave the loop */
                break;
            } 
        }
        /* current block is unavaiable or block size is too small and move to next block*/
        current_location += current_location_mcb->current_blocksize \
                                    + sizeof(mem_control_block);
    }
    /* if we still don't have a valid location,we'll have to return NULL */
    if(memory_location == NULL)    {
        return NULL;
    }
    /* return the pointer */
    return memory_location;    
}

/**
  * @brief  free your unused block 
  * @param  firstbyte: a pointer to first address of your unused block
  * @retval None
  */ 
void free(void *firstbyte) 
{
    unsigned int current_location,otherbck_location;
    mem_control_block * current_mcb = NULL,* next_mcb = NULL,* prior_mcb \
                                = NULL,* other_mcb = NULL;
    /* Backup from the given pointer to find the current block */
    current_location = (unsigned int)firstbyte - sizeof(mem_control_block);
    current_mcb = (mem_control_block *)current_location;
    /* Mark the block as being avaiable */
    current_mcb->is_available = 1;
    
    /* find next block location */
    otherbck_location = current_location + sizeof(mem_control_block) \
                                    + current_mcb->current_blocksize;
    /* We need check wehter this block is on the edge of managed memeory! */
    if(otherbck_location != managed_memory_end){
        /* point to next block */
        next_mcb = (mem_control_block *)otherbck_location;
        /* We need check whether its next block is avaiable */ 
        if(next_mcb->is_available){
            /* Because its next block is also avaiable,we should merge blocks */
            current_mcb->current_blocksize = current_mcb->current_blocksize \
                + sizeof(mem_control_block) + next_mcb->current_blocksize;
            
            /* We have merge two blocks,so we need change prior_blocksize of
             * block after the two blocks,just find next block location. 
             */
            otherbck_location = current_location + sizeof(mem_control_block) \
                                    + current_mcb->current_blocksize;
            /* We need check wehter this block is on the edge of managed memeory! */
            if(otherbck_location != managed_memory_end){
                other_mcb = (mem_control_block *)otherbck_location;
                /*  its prior block size has changed! */
                other_mcb->prior_blocksize = current_mcb->current_blocksize;
            }
        }
    }
    
    /* We need check wehter this block is on the edge of managed memeory! */
    if(current_location != managed_memory_start){
        /* point to prior block */
        prior_mcb = (mem_control_block *)(current_location - sizeof(mem_control_block)\
                                            - current_mcb->prior_blocksize);
        /* We need check whether its prior block is avaiable */ 
        if(prior_mcb->is_available){
            /* Because its prior block is also avaiable,we should merge blocks */
            prior_mcb->current_blocksize = prior_mcb->current_blocksize \
                + sizeof(mem_control_block) + current_mcb->current_blocksize;
            
            /* We have merge two blocks,so we need change prior_blocksize of
             * block after the two blocks,just find next block location. 
             */
            otherbck_location = current_location + sizeof(mem_control_block) \
                                    + current_mcb->current_blocksize;
            /* We need check wehter this block is on the edge of managed memeory! */
            if(otherbck_location != managed_memory_end){
                other_mcb = (mem_control_block *)otherbck_location;
                /*  its prior block size has changed! */
                other_mcb->prior_blocksize = prior_mcb->current_blocksize;
            }
        }
    }
}
    #+end_src
** wait_queue
*** schedule make process hang
**** important step 1: pick_next_task pick from sched_class
**** sched_class: rt_sched_class > fair_sched_class > idle_sched_class
**** important step 2: context_switch
***** switch_mm
***** switch_to
      #+begin_src c
 /*
  * context_switch - switch to the new MM and the new thread's register state.
  */
 static __always_inline struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
		struct task_struct *next, struct rq_flags *rf)
 {
	 prepare_task_switch(rq, prev, next);

	 /*
	  * For paravirt, this is coupled with an exit in switch_to to
	  * combine the page table reload and the switch backend into
	  * one hypercall.
	  */
	 arch_start_context_switch(prev);

	 /*
	  * kernel -> kernel   lazy + transfer active
	  *   user -> kernel   lazy + mmgrab() active
	  *
	  * kernel ->   user   switch + mmdrop() active
	  *   user ->   user   switch
	  */
	 if (!next->mm) {                                // to kernel
		 enter_lazy_tlb(prev->active_mm, next);

		 next->active_mm = prev->active_mm;
		 if (prev->mm)                           // from user
			 mmgrab(prev->active_mm);
		 else
			 prev->active_mm = NULL;
	 } else {                                        // to user
		 membarrier_switch_mm(rq, prev->active_mm, next->mm);
		 /*
		  * sys_membarrier() requires an smp_mb() between setting
		  * rq->curr / membarrier_switch_mm() and returning to userspace.
		  *
		  * The below provides this either through switch_mm(), or in
		  * case 'prev->active_mm == next->mm' through
		  * finish_task_switch()'s mmdrop().
		  */
		 switch_mm_irqs_off(prev->active_mm, next->mm, next);

		 if (!prev->mm) {                        // from kernel
			 /* will mmdrop() in finish_task_switch(). */
			 rq->prev_mm = prev->active_mm;
			 prev->active_mm = NULL;
		 }
	 }

	 rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

	 prepare_lock_switch(rq, next, rf);

	 /* Here we just switch the register state and the stack. */
	 switch_to(prev, next, prev);
	 barrier();

	 return finish_task_switch(prev);
 }
      #+end_src
**** code
     #+begin_src c
static void __sched notrace __schedule(bool preempt)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;

	schedule_debug(prev, preempt);

	if (sched_feat(HRTICK))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up().
	 *
	 * The membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;
	if (!preempt && prev->state) {
		if (signal_pending_state(prev->state, prev)) {
			prev->state = TASK_RUNNING;
		} else {
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf);
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();

	if (likely(prev != next)) {
		rq->nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq->curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq->curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		trace_sched_switch(preempt, prev, next);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf);
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);
	}

	balance_callback(rq);
}
     #+end_src
*** wake_up_process
*** sleep_on interruptible_sleep_on
*** wake_up wake_up_interruptible
** memeory management
*** buddy
**** __get_free_pages
*** slab
**** kmem_cache_alloc
**** high speed cache
***** kmem_cache
**** cache -> slab -> object
**** task_struct filp
**** /proc/slabinfo
*** do_page_fault
** fd -> file
*** struct file
    #+begin_src c
    // in struct task_struct
    ..
    /* Open file information: */
    struct files_struct		*files;

    /*
     * Open file table structure
     */
    struct files_struct {
      /*
       * read mostly part
       */
	    atomic_t count;
	    bool resize_in_progress;
	    wait_queue_head_t resize_wait;

	    struct fdtable __rcu *fdt;
	    struct fdtable fdtab;
      /*
       * written part on a separate cache line in SMP
       */
	    spinlock_t file_lock ____cacheline_aligned_in_smp;
	    unsigned int next_fd;
	    unsigned long close_on_exec_init[1];
	    unsigned long open_fds_init[1];
	    unsigned long full_fds_bits_init[1];
	    struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    struct fdtable {
	unsigned int max_fds;
	struct file __rcu **fd;      /* current fd array */ // fd is index in the fd array
	unsigned long *close_on_exec;
	unsigned long *open_fds;
	unsigned long *full_fds_bits;
	struct rcu_head rcu;
    };
    #+end_src

** fork
*** ref[https://blog.csdn.net/liushengxi_root/article/details/81332740]
** c++ four kinds of explicit type conversion
*** static_cast
**** 
*** dynamic_cast
**** ensuring safe downcast
***** If the cast is successful, dynamic_cast returns a value of type new_type.
***** If the cast fails and new_type is a pointer type, it returns a null pointer of that type.
***** If the cast fails and new_type is a reference type, it throws an exception that matches a handler of type std::bad_cast
*** reinterpret_cast
**** long -> pointer or pointer -> long
*** const_cast
**** remove or add `const` `volatile` attributes
** calc number of args with macro in glibc
*** INLINE_SYSCALL
*** code
    #+begin_src c
    #define __nargs(a,b,c,d,e,f,g,h,n) n
    #define nargs(...) __nargs(VARGS, 7, 6, 5, 4, 3, 2, 1)
    #+end_src
* linux kernel
** start_kernel
*** trap_init
**** idt_setup_traps
***** register IDT(interrupt description table)
      #+begin_src c
      static const __initconst struct idt_data def_idts[] = {
	INTG(X86_TRAP_DE,		divide_error),
	INTG(X86_TRAP_NMI,		nmi),
	INTG(X86_TRAP_BR,		bounds),
	INTG(X86_TRAP_UD,		invalid_op),
	INTG(X86_TRAP_NM,		device_not_available),
	INTG(X86_TRAP_OLD_MF,		coprocessor_segment_overrun),
	INTG(X86_TRAP_TS,		invalid_TSS),
	INTG(X86_TRAP_NP,		segment_not_present),
	INTG(X86_TRAP_SS,		stack_segment),
	INTG(X86_TRAP_GP,		general_protection),
	INTG(X86_TRAP_SPURIOUS,		spurious_interrupt_bug),
	INTG(X86_TRAP_MF,		coprocessor_error),
	INTG(X86_TRAP_AC,		alignment_check),
	INTG(X86_TRAP_XF,		simd_coprocessor_error),

#ifdef CONFIG_X86_32
	TSKG(X86_TRAP_DF,		GDT_ENTRY_DOUBLEFAULT_TSS),
#else
	INTG(X86_TRAP_DF,		double_fault),
#endif
	INTG(X86_TRAP_DB,		debug),

#ifdef CONFIG_X86_MCE
	INTG(X86_TRAP_MC,		&machine_check),
#endif

	SYSG(X86_TRAP_OF,		overflow),
#if defined(CONFIG_IA32_EMULATION)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_compat),
#elif defined(CONFIG_X86_32)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
#endif
};
      #+end_src
***** interrupt handler of syscall: entry_INT80_32
****** save context registers to pt_regs struct
****** code
#+begin_src c
       #ifdef __i386__
       struct pt_regs {
	 unsigned long bx;
	 unsigned long cx;
	 unsigned long dx;
	 unsigned long si;
	 unsigned long di;
	 unsigned long bp;
	 unsigned long ax;
	 unsigned long ds;
	 unsigned long es;
	 unsigned long fs;
	 unsigned long gs;
	 unsigned long orig_ax;
	 unsigned long ip;
	 unsigned long cs;
	 unsigned long flags;
	 unsigned long sp;
	 unsigned long ss;
       };
       #else 
       struct pt_regs {
	 unsigned long r15;
	 unsigned long r14;
	 unsigned long r13;
	 unsigned long r12;
	 unsigned long bp;
	 unsigned long bx;
	 unsigned long r11;
	 unsigned long r10;
	 unsigned long r9;
	 unsigned long r8;
	 unsigned long ax;
	 unsigned long cx;
	 unsigned long dx;
	 unsigned long si;
	 unsigned long di;
	 unsigned long orig_ax;
	 unsigned long ip;
	 unsigned long cs;
	 unsigned long flags;
	 unsigned long sp;
	 unsigned long ss;
       /* top of stack page */
       };
       #endif
       #+end_src

** syscall
*** `int 0x80` or `syscall` instruction
*** entry_INT80_32 or entry_SYSCALL_64
**** save user space regs to pt_regs
**** do_syscall_64 -> x32_sys_call_table[nr](regs);
*** open
**** 
*** exit_to_usermode_loop()
**** _TIF_NEED_RESCHED -> schedule()
** interrupt
*** per_cpu idt_table
**** 0-31 system intr and 0x80 syscall intr
**** others are device intr
*** device interrupt
**** irq_entries_table -> common_interrupt -> do_IRQ -> ret_from_intr
** task_struct
*** categories
**** id
     #+begin_src c
     pid_t pid;
     pid_t tgid; // pid of thread group leader
     struct task_struct *group_leader;
     #+end_src
**** status
     #+begin_src c
     volatile long state; // TASK_RUNNING, TASK_INTERRUPTIBLE, TASK_UNINTERUPTIBLE
     int exit_state;
     unsigned int flags;

     /* Used in tsk->state: */
     #define TASK_RUNNING			0x0000
     #define TASK_INTERRUPTIBLE		        0x0001
     #define TASK_UNINTERRUPTIBLE		0x0002
     #define __TASK_STOPPED			0x0004
     #define __TASK_TRACED			0x0008
     /* Used in tsk->exit_state: */
     #define EXIT_DEAD			        0x0010
     #define EXIT_ZOMBIE			0x0020
     #define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
     /* Used in tsk->state again: */
     #define TASK_PARKED			0x0040
     #define TASK_DEAD			        0x0080
     #define TASK_WAKEKILL			0x0100
     #define TASK_WAKING			0x0200
     #define TASK_NOLOAD			0x0400
     #define TASK_NEW			        0x0800
     #define TASK_STATE_MAX			0x1000

     /* Convenience macros for the sake of set_current_state: */
     #define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
     #define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)
     #define TASK_TRACED			(TASK_WAKEKILL | __TASK_TRACED)

     #define TASK_IDLE			        (TASK_UNINTERRUPTIBLE | TASK_NOLOAD)

     /* Convenience macros for the sake of wake_up(): */
     #define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)

     /* get_task_state(): */
     #define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \
					      TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
					      __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
					      TASK_PARKED)
	    #+end_src
**** schedule
     #+begin_src c
     //是否在运行队列上
     int on_rq;
     //优先级
     int prio;
     int static_prio;
     int normal_prio;
     unsigned int rt_priority;
     //调度器类
     const struct sched_class *sched_class;
     //调度实体
     struct sched_entity se;
     struct sched_rt_entity rt;
     struct sched_dl_entity dl;
     //调度策略
     unsigned int policy;
     //可以使用哪些CPU
     int nr_cpus_allowed;
     cpumask_t cpus_allowed;
     
     struct sched_info sched_info;
     
     #+end_src
**** signal
     #+begin_src c
     /* Signal handlers: */
     struct signal_struct *signal; // signal->shared_pending is process signal set
     struct sighand_struct *sighand;
     sigset_t blocked;
     sigset_t real_blocked;
     sigset_t saved_sigmask;
     struct sigpending pending; // signal set of current thread
     unsigned long sas_ss_sp;
     size_t sas_ss_size;
     unsigned int sas_ss_flags;    
     #+end_src

**** running statistics
     #+begin_src c
     u64        utime;//用户态消耗的CPU时间
     u64        stime;//内核态消耗的CPU时间
     unsigned long      nvcsw;//自愿(voluntary)上下文切换计数
     unsigned long      nivcsw;//非自愿(involuntary)上下文切换计数
     u64        start_time;//进程启动时间，不包含睡眠时间
     u64        real_start_time;//进程启动时间，包含睡眠时间
     #+end_src
**** process affinity
     #+begin_src c     
     struct task_struct __rcu *real_parent; /* real parent process */
     struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */
     struct list_head children;      /* list of my children */
     struct list_head sibling;       /* linkage in my parent's children list */
     #+end_src
**** credential
     #+begin_src c
     /* Objective and real subjective task credentials (COW): */
     const struct cred __rcu         *real_cred;
     /* Effective (overridable) subjective task credentials (COW): */
     const struct cred __rcu         *cred;
     
     struct cred {
	atomic_t	usage;
#ifdef CONFIG_DEBUG_CREDENTIALS
	atomic_t	subscribers;	/* number of processes subscribed */
	void		*put_addr;
	unsigned	magic;
#define CRED_MAGIC	0x43736564
#define CRED_MAGIC_DEAD	0x44656144
#endif
	kuid_t		uid;		/* real UID of the task */
	kgid_t		gid;		/* real GID of the task */
	kuid_t		suid;		/* saved UID of the task */
	kgid_t		sgid;		/* saved GID of the task */
	kuid_t		euid;		/* effective UID of the task */
	kgid_t		egid;		/* effective GID of the task */
	kuid_t		fsuid;		/* UID for VFS ops */
	kgid_t		fsgid;		/* GID for VFS ops */
	unsigned	securebits;	/* SUID-less security management */
	kernel_cap_t	cap_inheritable; /* caps our children can inherit */
	kernel_cap_t	cap_permitted;	/* caps we're permitted */
	kernel_cap_t	cap_effective;	/* caps we can actually use */
	kernel_cap_t	cap_bset;	/* capability bounding set */
	kernel_cap_t	cap_ambient;	/* Ambient capability set */
#ifdef CONFIG_KEYS
	unsigned char	jit_keyring;	/* default keyring to attach requested
					 * keys to */
	struct key	*session_keyring; /* keyring inherited over fork */
	struct key	*process_keyring; /* keyring private to this process */
	struct key	*thread_keyring; /* keyring private to this thread */
	struct key	*request_key_auth; /* assumed request_key authority */
#endif
#ifdef CONFIG_SECURITY
	void		*security;	/* subjective LSM security */
#endif
	struct user_struct *user;	/* real user ID subscription */
	struct user_namespace *user_ns; /* user_ns the caps and keyrings are relative to. */
	struct group_info *group_info;	/* supplementary groups for euid/fsgid */
	/* RCU deletion */
	union {
		int non_rcu;			/* Can we skip RCU deletion? */
		struct rcu_head	rcu;		/* RCU deletion hook */
	};
} __randomize_layout;
     #+end_src
**** memory management
     #+begin_src c
     struct mm_struct                *mm;
     struct mm_struct                *active_mm;
     #+end_src
**** file system and opened files
     #+begin_src c
     /* Filesystem information: */
     struct fs_struct                *fs;
     /* Open file information: */
     struct files_struct             *files;
     #+end_src
*** current pointer in kernel code
**** points to the current task which invokes the system call
**** /home/jocoo/d/linux-5.4/include/asm-generic/current.h
     #+begin_src c
     #define get_current() (current_thread_info()->task)
     #define current get_current()
     #+end_src
**** /home/jocoo/d/linux-5.4/arch/arm/include/asm/thread_info.h
     #+begin_src c
     static inline struct thread_info *current_thread_info(void) __attribute_const__;

     static inline struct thread_info *current_thread_info(void)
     {
        return (struct thread_info *)
           (current_stack_pointer & ~(THREAD_SIZE - 1));
     }
     #+end_src
*** task switch
**** schedule()
**** __switch_to(struct task_srtuct *prev_p, struct task_struct *next_p)
***** read current task pointer from PerCPU variable 
      #+begin_src c
      __visible __notrace_funcgraph struct task_struct *
      __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
      {
      //......
      this_cpu_write(current_task, next_p);
      //......
      return prev_p;
      }

      DECLARE_PER_CPU(struct task_struct *, current_task);
      #define this_cpu_read_stable(var)       percpu_stable_op("mov", var)
      
      static __always_inline struct task_struct *get_current(void){ 
        return this_cpu_read_stable(current_task);
      }
      #+end_src
*** task kernel stack
    #+begin_src c
    // include/linux/sched.h
    union thread_union {
#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK
	struct task_struct task;
#endif
#ifndef CONFIG_THREAD_INFO_IN_TASK
	struct thread_info thread_info;
#endif
	unsigned long stack[THREAD_SIZE/sizeof(long)];
};
    #+end_src
*** mm_struct
**** vm_area_struct
***** vm_start vm_end
**** count
***** count > 0 means lightweight process
** scheduler
*** category
**** stop_sched_class
**** dl_sched_class
**** rt_sched_class
**** fair_sched_class
**** idle_sched_class
*** diagram
   +--------------------+        +---------------------------------+
   |struct rq {//perCPU |        |struct cfs_rq {                  |                         +-------+
   |  struct cfs_rq cfs;+------->+  strcut rb_root tasks_timesline;+------------------------>+rb_node|
   |  struct rt_rq rt;  |        |  struct rb_node *rb_leftmost;   +----------+             X+-------+X
   |  struct dl_rq dl;  |        |}                                |          |            X           X
   |}                   |        +---------------------------------+          |           X             X
   +--------------------+                                                     |      +-------+        +-------+
									      |      |rb_node|        |rb_node|
									      |     X+------X+        +---X--- X
									      |    X        X             X     X
									      v   X         X             X      X
									   +--+----+     +-------+    +-------+   +-------+
									   |rb_node|     |rb_node|    |rb_node|   |rb_node|
									   +-------+     +-------+    +-------+   +-+-----+
														    ^
														    |
														    |
														    |
				   +-----------------------------+               +--------------------------+       |
				   |struct task_struct {         |               |struct sched_entity {     |       |
				   |  struct sched_entity se;    +-------------->+  struct rb_node run_node;+-------+
				   |  struct sched_rt_entity rt; |               |  u64 vruntime;           |
				   |}                            |               |}                         |
				   +-----------------------------+               +--------------------------+
*** schedule()
**** pick_next_task(rq, prev, &rf)
**** context_switch()
***** switch mm_struct
***** switch_to()
****** save %esp to TASK_threadsp(prev), restore %esp from TASK_threadsp(next)
****** __switch_to_asm()->__switch_to()
******* this_cpu_write(current_task, next_p)
******* load_sp0(tss, next) //restore to TSS
***** finish_task_switch(prev)
*** preempt
**** set flag TIF_NEED_RESCHED
***** scheduler_tick
***** try_to_wake_up
**** schedule time
***** exit from syscall to userspace; syscall_return_slowpath -> exit_to_usermode_loop() -> schedule()
***** exit from interrupt to userspace; do_IRQ() -> retint_user -> schedule()
***** exit from interrupt to kernel space; do_IRQ() -> retint_kernel -> schedule()
***** kernel space enable preempt; preempt_enable() -> schedule()
**** code
     #+begin_src c
     static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
     {
	     /*
	      * In order to return to user mode, we need to have IRQs off with
	      * none of EXIT_TO_USERMODE_LOOP_FLAGS set.  Several of these flags
	      * can be set at any time on preemptible kernels if we have IRQs on,
	      * so we need to loop.  Disabling preemption wouldn't help: doing the
	      * work to clear some of the flags can sleep.
	      */
	     while (true) {
		     /* We have work to do. */
		     local_irq_enable();

		     if (cached_flags & _TIF_NEED_RESCHED)
			     schedule();

		     if (cached_flags & _TIF_UPROBE)
			     uprobe_notify_resume(regs);

		     if (cached_flags & _TIF_PATCH_PENDING)
			     klp_update_patch_state(current);

		     /* deal with pending signal delivery */
		     if (cached_flags & _TIF_SIGPENDING)
			     do_signal(regs);

		     if (cached_flags & _TIF_NOTIFY_RESUME) {
			     clear_thread_flag(TIF_NOTIFY_RESUME);
			     tracehook_notify_resume(regs);
			     rseq_handle_notify_resume(NULL, regs);
		     }

		     if (cached_flags & _TIF_USER_RETURN_NOTIFY)
			     fire_user_return_notifiers();

		     /* Disable IRQs and retry */
		     local_irq_disable();

		     cached_flags = READ_ONCE(current_thread_info()->flags);

		     if (!(cached_flags & EXIT_TO_USERMODE_LOOP_FLAGS))
			     break;
	     }
     }
     #+end_src
** fork
*** copy_process
**** dup_task_struct
**** copy_files copy_fs copy_sighand copy_signal copy_mm
*** wake_up_new_task
**** set TIF_NEED_RESCHED
**** exit_to_usermode_loop() -> schedule()
** pthread_create
*** create pthread stack in heap(with mmap) of userspace with pthread and tls and guard-space configed
*** clone syscall
*** copy_process
**** dup_task_struct
*** call `start_thread` as a trampoline to call `start_routine` of thread with user-defined arg
** signal
*** [1,31] non-rt signal [32, x] real-time signal
*** kill -> process
**** task_struct -> signal_struct -> struct sigpending `shared_pending` shared by all threads in the process
*** tgkill -> thread
**** task_struct -> `sigpending` stores signal data for thread
*** exit_to_usermode_loop() check and handle signal
*** creating frame in userspace stack for sigaction then return to kernel space with sigreturn syscall which is on sigframe
*** sigaction
**** SA_ONESHOT - called once then restored to default handler
**** rt_sigaction
**** do_sigaction
***** task_struct->sighand_struct->struct k_sigaction action[_NSIG]
***** signum to k_sigaction(associated user sigaction)
** fs
*** open syscall
**** get_unused_fd_flags
***** fdtable->open_fds(bitmap)->find_next_fd->find_next_zero_bit->__ffs->__buildin_ctzl(count trailing zero bits long)
**** do_filp_open
***** path_openat
****** do_last
******* lookup_fast(search in dcache)
******* vfs_open
******** open(inode, file) - assigned as inode->i_fop->open
*** read syscall
**** find_get_page
**** page_cache_sync_readahead
**** copy_page_to_iter
*** struct file - opened file
**** file_operations -> filesystem operations, eg ext4_file_operations
*** strcut mount - mount info
*** struct files_struct - opened files list
*** struct path - mount info and dentry
**** code
     #+begin_src c
     struct path {
       struct vfsmount *mnt;
       struct dentry *dentry;
     } __randomize_layout;
     #+end_src

*** struct dentry - dir name, file name, associated inode
*** struct address_space - contents of mappable or cacheable objects, eg, mmap, memory cache for open read syscall
*** kmap_atomic(page) - map page to a kernel vm address
*** register_filesystem
**** just add a struct file_system_type to global file_systems linked list
*** super_block do management of inode(create, write, destroy)
**** struct super_block->s_op(struct super_operations)
*** block device
*** 
**** bdev pseudo filesystem
**** bdev_map
***** dev_t to gendisk
** cdev
*** insmod
**** module_init
***** install pair of dev_t and `struct cdev` into cdev_map
***** struct cdev - {dev_t, struct file_operations}
*** mknod /dev/{name}
**** do_mknodat
***** inode->i_fop = &def_chr_fops
***** def_chr_fops = {.open = chrdev_open}
*** open
**** call chrdev_open
***** replace_fops(filp, fops) - f
** radix tree
*** a compressed trie
*** associated array
*** echo node represents a fragment of key,and points to child fragments
*** linux 4bit per node, nginx 2bit per node
*** shift is high to low from top to bottom
*** graph
               +--------+
               |  0100  |
               +---+----+
                   |
                   |
     +----------------------------+
     |             |              |
     |             |              |
     v             v              v
+----+---+    +----+---+     +----+---+
|  0000  |    |  0001  |     |  0010  |  ....
+--------+    +--------+     +--------+



                 ...



 +--------+       +--------+     +--------+
 |  0010  |       |  0011  |     |  0100  |
 +---+----+       +----+---+     +----+---+
     |                 |              |
     |                 |              |
     v                 v              v
 +---+----+       +----+---+     +----+---+
 |  value |       |  value |     |  value |
 +--------+       +--------+     +--------+
** ipc
*** cmd
**** ipcmk
**** ipcs - inspect
**** ipcrm
*** pipe
**** mkfifo
**** pipe2 syscall
***** do_pipe2 -> __do_pipe_flags -> copy_to_user -> fd_install
***** __do_pipe_flags -> create_pipe_files -> get_pipe_inode -> new_inode_pseudo(pipefs)
****** pipe_mnt->mnt_sb -> alloc_inode
***** struct file -> private_data = pipe_inode_info(has a member: struct pipe_buffer *bufs)
***** f_op = pipefifo_fops
*** msgqueue
**** msgget
**** msgsnd
**** msgrcv
*** shmem
**** shmget
***** create a file on shmem filesystem
**** shmat - attach
**** shmdt - detach
**** shmctl
*** semaphore
**** semget
**** semctl
**** semop
** socket
*** socket(int domain, int type, int protocol)
**** args
***** domain - AF_UNIX, AF_INET
***** type - SOCK_STREAM, SOCK_DGRAM, SOCK_RAW
***** protocol - IPPROTO_TCP, IPPROTO_UDP, IPPROTO_ICMP, 0(inferred) or specified
**** syscall
***** __sys_socket => sock_create -> sock_map_fd
***** sock_create(aka. __sock_create) => sock_alloc -> pf->create()
****** sock_alloc
******* new_pseudo_inode(sock_mnt->mnt_sb) -> alloc_inode -> sock_alloc_inode
******* sock_mnt->mnt_sb->s_op = &sockfs_ops {.alloc_inode = sock_alloc_inode} in sockfs mount
****** pf = rcu_dereference(net_families[family])
******* af_net.c inet_init => proto_register(&tcp_prot, 1) -> sock_register(&inet_family_ops){.create = inet_create} -> inet_register_protosw
****** inet_create => lookup inet_protosw in inetsw array with type and protocol -> sk_alloc ->
******* lookup -> answer{.prot = &tcp_prot, .ops = &inet_stream_ops} for instance
******* sk_alloc
******** sock_common <- sock <- inet_sock <- inet_connection_sock <- tcp_sock (sock class hierachy)
******** sock_common <- sock <- inet_sock <- udp_sock
******** sock_common <- sock <- inet_sock (for ping sock)
******** sock_common <- sock <- raw_sock
****** sock_map_fd => get_unused_fd_flags -> sock_alloc_file
******* sock_alloc_file
******** file = alloc_file_pseudo(..., &socket_file_ops)
******** sock->file = file;
******** file->private_data = sock;
*** bind(int fd, struct sockaddr* addr, int addrlen)
**** sockfd_lookup_light => sock_from_file
**** move_addr_to_kernel
**** sock->ops->bind() -> inet_bind -> __inet_bind
***** sk->sk_prot->get_port() -> inet_csk_get_port()
*** listen(int fd, int backlog)
**** sockfd_lookup_light => sock_from_file
**** sock->ops->listen() -> inet_listen -> inet_csk_listen_start()
**** inet_csk_listen_start
***** init accept queue - reqsk_queue_alloc(&icsk->icsk_accept_queue);
***** set listen state - inet_sk_state_store(sk, TCP_LISTEN);
* http protocal
** cookie
*** ref [[https://tools.ietf.org/html/rfc6265][RFC6265]]
*** syntax
**** set-cookie-header = Set-Cookie: name=value(; cookie-av)*
**** cookie-av = expires-av / max-age-av / domain-av / path-av / secure-av / httponly-av / extension-av
*** Max-Age prior to Expires
*** Domain
**** The user agent will reject cookies unless the Domain attribute specifies a scope for the cookie that would include the origin server.
**** For example, the user agent will accept a cookie with a Domain attribute of "example.com" or of "foo.example.com" from foo.example.com,
**** but the user agent will not accept a cookie with a Domain attribute of "bar.example.com" or of "baz.foo.example.com".
* java lambda vs inner class
** inner class create an instance if the inner class
** lambda invoke the lambda function through INVOKEDYNAMIC instruction and MethodLookup MethodHandle
* data structure
** HashMap
*** key is nullable
*** hash()
    #+begin_src java
    static final int hash(Object key) {
      int h;
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >> 16);
    }
    #+end_src
*** index of key
    #+begin_src java
    int i = (table.lenght - 1) & hash(key);
    #+end_src
* get generic type in java at runtime
** anonymous inner class
*** code
    #+begin_src kotlin
      open class GenericsToken<T> {
	  var type: Type = Any::class.java

	  init {
	      val superClass = this.javaClass.genericSuperclass
	      type = (superClass as ParameterizedType).actualTypeArguments[0]
	  }
      }
      val gt = object : GenericsToken<List<String>>(){}
      println(gt.type)
    #+end_src
** kotlin reified inline function
*** code
    #+begin_src kotlin
       inline fun <reified T: Any> Gson.fromJson(json: String) : T {
	   return Gson().fromJson<T>(json, T::class.java)
       }
    #+end_src
* java generics
** generic types in Java are invariant
   #+begin_src java
   // Java
   List<String> strs = new ArrayList<String>();
   List<Object> objs = strs; // !!! The cause of the upcoming problem sits here. Java prohibits this!
   objs.add(1); // Here we put an Integer into a list of Strings
   String s = strs.get(0); // !!! ClassCastException: Cannot cast Integer to String
   #+end_src
** covariant
*** Collection<String> is a subtype of Collection<? extends Object>.
*** In "clever words", the wildcard with an extends-bound (upper bound) makes the type covariant.
** contravariance
*** in Java we have List<? super String> a supertype of List<Object>
** mnemonic
*** PECS stands for Producer-Extends, Consumer-Super.
** kotlin Declaration-site variance
*** in
    #+begin_src kotlin
    interface Source<out T> {
      fun nextT(): T
    }

    fun demo(strs: Source<String>) {
      val objects: Source<Any> = strs // This is OK, since T is an out-parameter
      // ...
    }
    #+end_src
*** out
    #+begin_src kotlin
    interface Comparable<in T> {
      operator fun compareTo(other: T): Int
    }

    fun demo(x: Comparable<Number>) {
      x.compareTo(1.0) // 1.0 has type Double, which is a subtype of Number
      // Thus, we can assign x to a variable of type Comparable<Double>
      val y: Comparable<Double> = x // OK!
    }
    #+end_src
** notes
*** if you use a producer-object, say, List<? extends Foo>, you are not allowed to call add() or set() on this object, but this does not mean that this object is immutable: for example, nothing prevents you from calling clear() to remove all items from the list, since clear() does not take any parameters at all. The only thing guaranteed by wildcards (or other types of variance) is type safety. Immutability is a completely different story.

* PriorityQueue
** offer
   #+begin_src java
    public boolean offer(E e) {
        if (e == null)
            throw new NullPointerException();
        modCount++;
        int i = size;
        if (i >= queue.length)
            grow(i + 1);
        size = i + 1;
        if (i == 0)
            queue[0] = e;
        else
            siftUp(i, e);
        return true;
    }

    private void siftUp(int k, E x) {
        if (comparator != null)
            siftUpUsingComparator(k, x);
        else
            siftUpComparable(k, x);
    }

    @SuppressWarnings("unchecked")
    private void siftUpComparable(int k, E x) {
        Comparable<? super E> key = (Comparable<? super E>) x;
        while (k > 0) {
            int parent = (k - 1) >>> 1;
            Object e = queue[parent];
            if (key.compareTo((E) e) >= 0)
                break;
            queue[k] = e;
            k = parent;
        }
        queue[k] = key;
    }
   #+end_src
** poll
   #+begin_src java
    public E poll() {
        if (size == 0)
            return null;
        int s = --size;
        modCount++;
        E result = (E) queue[0];
        E x = (E) queue[s];
        queue[s] = null;
        if (s != 0)
            siftDown(0, x);
        return result;
    }

    private void siftDown(int k, E x) {
        if (comparator != null)
            siftDownUsingComparator(k, x);
        else
            siftDownComparable(k, x);
    }

    @SuppressWarnings("unchecked")
    private void siftDownComparable(int k, E x) {
        Comparable<? super E> key = (Comparable<? super E>)x;
        int half = size >>> 1;        // loop while a non-leaf
        while (k < half) {
            int child = (k << 1) + 1; // assume left child is least
            Object c = queue[child];
            int right = child + 1;
            if (right < size &&
                ((Comparable<? super E>) c).compareTo((E) queue[right]) > 0)
                c = queue[child = right];
            if (key.compareTo((E) c) <= 0)
                break;
            queue[k] = c;
            k = child;
        }
        queue[k] = key;
    }
   #+end_src
* epoll
** interest list and ready list
*** epoll_create
*** epoll_ctl
*** epoll_wait
** multiplex io should use nonblocking fd
** edge-triggered
*** notify when state changes
** level-triggered
*** notify each state
* dynamic programming
** child problem with overlapping
*** divide-and-conquer without overlapping
** top to bottom
*** memorizing result of child problem
*** handling child-problem in depth-first search
** bottom up
*** handling child problem in reverse topological sort
** Longest common subsequence
*** minimum edit distance
*** diff program in unix
*** k-candidates optimization
**** https://www.cs.dartmouth.edu/~doug/diff.pdf
* catlan number
** h(n) = h(0)*h(n-1) + h(1)*h(n-2) + ... + h(n-1)*h(0), h(0)=h(1)=1
** h(n) = C(2n,n)/(n+1)
** h(n) = C(2n,n) - C(2n,n-1)
** parens inserting
** binary search tree enuming
** solution: reflection
*** S - push, X - pop
*** find first position of Count(X) - Count(S) = 1 in sequence (S of n, X of n, C(2n,n) )
*** S: s t (sum n)
*** X: s+1 t-1 (sum n)
*** reflect left partition-> S: s+1+t=n+1 X: s+t-1=n-1
*** find first position of Count(S) - Count(X) = 1 in sequence(S of n+1, X of n-1, C(2n,n-1))
*** S: s t (sum n+1)
*** X: s-1 t-1(sum n-1)
*** reflect left partition -> S: s-1+t=n X: s+t-1=n
* ViewGroup
** isMotionEventSplittingEnabled
*** when true(default), ACTION_POINTER_DOWN will be split
*** case two buttons in a viewgroup, press down two buttons at the same time, two click will invoked
**** viewgroup: ACTION_DOWN -> ACTION_POINTER_DOWN -> ACTION_POINTER_UP -> ACTION_UP
**** both buttons: ACTION_DOWN -> ACTION_UP
* socket
** unix domain socket vs IP socket
*** unix domain socket
**** fast (no need protocol layer encode-decode)
***** send data -> kernel buffer -> TCP -> IP -> LINK -> ... LINK -> IP -> TCP -> kernel buffer -> recv data
**** bind file path while ip socket bind ip address and port
** int socket(int domain, int type, int protocol);
*** domain - address family
**** AF_UNIX
**** AF_INET
*** type
**** SOCK_STREAM
**** SOCK_DGRAM
*** protocol
** int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
*** bind a name to socket
** int listen(int sockfd, int backlog);
*** The backlog argument defines the maximum length to which the queue of pending connections for sockfd may grow.  If a connection request  arrives  when  the queue  is  full, the client may receive an error with an indication of ECONNREFUSED or, if the underlying protocol supports retransmission, the request may be ignored so that a later reattempt at connection succeeds.
** int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
*** may block for incomming connection in runtime
*** The  accept()  system call is used with connection-based socket types (SOCK_STREAM, SOCK_SEQPACKET).  It extracts the first connection request on the queue of pending connections for the listening socket, sockfd, creates a new connected socket, and returns a new file descriptor referring to that  socket. The newly created socket is not in the listening state.  The original socket sockfd is unaffected by this call.
* kotlin coroutine
** kotlin compiler
*** suspend lambda compiled into a class extends `SuspendLambda` class and implementing `Function2` interface
*** `SuspendLambda` super class is BaseContinuationImpl which is `Continuation`
*** user defined code is compiled into `invokeSuspend` member function with a Result object parameter
*** member integer variable `label` is a black magic which acks like `swich-case` thus the `Continuation` can resume with result returned from the suspend point
** kotlinx.coroutines SDK
*** Dispatchers
**** Default
***** DefaultScheduler
**** Main
***** UI operations
**** Unconfined
**** IO
***** share threads with `Default` dispatcher
*** CoroutineContext
**** an indexed set of `Element` instances
**** plus operator
***** contextA + contextB
****** Returns a context containing elements from contextA and elements from contextB. The elements from contextA with the same key as in contextB is dropped.
*** common behavior
**** launch
***** append `Dispatchers.Default` when `CoroutineInterceptor` is empty
***** Create `StandaloneCoroutine`
***** start coroutine
****** create
****** intercepted
******* interceptContinuation
******** Returns a continuation that wraps the provided continuation, thus intercepting all resumptions.
******** create a `DispatchedContinuation` which dispatches a task wrapping continuation resumption to dispatches worker thread.
****** resumeWith
******* can be dispatched to Dispatchers's thread
***** return coroutine
**** delay
***** create `CancellableContinuationImpl`
***** `DefaultDelay` (alias of `DefaultExecutor` which extends `EventLoopImplBase`) `scheduleResumeAfterDelay` continuation created above
****** create `DelayResumeTask` and `schedule`
****** if `schedule` successfully then call `unpark` of `EventLoopImplPlatform` within which a worker thread was created and unparked
***** when delay time is reached the `continuation` is resumed
*** exceptions
**** -> parent coroutine -> context[CoroutineExceptionHandler] -> Global CoroutineExceptionHandler with ServiceLoader -> Thread.uncaughtExceptionHandler
*** a continuation parameter(as callback, which is outer scope coroutine) added to suspending function by compiler
**** receive with `suspendCoroutineUninterceptedOrReturn` function
* android view drawing process
** Measuring layouting drawing
** custom view
** MeasureSpec
** ?attr
** overloads
*** onSizeChanged
*** onFinishInflate
** dispatchTouchEvent
*** TouchTarget aka mFirstTouchTarget
**** TouchTarget is a view with pointers touched to it
**** TouchTargets connected as a linked list
*** when (event is not canceled or intercepted) and (action is ACTION_DOWN or ACTION_POINTER_DOWN), find if a child would to consume the event, if so create a touch target as the head of mFirstTouchTarget(linked list)
**** special case
***** when an event with ACTION_POINTER_DOWN can't find a consumer, it will add the associated pointer id to the TouchTarget  created by the early ACTION_DOWN event
*** other events with pointer_up action_up will delivered to mFirstTouchTarget
*** if no child would to consume the event, the viewgroup it self will try it
** click listener
*** performed when view is clickable or longclickable
*** performed between action_down and action_up event(not the others, eg. action_pointer_down or action_pointer_up)
** window
** nested scrolling
* RxJava
** CacheObservable and NetworkObservable case
*** requires
**** CacheObservable return first then NetworkObservable, take Cache then Network
**** NetworkObservable return first then CacheObservable, take Network only
*** operators
**** publish(optional) + merge + takeUtil
**** publish
***** Returns an observable that emits the results of invoking a specified selector on items emitted by a ConnectableObservable that shares a single subscription to the underlying sequence.
***** upstream -> SourceObserver -> PublishSubject -> applying selector function -> new Observable -> downstream
**** merge
***** delegated to flatMap with non-action(Functions.identity()) mapper function
**** flatMap
***** Returns an Observable that emits items based on applying a function that you supply to each item emitted by the source ObservableSource, where that function returns an ObservableSource, and then merging those resulting ObservableSources and emitting the results of this merger, while limiting the maximum number of concurrent subscriptions to these ObservableSources.
***** upstream -> MergeObserver -> mapper function -> new Observable -> InnerObserver -> dispatch to downstream or cache data in queue of InnerObserver
**** takeUtil
***** Returns an observable that emits the items emitted by the source observable until a second observable emits an item.
**** code
     #+begin_src java
     networkObservable.publish(new Function<Observable<LocalLifeModel>, ObservableSource<LocalLifeModel>>() {
       @Override
       public ObservableSource<LocalLifeModel> apply(@io.reactivex.annotations.NonNull Observable<LocalLifeModel> localLifeModelObservable) throws Exception {
         return Observable.merge(localLifeModelObservable, cacheObservable.takeUntil(localLifeModelObservable));
       }
   })
     #+end_src
** PublishSubject
*** A Subject that emits(multicasts) items to currently subscribed Observers and terminal events to current or late Observers
*** as a Observable and a Observer, thus Connectable Observable
** Disposable$dispose
*** cancelling the connection with parent(upstream) which is passed to `onSubscribe` when calling `subscribe`
** comprehensible diagram
*** Observable S -> operator A -> operator B -> operator C -> Observable D
**** conceptually, D = ObservableWrapperC( ObservableWrapperB( ObservableWrapperA( S ) ) ), which is wrapped in operator function
*** Observable D subscribes Observer O
**** conceptually, FinalObserver FO = ObserverWrapperA( ObserverWrapperB( ObserverWrapperC( O ) ) ), which is wrapped in `subscribeActual` function generally
**** thus the data flow is: Observable S -> ObserverWrapperA -> ObserverWrapperB -> ObserverWrapperC -> Observer O
** fusion
*** BasicFuseableObserver
*** wip(work-in-process) counter/indicator
** error handling
*** after disposing
**** exception -> RxJavaPlugins.errorHandler -> Thread.getUncaughtExceptionHandler()(defaultUncaughtExceptionHandler -> thread group)
*** before disposing
**** onErrorResumeNext
* android input system
** diagram
*** user touch -> driver -> /dev/input/event* (EventHub via epoll(polling new events) and inotify(adding or removing devices)) -> InputReader enqueue EventEntry to mInboundQueue(property of InputDispatcher) -> InputDispatcher dequeue DispatchEntry to outboundQueue(property of Connection), InputManagerService created a pair socket(socketpair) with UI thread of currently focused window, thus InputDispatcher send input data to UI thread via Connection -> Looper.loop -> MessageQueue.next -> MessageQueue.nativePollOnce -> InputEventReceiver.dispatchInputEvent -> call ViewRootImpl$WindowInputEventReceiver.onInputEvent -> mView.dispatchPointerEvent -> windowCallback.dispatchTouchEvent(Activity or Dialog) -> PhoneWindow.dispatchTouchEvent -> view.dispatchTouchEvent
*** ref[https://wanandroid.com/wenda/show/12119]
*** InputManagerService
**** InputChannel client end: ViewRootImpl.mInputChannel, epoll-controlled by Looper of UI thread
**** InputChannel server end: WindowState.mInputChannel, epoll-controlled by Looper of InputDispatcher
**** diagram
***** ViewRootImpl$setView -> mWindowSession.addToDisplay(...mInputChannel) --binder-> Session$addToDisplay(outInputChannel) -> WMS$addWindow() -> win = new WindowState() -> win.openInputChannel() -> InputChannel$openInputChannelPair() --jni-> socketpair() -> mClientChannel.transferTo(outInputChannel) -> mService.mInputManager.registerInputChannel() -> mInputManager->getDispatcher()->registerInputChannel -> new Connection and mLooper.addFd(fd, 0, ALOOPER_EVENT_INPUT, handleReceiveCallback, this)
***** ViewRootImpl$setView -> new WindowInputReceiver(mInputChannel, Looper.myLooper()) -> InputEventReceiver.nativeInit --jni-> receiver = new NativeInputReceiver -> receiver.initialize() -> setFdEvents(ALOOPER_EVENT_INPUT) -> mMessageQueue->getLooper()->addFd(fd, 0, events, this/*callback*/, NULL)
* android smart pointer
** RefBase
*** wrapping the strong reference count and weak reference count, for auto deallocate objects which usually extends from `RefBase` class, when decStrong() encounts 0
** sp template class
*** wrapping RefBase objects, implementing object copy constructing, copy assignment, move constructing, move assignment and deallocating, increasing or decreasing strong references of delegated RefBase objects
* android Watchdog
** monitoring deadlock of targets
** started as a thread in SystemServer
* binder
** binder driver
*** binder_init
**** create debugfs
**** register misc device, install file_operations struct
*** binder_open
**** create binder_proc struct which is added to global `binder_procs` list
**** filep.private_data = proc, store binder_proc data to file private_data for later use
*** binder_mmap
**** 
* todo
** binder in c implementation on android device
** open device /dev/rtc
