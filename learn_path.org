* crash
*** 进程crash时会发送signal（SIGSEGV，SIGART etc），提前注册sigaction处理目标signal，然后clone进程，使用ptrace syscall获取进程mem reg信息，同时获取procfs信息得到thread stacktrace
*** breakpad   ---> crashDump --> addr2line
*** fork使用fork syscall实现, pthread_create都使用clone syscall实现，可以指定共享哪些数据区域
*** thread specific stack is on heap(mmap) with guard page of guardsize protected by mprotect
*** core dump file
**** with systemd systems cat /proc/sys/kernel/core_pattern -> |/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e
**** core dump files stored in directory /var/lib/systemd/coredump
**** core dump file managed by coredumpctl
**** list all core dump files: coredumpctl list | tail -5
**** extract core dump file: coredumpctl dump ${pid} -o core
* Finalizer
*** object.finalize() run in FinalizerDaemon thread
*** FinalizerWatchdogDaemon thread 监控FinalizerDaemon thread的执行，若FinalizerDaemon执行某个object的finalize方法超过10s，则发出SIGQUIT signal
*** finalize timeout 可以通过反射停止FinalizerWatchdogDaemon，Android P限制反射调用失效

* generate code at runtime
*** u1 code[PAGE_SIZE]; mprotect(code, PAGE_SIZE, PROT_READ | PROT_WRITE | PROT_EXEC); memcpy(code, "\xC3", 1);
* RecyclerView
*** Recycler.tryGetViewHolderForPositionByDeadline缓存逻辑:
    # 根据position查找mAttachedScrap-->mHiddenViews-->mCachedViews，
    # 根据type查找mAttachedScrap-->mCachedViews
    # if mViewCacheExtension != null, 通过mViewCacheExtension查找
    # mRecyclerPool.mScrap中查找
    # mAdapter.createViewHolder

* android gradle plugin
*** 方法的最后一个参数是闭包时可以将{}移到()外，同时()可以省略，所以常见的android {...}
*** android包含属性：compileSdkVersion、buildToolsVersion、defaultConfig、buildTypes、signingConfigs、lintOptions、compileOptions etc
*** defaultConfig是默认的productFlavor类型，包含applicationId、minSdkVersion、versionCode、versionName、testInstrumentationRunner、ndk etc
*** buildType属性：applicationSuffix、debuggable、jniDebuggable、minifyEnabled、multiDexEnabled、proguardFile、shrinkResources、signingConfig、zipAlignEnabled
*** variant 是productFlavors和buildTypes组合产物，variant有applicationVariants、libraryVariants、testVariants
*** 使用exec可以执行命令行操作
*** 可以通过环境变量System.getenv()获取签名信息，从而隐藏签名信息
*** manifestPlaceHolders buildConfigField resValue
* Java Thread start
*** State
    #+begin_src java
        public enum State {
        /**
         * Thread state for a thread which has not yet started.
         */
        NEW,

        /**
         * Thread state for a runnable thread.  A thread in the runnable
         * state is executing in the Java virtual machine but it may
         * be waiting for other resources from the operating system
         * such as processor.
         */
        RUNNABLE,

        /**
         * Thread state for a thread blocked waiting for a monitor lock.
         * A thread in the blocked state is waiting for a monitor lock
         * to enter a synchronized block/method or
         * reenter a synchronized block/method after calling
         * {@link Object#wait() Object.wait}.
         */
        BLOCKED,

        /**
         * Thread state for a waiting thread.
         * A thread is in the waiting state due to calling one of the
         * following methods:
         * <ul>
         *   <li>{@link Object#wait() Object.wait} with no timeout</li>
         *   <li>{@link #join() Thread.join} with no timeout</li>
         *   <li>{@link LockSupport#park() LockSupport.park}</li>
         * </ul>
         *
         * <p>A thread in the waiting state is waiting for another thread to
         * perform a particular action.
         *
         * For example, a thread that has called <tt>Object.wait()</tt>
         * on an object is waiting for another thread to call
         * <tt>Object.notify()</tt> or <tt>Object.notifyAll()</tt> on
         * that object. A thread that has called <tt>Thread.join()</tt>
         * is waiting for a specified thread to terminate.
         */
        WAITING,

        /**
         * Thread state for a waiting thread with a specified waiting time.
         * A thread is in the timed waiting state due to calling one of
         * the following methods with a specified positive waiting time:
         * <ul>
         *   <li>{@link #sleep Thread.sleep}</li>
         *   <li>{@link Object#wait(long) Object.wait} with timeout</li>
         *   <li>{@link #join(long) Thread.join} with timeout</li>
         *   <li>{@link LockSupport#parkNanos LockSupport.parkNanos}</li>
         *   <li>{@link LockSupport#parkUntil LockSupport.parkUntil}</li>
         * </ul>
         */
        TIMED_WAITING,

        /**
         * Thread state for a terminated thread.
         * The thread has completed execution.
         */
        TERMINATED;
    }
    #+end_src

*** android-source/art/runtime/thread_state.h
enum ThreadState {
  //                                   Thread.State   JDWP state
  kTerminated = 66,                 // TERMINATED     TS_ZOMBIE    Thread.run has returned, but Thread* still around
  kRunnable,                        // RUNNABLE       TS_RUNNING   runnable
  kTimedWaiting,                    // TIMED_WAITING  TS_WAIT      in Object.wait() with a timeout
  kSleeping,                        // TIMED_WAITING  TS_SLEEPING  in Thread.sleep()
  kBlocked,                         // BLOCKED        TS_MONITOR   blocked on a monitor
  kWaiting,                         // WAITING        TS_WAIT      in Object.wait()
  kWaitingForLockInflation,         // WAITING        TS_WAIT      blocked inflating a thin-lock
  kWaitingForTaskProcessor,         // WAITING        TS_WAIT      blocked waiting for taskProcessor
  kWaitingForGcToComplete,          // WAITING        TS_WAIT      blocked waiting for GC
  kWaitingForCheckPointsToRun,      // WAITING        TS_WAIT      GC waiting for checkpoints to run
  kWaitingPerformingGc,             // WAITING        TS_WAIT      performing GC
  kWaitingForDebuggerSend,          // WAITING        TS_WAIT      blocked waiting for events to be sent
  kWaitingForDebuggerToAttach,      // WAITING        TS_WAIT      blocked waiting for debugger to attach
  kWaitingInMainDebuggerLoop,       // WAITING        TS_WAIT      blocking/reading/processing debugger events
  kWaitingForDebuggerSuspension,    // WAITING        TS_WAIT      waiting for debugger suspend all
  kWaitingForJniOnLoad,             // WAITING        TS_WAIT      waiting for execution of dlopen and JNI on load code
  kWaitingForSignalCatcherOutput,   // WAITING        TS_WAIT      waiting for signal catcher IO to complete
  kWaitingInMainSignalCatcherLoop,  // WAITING        TS_WAIT      blocking/reading/processing signals
  kWaitingForDeoptimization,        // WAITING        TS_WAIT      waiting for deoptimization suspend all
  kWaitingForMethodTracingStart,    // WAITING        TS_WAIT      waiting for method tracing to start
  kWaitingForVisitObjects,          // WAITING        TS_WAIT      waiting for visiting objects
  kWaitingForGetObjectsAllocated,   // WAITING        TS_WAIT      waiting for getting the number of allocated objects
  kWaitingWeakGcRootRead,           // WAITING        TS_WAIT      waiting on the GC to read a weak root
  kWaitingForGcThreadFlip,          // WAITING        TS_WAIT      waiting on the GC thread flip (CC collector) to finish
  kStarting,                        // NEW            TS_WAIT      native thread started, not yet ready to run managed code
  kNative,                          // RUNNABLE       TS_RUNNING   running in a JNI native method
  kSuspended,                       // RUNNABLE       TS_RUNNING   suspended by GC or debugger
};
*** android-source/art/runtime/native/java_lang_Thread.cc
Thread.start()-->nativeCreate()-->Thread.CreateNativeThread()-->JNIEnvExt::Creat()-->pthread_create()-->child_thread invoke java Thread.run() method
*** JNIEnv implemented in android-source/art/runtime/jni_internal.cc
*** why wait need to be called in a synchronized block?
**** common case: one thread testes a state and wait, another thread changes a state and notify
**** so test and wait, as well as change and notify must be atomic
* x86_64
*** 函数传参寄存器顺序rdi, rsi, rdx, rcx, r8, r9，超过6个则压栈
*** callq会push %rip(return address aka.下一条指令)
*** %rax保存返回值
* gdb
*** p foo print foo
*** set foo = 123 set variable foo = 123
*** $sp $pc $fp 别名适用所有平台
*** x/i $pc 当前指令
*** x/32x $sp 显示stack内存信息
*** x/32x addr 显示addr开始的32个dword
*** objdump -dS elf显示汇编
*** disas/m 反汇编code
*** tui enable -- enable TUI display mode
* lag analyze tool
*** TraceView
  - Debug.startMethodTracing("sample") Debug.startMethodTracingSampling() Debug.stopMethodTracing()
*** Systrace
  - ./systrace.py sched freq idle am wm gfx view sync binder_driver irq workq input -b 96000
  - java framework: Trace.traceBegin(long traceTag, String methodName) Trace.traceEnd(long traceTag)
  - app: Trace.beginSection(String sectionName) Trace.endSection()
  - native: ATRACE_CALL();
*** 获取GC统计信息
**** - // GC 使用的总耗时，单位是毫秒
**** Debug.getRuntimeStat("art.gc.gc-time");
**** // 阻塞式 GC 的总耗时
**** Debug.getRuntimeStat("art.gc.blocking-gc-time");

* C++ mangle/demangle tool
*** c++filt -n _ZN7android6Tracer12sEnabledTagsE
* Hook
** inline hook
*** Substrate
**** MSHookFunction(void *symbol, void *replace, void **result)
- 作用：symbol：原函数地址，replace：hook函数地址，result：返回动态生成的代替原函数的指针，用于在hook函数中调用原来的逻辑
- 替换symbol的前几个指令，将其跳转到replace的首地址，replace中调用*result(mmap新的buffer，保存old function头部被替换的字节，尾部跳转到原函数未被替换字节的首地址)
** PLT/GOT hook
*** PLT(procedure linkage table) GOT(global offset table)
*** example:
- callq <printf@PLT> 查找GOT中相应记录，若没有加载printf地址，加载so库，修改GOT中printf记录地址为真实地址，后续调用直接调用GOT中真实地址
- trampoline code
*** dl_iterate_phdr
- walk through list of shared objects
*** facebook profilo iqiyi xHook
* vcpkg
*** visual stduio管理第三方libs
* unit test
*** 安全的重构代码
*** cmake && google test
*** gtest
**** assertions
| assertions | fatal | intercept |
|------------+-------+-----------|
| ASSERT_*   | YES   | YES       |
| EXPECT_*   | NO    | NO        |
**** fixtures
***** Using the Same Data Configuration for Multiple Tests
#+begin_src cpp
  // class Queue wanted to be tested
  template <typename E>  // E is the element type.
  class Queue {
   public:
    Queue();
    void Enqueue(const E& element);
    E* Dequeue();  // Returns NULL if the queue is empty.
    size_t size() const;
    ...
  };

  // test case fixture class
  class QueueTest : public ::testing::Test {
   protected:
    void SetUp() override {
       q1_.Enqueue(1);
       q2_.Enqueue(2);
       q2_.Enqueue(3);
    }

    // void TearDown() override {}

    Queue<int> q0_;
    Queue<int> q1_;
    Queue<int> q2_;
  };

  // tests
  TEST_F(QueueTest, IsEmptyInitially) {
    EXPECT_EQ(q0_.size(), 0);
  }

  TEST_F(QueueTest, DequeueWorks) {
    int* n = q0_.Dequeue();
    EXPECT_EQ(n, nullptr);

    n = q1_.Dequeue();
    ASSERT_NE(n, nullptr);
    EXPECT_EQ(*n, 1);
    EXPECT_EQ(q1_.size(), 0);
    delete n;

    n = q2_.Dequeue();
    ASSERT_NE(n, nullptr);
    EXPECT_EQ(*n, 2);
    EXPECT_EQ(q2_.size(), 1);
    delete n;
  }
#+end_src
**** running tests
***** TEST TEST_F 隐式注册到googletest，不需要显示指定需要运行哪些测试
***** RUN_ALL_TESTS()
* ASM
** event-based and tree-based api
** Parsing Generating Transforming class
** ClassReader ClassVisitor ClassWriter
** ClassWriter implemented ClassVisitor
*** visitXXX方法调用时会写入字节码数据
*** toByteArray返回记录的字节码数据
** ASMifier class -> java (generating class bytecode with ASM ClassWriter)
** visitor pattern
*** the visitor design pattern is a way of separating an algorithm from an object structure on which it operates
*** [[https://en.wikipedia.org/wiki/Visitor_pattern][wiki]]
* clojure
** cider
*** M-x cider-jack-in C-c M-j
* FPS tracer
** Choreographer.FrameCallback
* tracing activity startup
** reflect android.app.ActivityThread -> sCurrentActivityThread -> mH -> mCallback(hook with new one)
* ClassLoader
** locate or generate data that constitutes a definition for the class
** Class object contains a reference to the ClassLoader that defined it
** 数组对象的Class由JVM创建，非ClassLoader，且与其元素类型Class的ClassLoader相同；基本类型数据数组的Class无ClassLoader
** 代理加载机制，即先向父ClassLoader请求加载类，未找到则自己加载
** defineClass 将字节数组转换成Class对象
* gradle
** phase
*** init
**** load settings.properties
*** configure
*** execute
** projects
*** one build.gradle file per project
*** gradle project
**** list all projects
** configuration
*** configuration for dependencies of a project
**** annotationProcessor
**** apiElements
**** implementation
**** compile
**** runtime
**** testImplementation
**** etc
** resolutionStrategy
*** force
**** force a dependency when conflict occurred
** buildSrc
*** a included build
*** automatically compiles and tests this code and puts it in the classpath of your build script
*** configs of project dependencies and versions can use in build.gradle file
** gradle init --type java-application
** gradle jar
#+begin_src groovy
  jar {
      manifest {
          attributes("Main-Class": "App")
      }
  }

  task uberJar(type: Jar) {
      classifier = "all"
      from sourceSets.main.output
      manifest {
          attributes("Main-Class": "App")
      }

      dependsOn configurations.runtimeClasspath
      from {
          configurations.runtimeClasspath.findAll { it.name.endsWith('jar') }.collect { zipTree(it) }
      }

      with jar
  }
#+end_src

* PhontomReference
** 必须与引用队列一起使用，提供在finalize执行之后得到通知的机会，比如执行post-mortem清理机制
* line-oriented search tools
** the silver searcher
** git grep
** ripgrep
* linux process group and session group
** process group
*** 一组进程，具有相同的进程组id，用于向这个进程组发送信号，fork pipe创建的进程属于一个进程组
** session group
*** 多个进程组组成会话
*** 一个进程组不能从一个会话迁移到另外一个会话
*** 一个进程组只能属于一个会话
*** 一个进程不能创建属于其他会话的进程组
* daemon process
** fork()
*** 子进程不是一个进程组的组长进程,这为下面执行setsid创建新会话创建条件
** setsid()
*** 成为新会话的首进程
*** 成为新进程组的组长进程
*** 没有控制终端与之相连
** umask(0)
*** 防止继承得来的文件模式创建屏蔽字在创建文件时会拒绝设置某些权限
** close fds: STDIN_FILENO STDOUT_FILENO STDERR_FILENO
#+begin_src c
  int fd = open("/dev/null", O_RDWR);
  dup2(fd, STDIN_FILENO);
  dup2(fd, STDOUT_FILENO);
#+end_src

* UI
** smallestWidth适配
*** [https://mp.weixin.qq.com/s?__biz=MzAxMTI4MTkwNQ==&mid=2650826034&idx=1&sn=5e86768d7abc1850b057941cdd003927&chksm=80b7b1acb7c038ba8912b9a09f7e0d41eef13ec0cea19462e47c4e4fe6a08ab760fec864c777&scene=21#wechat_redirect]
*** dp = px / density density = DPI/160
** 今日头条计算density
* APK size
** proguard
*** Shrink、Optimize 和 Obfuscate，也就是裁剪、优化、混淆
** dex
*** facebook redex byte code optimizer
*** so file 7-zip XZ
** shrinkresources
*** Lint 提示无用的资源
*** shrinkResources true in gradle
**** 没有处理resources.arsc文件
**** 没有删除资源文件
**** R.java文件需要提前准备好，所有资源都分配了一个常量ID，编译Java代码过程，将代码中的资源引用替换成常量
* find duplicated number in array
** [http://keithschwarz.com/interesting/code/?dir=find-duplicate]
** 此问题等价于链表找环问题
* linked list cycle
** 判断是否有环
*** h t两个指针从起点S出发，t每前进1步，h前进2步，只要二者都可以前进而且没有相遇，就保持二者推进。
*** 当h无法前进，即到达某个没有后继节点时，可以确定从S出发没有环，反之当t和h再次相遇时，就可以确定从S出发一定会进入某个环，设其为环C
** 环的长度
*** 判断出存在环C时，t和h位于同一点，设其为节点M。显然，仅需令h不动，而t不断推进，最终又会回到节点M，统计这一次t推进的步数，即得到环的长度
** 环的起点
*** t从起点S到相遇点M走过的距离是环C长度的整数倍，因为h走过的距离比t走过的距离多环长度的整数倍，而h的速度是t的2倍
*** 令t回到起点S，同时让h从节点M共同推进，h和t都一次前进一步，当h和t再次相遇时，设此次相遇时位于同一节点P，则P即为从起点S出发所到达环C的第一个节点
* git submodule
** git submodule add ${url}
** git clone -> git submodule init -> git submodule update
* bookmark optimization
** #+DESCRIPTION: emacs lisp multibyte string
   #+BEGIN_SRC emacs-lisp
     (defun compare (string-a string-b)
       (cl-loop for a being the elements of string-a
                for b being the elements of string-b
                unless (eql a b)
                return (cons a b)))

     (benchmark-run
         (let ((a (make-string 100000 0))
               (b (make-string 100000 0)))
           (compare a b)))
     ;; => (0.012568031 0 0.0)

     (benchmark-run
         (let ((a (make-string 100000 0))
               (b (make-string 100000 0)))
           (setf (aref a (1- (length a))) 256)
           (compare a b)))
     ;; => (0.012680513 0 0.0)

     (benchmark-run
         (let ((a (make-string 100000 0))
               (b (make-string 100000 0)))
           (setf (aref a (1- (length a))) 256
                 (aref b (1- (length b))) 256)
           (compare a b)))
     ;; => (2.327959762 0 0.0)
   #+END_SRC
** To avoid the O(n) cost on this common indexing operating, Emacs keeps a “bookmark” for the last indexing location into a multibyte string. If the next access is nearby, it can starting looking from this bookmark, forwards or backwards.
* application binary interface
** an interface between two binary program modules, often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user.
** a common aspect of an ABI is the calling convention
** X86 calling convention
*** The order in which atomic parameters, or individual parts of a complex parameter, are allocated
*** How parameters are passed (pushed on the stack, placed in registers, or a mix of both)
*** Which rigisters the called function must preserve for the caller
*** How the task of preparing the stack for, and restoring after, a function call is divided between the caller and the callee
* sqlite3
** shell
*** dot command
**** .help
**** .width
**** .mode
**** .echo
**** .headers
**** .open
**** .cd
*** sql statement
**** Think of each SQL statement as a separate computer program.  The
**** original SQL text is source code.  A prepared statement object
**** is the compiled object code.  All SQL must be converted into a
**** prepared statement before it can be run.
*** The life-cycle of a prepared statement object usually goes like this:
**** 1. Create the prepared statement object using [sqlite3_prepare_v2()].
**** 2. Bind values to [parameters] using the sqlite3_bind_*() interfaces.
**** 3. Run the SQL by calling [sqlite3_step()] one or more times.
**** 4. Reset the prepared statement using [sqlite3_reset()] then go back
**** 5. to step 2.  Do this zero or more times.
**** 6. Destroy the object using [sqlite3_finalize()].
** Hash table
*** code snippet
     #+begin_src c
       struct Hash {
	 unsigned int htsize;      /* Number of buckets in the hash table */
	 unsigned int count;       /* Number of entries in this table */
	 HashElem *first;          /* The first element of the array */
	 struct _ht {              /* the hash table */
	   int count;                 /* Number of entries with this hash */
	   HashElem *chain;           /* Pointer to first entry with this hash */
	 } *ht;
       };

       struct HashElem {
	 HashElem *next, *prev;       /* Next and previous elements in the table */
	 void *data;                  /* Data associated with this element */
	 const char *pKey;            /* Key associated with this element */
       };
       
       /*
	** The hashing function.
	*/
       static unsigned int strHash(const char *z){
	 unsigned int h = 0;
	 unsigned char c;
	 while( (c = (unsigned char)*z++)!=0 ){     /*OPTIMIZATION-IF-TRUE*/
	   /* Knuth multiplicative hashing.  (Sorting & Searching, p. 510).
	   ** 0x9e3779b1 is 2654435761 which is the closest prime number to
	   ** (2**32)*golden_ratio, where golden_ratio = (sqrt(5) - 1)/2. */
	   h += sqlite3UpperToLower[c];
	   h *= 0x9e3779b1;
	 }
	 return h;
	}
    #+end_src
*** All elements of the hash table are on a single doubly-linked list.
*** Hash.first points to the head of this list.
*** There are Hash.htsize buckets.  Each bucket points to a spot in the global doubly-linked list.
*** The contents of the bucket are the element pointed to plus the next _ht.count-1 elements in the list.
*** Hash.htsize and Hash.ht may be zero.  In that case lookup is done by a linear search of the global list. 
*** For small tables, the Hash.ht table is never allocated because if there are few elements in the table, it is faster to do a linear search than to manage the hash table.
** lemon parser
*** similar to bison yacc
*** grammr file parse.y
*** token(sqlte3GetToken()) -> parse(sqlite3Parser()) -> prepared Vdbe(in Parse context)
*** sqlite3_stmt == Vdbe
** prepare
*** -> sqlite3_prepare_v2
*** -> sqlite3LockAndPrepare
*** -> sqlite3Prepare
*** -> sqlite3RunParser
*** -> while(1) { sqlite3GetToken; sqlite3Parser; }
** step
*** -> sqlite3_step
*** -> sqlite3VdbeExec
**** big switch( pOp->opcode )
** atomic commit
*** rollback journal file
**** single file commit
***** acquiring a shared lock
****** allows two or more database connections read at the same time, prevent another connection from writing while we are reading it
***** reading information out of the database
****** reading from mass storage into os cache, then transferred from os cache into user space
***** obtaining a reserved lock
****** allows to read, but there can only be a single reserved lock on the database file
****** it signals that a process intends to modify the database file in the near future but has not yet started to make the modifications
***** creating a rollback journal file
****** write the original content of the database pages that are to altered into a rollback journal file
****** it contains all the information needed to restore the database back to its original state before the transaction
***** changing database pages in user space
****** each connection has its own private copy of user space, so the changes are only visible to the database connection that is making the changes
***** flushing the rollback journal file to mass storage
****** this is a critical step in ensuring that the database can survive an unexpected power loss
***** obtaining an exclusive lock
****** first obtains a pending lock, then it escalates the pending lock to an exclusive lock
****** a pending lock allows other processes that already have a shared lock to continue reading the database file, but it prevents new share lock from being established
****** the idea behind the pending lock is to prevent writer starvation caused by a large pool of readers
****** evetually all shared locks will be clear and the pending lock will then be able to escalate into an exclusive lock
***** writing changes to database file
****** changes only go as far as the system cache
***** flushing changes to mass storage
***** deleting the rollback journal file
****** SQLite gives the apprearance of having made no changes to the database file or having made the complete set of changes to the database file depending on whether or not the rollback journal file exists
***** releasing the lock
**** rollback
***** hot rollback journals
****** The rollback journal exists.
****** The rollback journal is not an empty file.
****** There is no reserved lock on the main database file.
****** The header of the rollback journal is well-formed and in particular has not been zeroed out.
****** The rollback journal does not contain the name of a master journal file (see section 5.5 below) or if does contain the name of a master journal, then that master journal file exists.
***** obtaining an exlusive lock
***** rolling back incomplete changes
***** deleting the hot journal
***** continue as if the uncompleted writes has never happened 
*** write-ahead logging(wal) mode
**** journal approach
***** The traditional rollback journal works by writing a copy of the original database content into a separate rollback journal file and then writing
***** changes directly into the original database file. In the event of a crash or ROLLBACK, the original content contained in the rollback journal is
***** played back into the database file to revert the database file to its original state. The COMMIT occurs when the rollback journal is deleted. 
**** journal approach
***** The WAL approach inverts this. The original content is preserved in the database file and the changes are appended into a separate WAL file. A 
***** COMMIT occurs when a special record indicating a commit is appended to the WAL. Thus a COMMIT can happen without ever writing to the original database
***** file, which allows readers to continue operating from the original unaltered database while changes are simultaneously being committed into the WAL.
***** Multiple transactions can be appened to the end of a single WAL file.
** sql tips
*** a single column with type (INTEGER PRIMARY KEY) is an alias for rowid(all rows within SQLite tables have a 64-bit signed integer key that identifies the row within its table)
*** column with INTEGER PRIMARY KEY is used as the rowid, and Table.iPKey is set to be the index of the column, -1 by default
*** if the key is not an INTEGER PRIMARY KEY, then create a UNIQUE index for the key, No index is created for INTEGER PRIMARY KEYs 
*** foreign key requires parent key columns must be subject to a UNIQUE constraint or have a UNIQUE index
*** An index should be created on the child key columns of each foreign key constraint, because each time an application deletes a row from the parent table, it performs a searching for referencing rows in the child table
*** ON UPDATE CASCADE or ON DELETE CASCADE means doing the same action on child key columns which is similar to trigger
*** any column in an SQLite3 database, except an INTEGER PRIMARY KEY column, may be used to store a value of any storage class, it is just that some columns, given a choice, will prefer to use one storage over another(aka. type affinity) 
*** INSERT OR IGNORE == INSERT ON CONFICT IGNORE
*** COLLATE NOCASE means ignore case when used in select or where statements
*** a default value of a column may be CURRENT_TIME, CURRENT_DATE, CURRENT_TIMESTAMP
** misc
*** db at index 0 is "main", db at index 1 is "temp"
*** column count limit in a table is 2000 by default
** Robson proof
*** N	The amount of raw memory needed by the memory allocation system in order to guarantee that no memory allocation will ever fail.
*** M	The maximum amount of memory that the application ever has checked out at any point in time.
*** n	The ratio of the largest memory allocation to the smallest. We assume that every memory allocation size is an integer multiple of the smallest memory allocation size.
*** N = M*(1 + (log2 n)/2) - n + 1
* c/c++ tips
** assert(argv[argc] == null)
** -DNDEBUG disable assert
** oop in c
*** define a strcut of class which contains constructor, destructor, etc, describing the class infomation 
*** a object is void *obj which is created using constructor in struct class
*** object has a pointer points to the struct class
*** analogy to Java
*** code snippets
    #+begin_src c
    void * new (const void * _class, ...)
    { 
      const struct Class * class = _class;
      void * p = calloc(1, class —> size);
      assert(p);
      *(const struct Class **) p = class;
      if (class —> ctor)
      { 
	va_list ap;
	va_start(ap, _class);
	p = class —> ctor(p, & ap);
	va_end(ap);
      }
      return p;
    }
    #+end_src
** we need to pass size param to malloc, then why not to call free?
*** malloc allocate a bit more memory than you asked for, this extra memory is used to store information such as the size of the allocated block
*** and a link to the next free block in a chain of blocks
*** and sometimes the "guard data" that helps the system to detect if you past the end of the allocated block
*** usually, most allocators will round up the size and/or the start of the block to a multiple of bytes such as 64bit in a 64-bit system
** c struct alignment rules
*** address of each member = 0 (mod sizeof(each member))
*** sizeof(struct) = 0 (mod sizeof(largest member))
*** char and char[] have no padding between them
** LD_PRELOAD environment variable could load your library before any other ones aka. program -> your library -> destination library
** Explicitly call the 64-bit version of lseek() on Android. Otherwise, lseek() is the 32-bit version, even if _FILE_OFFSET_BITS=64 is defined.
** fstat obtain information about an open file, such as owner, permission, size, file type symbolic directory socket character etc
** fork vs clone
*** fork create a new child process with 'copy-on-write' machanism, which executes in the child process from the point of the fork call
*** clone allows the child process to share parts of its execution context with the calling process, such as the virtual address space, the table of file descriptors, and the table of signal handlers
** /dev/random /dev/urandom are character files provide interface to system random generator
** memory barrier
*** asm volatile("" ::: "memory") compile-time memory barrier
*** __sync_synchronize runtime(HW) memory barrier
** new operator and operator new
*** operator new can be called explicitly as a regular function, 
*** but in C++, new is an operator with a very specific behavior: 
*** An expression with the new operator, 
*** first calls function operator new (i.e., this function) with the size of its type specifier as first argument,
*** and if this is successful, it then automatically initializes or constructs the object (if needed).
*** Finally, the expression evaluates as a pointer to the appropriate type.
*** placement new is constructing new object in a known address
** valgrind
*** leak checks benchmarks
** readelf and objdump
*** objdump -dC main.out
** smart pointer
*** unique_ptr create a object which take over the destruction of the other object
** delete is null-pointer safe
** malloc
*** block = mem_control_block + data
*** sbrk to expand head space
*** jemalloc in bionic
**** arena
**** thread cache
*** code
    #+begin_src c
    /**
  * @brief Dynamic distribute memory function
  * @param numbytes: what size you need   
  * @retval a void pointer to the distribute first address
  */ 
void * malloc(unsigned int numbytes)
{
    unsigned int current_location,otherbck_location;
    /* This is the same as current_location, but cast to a memory_control_block */
    mem_control_block * current_location_mcb = NULL,* other_location_mcb = NULL;
    /* varialbe for saving return value and be set to 0 until we find something suitable */
    void * memory_location = NULL;
    /* current dividing block size */
    unsigned int process_blocksize;
    
    /* Initialize if we haven't already done so */
    if(! has_initialized) {
        malloc_init();
    }
    
    /* Begin searching at the start of managed memory */
    current_location = managed_memory_start;
    /* Keep going until we have searched all allocated space */
    while(current_location != managed_memory_end){
        /* current_location and current_location_mcb point to the same address.  However, 
         * current_location_mcb is of the correct type, so we can use it as a struct. current_location 
         * is a void pointer so we can use it to calculate addresses.
         */
        current_location_mcb = (mem_control_block *)current_location;
        /* judge whether current block is avaiable */
        if(current_location_mcb->is_available){
            /* judge whether current block size exactly fit for the need */
            if((current_location_mcb->current_blocksize == numbytes)){
                /* It is no longer available */ 
                current_location_mcb->is_available = 0;            
                /* We own it */
                memory_location = (void *)(current_location + sizeof(mem_control_block));
                /* Leave the loop */
                break;
            /* judge whether current block size is enough for dividing a new block */
            }else if(current_location_mcb->current_blocksize >= numbytes + sizeof(mem_control_block)){
                /* It is no longer available */ 
                current_location_mcb->is_available = 0;
                /* because we will divide current blcok,before we changed current block size,we should
                 * save the integral size.
                 */
                process_blocksize = current_location_mcb->current_blocksize;
                /* Now blcok size could be changed */
                current_location_mcb->current_blocksize = numbytes;
                
                /* find the memory_control_block's head of remaining block and set parameter,block of no
                 * parameter can't be managed. 
                 */
                other_location_mcb = (mem_control_block *)(current_location + numbytes \
                                                + sizeof(mem_control_block));
                /* the remaining block is still avaiable */
                other_location_mcb->is_available = 1;
                /* of course,its prior block size is numbytes */
                other_location_mcb->prior_blocksize = numbytes;
                /* its size should get small */
                other_location_mcb->current_blocksize = process_blocksize - numbytes \
                                                - sizeof(mem_control_block);
                
                /* find the memory_control_block's head of block after current block and \
                 * set parameter--prior_blocksize. 
                 */
                otherbck_location = current_location + process_blocksize \
                                            + sizeof(mem_control_block);                
                /* We need check wehter this block is on the edge of managed memeory! */
                if(otherbck_location != managed_memory_end){
                    other_location_mcb = (mem_control_block *)(otherbck_location);
                    /*  its prior block size has changed! */
                    other_location_mcb->prior_blocksize = process_blocksize\
                        - numbytes - sizeof(mem_control_block);
                }
                /*We own the occupied block ,not remaining block */ 
                memory_location = (void *)(current_location + sizeof(mem_control_block));
                /* Leave the loop */
                break;
            } 
        }
        /* current block is unavaiable or block size is too small and move to next block*/
        current_location += current_location_mcb->current_blocksize \
                                    + sizeof(mem_control_block);
    }
    /* if we still don't have a valid location,we'll have to return NULL */
    if(memory_location == NULL)    {
        return NULL;
    }
    /* return the pointer */
    return memory_location;    
}

/**
  * @brief  free your unused block 
  * @param  firstbyte: a pointer to first address of your unused block
  * @retval None
  */ 
void free(void *firstbyte) 
{
    unsigned int current_location,otherbck_location;
    mem_control_block * current_mcb = NULL,* next_mcb = NULL,* prior_mcb \
                                = NULL,* other_mcb = NULL;
    /* Backup from the given pointer to find the current block */
    current_location = (unsigned int)firstbyte - sizeof(mem_control_block);
    current_mcb = (mem_control_block *)current_location;
    /* Mark the block as being avaiable */
    current_mcb->is_available = 1;
    
    /* find next block location */
    otherbck_location = current_location + sizeof(mem_control_block) \
                                    + current_mcb->current_blocksize;
    /* We need check wehter this block is on the edge of managed memeory! */
    if(otherbck_location != managed_memory_end){
        /* point to next block */
        next_mcb = (mem_control_block *)otherbck_location;
        /* We need check whether its next block is avaiable */ 
        if(next_mcb->is_available){
            /* Because its next block is also avaiable,we should merge blocks */
            current_mcb->current_blocksize = current_mcb->current_blocksize \
                + sizeof(mem_control_block) + next_mcb->current_blocksize;
            
            /* We have merge two blocks,so we need change prior_blocksize of
             * block after the two blocks,just find next block location. 
             */
            otherbck_location = current_location + sizeof(mem_control_block) \
                                    + current_mcb->current_blocksize;
            /* We need check wehter this block is on the edge of managed memeory! */
            if(otherbck_location != managed_memory_end){
                other_mcb = (mem_control_block *)otherbck_location;
                /*  its prior block size has changed! */
                other_mcb->prior_blocksize = current_mcb->current_blocksize;
            }
        }
    }
    
    /* We need check wehter this block is on the edge of managed memeory! */
    if(current_location != managed_memory_start){
        /* point to prior block */
        prior_mcb = (mem_control_block *)(current_location - sizeof(mem_control_block)\
                                            - current_mcb->prior_blocksize);
        /* We need check whether its prior block is avaiable */ 
        if(prior_mcb->is_available){
            /* Because its prior block is also avaiable,we should merge blocks */
            prior_mcb->current_blocksize = prior_mcb->current_blocksize \
                + sizeof(mem_control_block) + current_mcb->current_blocksize;
            
            /* We have merge two blocks,so we need change prior_blocksize of
             * block after the two blocks,just find next block location. 
             */
            otherbck_location = current_location + sizeof(mem_control_block) \
                                    + current_mcb->current_blocksize;
            /* We need check wehter this block is on the edge of managed memeory! */
            if(otherbck_location != managed_memory_end){
                other_mcb = (mem_control_block *)otherbck_location;
                /*  its prior block size has changed! */
                other_mcb->prior_blocksize = prior_mcb->current_blocksize;
            }
        }
    }
}
    #+end_src
** wait_queue
*** schedule make process hang
**** important step 1: pick_next_task pick from sched_class
**** sched_class: rt_sched_class > fair_sched_class > idle_sched_class
**** important step 2: context_switch
***** switch_mm
***** switch_to
      #+begin_src c
 /*
  * context_switch - switch to the new MM and the new thread's register state.
  */
 static __always_inline struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
		struct task_struct *next, struct rq_flags *rf)
 {
	 prepare_task_switch(rq, prev, next);

	 /*
	  * For paravirt, this is coupled with an exit in switch_to to
	  * combine the page table reload and the switch backend into
	  * one hypercall.
	  */
	 arch_start_context_switch(prev);

	 /*
	  * kernel -> kernel   lazy + transfer active
	  *   user -> kernel   lazy + mmgrab() active
	  *
	  * kernel ->   user   switch + mmdrop() active
	  *   user ->   user   switch
	  */
	 if (!next->mm) {                                // to kernel
		 enter_lazy_tlb(prev->active_mm, next);

		 next->active_mm = prev->active_mm;
		 if (prev->mm)                           // from user
			 mmgrab(prev->active_mm);
		 else
			 prev->active_mm = NULL;
	 } else {                                        // to user
		 membarrier_switch_mm(rq, prev->active_mm, next->mm);
		 /*
		  * sys_membarrier() requires an smp_mb() between setting
		  * rq->curr / membarrier_switch_mm() and returning to userspace.
		  *
		  * The below provides this either through switch_mm(), or in
		  * case 'prev->active_mm == next->mm' through
		  * finish_task_switch()'s mmdrop().
		  */
		 switch_mm_irqs_off(prev->active_mm, next->mm, next);

		 if (!prev->mm) {                        // from kernel
			 /* will mmdrop() in finish_task_switch(). */
			 rq->prev_mm = prev->active_mm;
			 prev->active_mm = NULL;
		 }
	 }

	 rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

	 prepare_lock_switch(rq, next, rf);

	 /* Here we just switch the register state and the stack. */
	 switch_to(prev, next, prev);
	 barrier();

	 return finish_task_switch(prev);
 }
      #+end_src
**** code
     #+begin_src c
static void __sched notrace __schedule(bool preempt)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;

	schedule_debug(prev, preempt);

	if (sched_feat(HRTICK))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up().
	 *
	 * The membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;
	if (!preempt && prev->state) {
		if (signal_pending_state(prev->state, prev)) {
			prev->state = TASK_RUNNING;
		} else {
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf);
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();

	if (likely(prev != next)) {
		rq->nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq->curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq->curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		trace_sched_switch(preempt, prev, next);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf);
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);
	}

	balance_callback(rq);
}
     #+end_src
*** wake_up_process
*** sleep_on interruptible_sleep_on
*** wake_up wake_up_interruptible
** memeory management
*** buddy
**** __get_free_pages
*** slab
**** kmem_cache_alloc
**** high speed cache
***** kmem_cache
**** cache -> slab -> object
**** task_struct filp
**** /proc/slabinfo
*** do_page_fault
** fd -> file
*** struct file
    #+begin_src c
    // in struct task_struct
    ..
    /* Open file information: */
    struct files_struct		*files;

    /*
     * Open file table structure
     */
    struct files_struct {
      /*
       * read mostly part
       */
	    atomic_t count;
	    bool resize_in_progress;
	    wait_queue_head_t resize_wait;

	    struct fdtable __rcu *fdt;
	    struct fdtable fdtab;
      /*
       * written part on a separate cache line in SMP
       */
	    spinlock_t file_lock ____cacheline_aligned_in_smp;
	    unsigned int next_fd;
	    unsigned long close_on_exec_init[1];
	    unsigned long open_fds_init[1];
	    unsigned long full_fds_bits_init[1];
	    struct file __rcu * fd_array[NR_OPEN_DEFAULT];
    };

    struct fdtable {
	unsigned int max_fds;
	struct file __rcu **fd;      /* current fd array */ // fd is index in the fd array
	unsigned long *close_on_exec;
	unsigned long *open_fds;
	unsigned long *full_fds_bits;
	struct rcu_head rcu;
    };
    #+end_src

** fork
*** ref[https://blog.csdn.net/liushengxi_root/article/details/81332740]
** c++ four kinds of explicit type conversion
*** static_cast
**** 
*** dynamic_cast
**** ensuring safe downcast
***** If the cast is successful, dynamic_cast returns a value of type new_type.
***** If the cast fails and new_type is a pointer type, it returns a null pointer of that type.
***** If the cast fails and new_type is a reference type, it throws an exception that matches a handler of type std::bad_cast
*** reinterpret_cast
**** long -> pointer or pointer -> long
*** const_cast
**** remove or add `const` `volatile` attributes
** calc number of args with macro in glibc
*** INLINE_SYSCALL
*** code
    #+begin_src c
    #define __nargs(a,b,c,d,e,f,g,h,n) n
    #define nargs(...) __nargs(VARGS, 7, 6, 5, 4, 3, 2, 1)
    #+end_src
** template meta programming
*** synopsis
**** using type specialization for branch chosing
**** using template arguments for calculation steps
**** using type matching for function overloads
*** check if any type
**** type specialization
*** check if type has any function
**** SFINAE
**** check return type using `decltype` `declval`
**** "," expression
*** non true
**** enable_if
* linux kernel
** start_kernel
*** trap_init
**** idt_setup_traps
***** register IDT(interrupt description table)
      #+begin_src c
      static const __initconst struct idt_data def_idts[] = {
	INTG(X86_TRAP_DE,		divide_error),
	INTG(X86_TRAP_NMI,		nmi),
	INTG(X86_TRAP_BR,		bounds),
	INTG(X86_TRAP_UD,		invalid_op),
	INTG(X86_TRAP_NM,		device_not_available),
	INTG(X86_TRAP_OLD_MF,		coprocessor_segment_overrun),
	INTG(X86_TRAP_TS,		invalid_TSS),
	INTG(X86_TRAP_NP,		segment_not_present),
	INTG(X86_TRAP_SS,		stack_segment),
	INTG(X86_TRAP_GP,		general_protection),
	INTG(X86_TRAP_SPURIOUS,		spurious_interrupt_bug),
	INTG(X86_TRAP_MF,		coprocessor_error),
	INTG(X86_TRAP_AC,		alignment_check),
	INTG(X86_TRAP_XF,		simd_coprocessor_error),

#ifdef CONFIG_X86_32
	TSKG(X86_TRAP_DF,		GDT_ENTRY_DOUBLEFAULT_TSS),
#else
	INTG(X86_TRAP_DF,		double_fault),
#endif
	INTG(X86_TRAP_DB,		debug),

#ifdef CONFIG_X86_MCE
	INTG(X86_TRAP_MC,		&machine_check),
#endif

	SYSG(X86_TRAP_OF,		overflow),
#if defined(CONFIG_IA32_EMULATION)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_compat),
#elif defined(CONFIG_X86_32)
	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
#endif
};
      #+end_src
***** interrupt handler of syscall: entry_INT80_32
****** save context registers to pt_regs struct
****** code
#+begin_src c
       #ifdef __i386__
       struct pt_regs {
	 unsigned long bx;
	 unsigned long cx;
	 unsigned long dx;
	 unsigned long si;
	 unsigned long di;
	 unsigned long bp;
	 unsigned long ax;
	 unsigned long ds;
	 unsigned long es;
	 unsigned long fs;
	 unsigned long gs;
	 unsigned long orig_ax;
	 unsigned long ip;
	 unsigned long cs;
	 unsigned long flags;
	 unsigned long sp;
	 unsigned long ss;
       };
       #else 
       struct pt_regs {
	 unsigned long r15;
	 unsigned long r14;
	 unsigned long r13;
	 unsigned long r12;
	 unsigned long bp;
	 unsigned long bx;
	 unsigned long r11;
	 unsigned long r10;
	 unsigned long r9;
	 unsigned long r8;
	 unsigned long ax;
	 unsigned long cx;
	 unsigned long dx;
	 unsigned long si;
	 unsigned long di;
	 unsigned long orig_ax;
	 unsigned long ip;
	 unsigned long cs;
	 unsigned long flags;
	 unsigned long sp;
	 unsigned long ss;
       /* top of stack page */
       };
       #endif
       #+end_src

** syscall
*** `int 0x80` or `syscall` instruction
*** entry_INT80_32 or entry_SYSCALL_64
**** save user space regs to pt_regs
**** do_syscall_64 -> x32_sys_call_table[nr](regs);
*** open
**** 
*** exit_to_usermode_loop()
**** _TIF_NEED_RESCHED -> schedule()
** interrupt
*** per_cpu idt_table
**** 0-31 system intr and 0x80 syscall intr
**** others are device intr
*** device interrupt
**** irq_entries_table -> common_interrupt -> do_IRQ -> ret_from_intr
** task_struct
*** categories
**** id
     #+begin_src c
     pid_t pid;
     pid_t tgid; // pid of thread group leader
     struct task_struct *group_leader;
     #+end_src
**** status
     #+begin_src c
     volatile long state; // TASK_RUNNING, TASK_INTERRUPTIBLE, TASK_UNINTERUPTIBLE
     int exit_state;
     unsigned int flags;

     /* Used in tsk->state: */
     #define TASK_RUNNING			0x0000
     #define TASK_INTERRUPTIBLE		        0x0001
     #define TASK_UNINTERRUPTIBLE		0x0002
     #define __TASK_STOPPED			0x0004
     #define __TASK_TRACED			0x0008
     /* Used in tsk->exit_state: */
     #define EXIT_DEAD			        0x0010
     #define EXIT_ZOMBIE			0x0020
     #define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
     /* Used in tsk->state again: */
     #define TASK_PARKED			0x0040
     #define TASK_DEAD			        0x0080
     #define TASK_WAKEKILL			0x0100
     #define TASK_WAKING			0x0200
     #define TASK_NOLOAD			0x0400
     #define TASK_NEW			        0x0800
     #define TASK_STATE_MAX			0x1000

     /* Convenience macros for the sake of set_current_state: */
     #define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
     #define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)
     #define TASK_TRACED			(TASK_WAKEKILL | __TASK_TRACED)

     #define TASK_IDLE			        (TASK_UNINTERRUPTIBLE | TASK_NOLOAD)

     /* Convenience macros for the sake of wake_up(): */
     #define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)

     /* get_task_state(): */
     #define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \
					      TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
					      __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
					      TASK_PARKED)
	    #+end_src
**** schedule
     #+begin_src c
     //是否在运行队列上
     int on_rq;
     //优先级
     int prio;
     int static_prio;
     int normal_prio;
     unsigned int rt_priority;
     //调度器类
     const struct sched_class *sched_class;
     //调度实体
     struct sched_entity se;
     struct sched_rt_entity rt;
     struct sched_dl_entity dl;
     //调度策略
     unsigned int policy;
     //可以使用哪些CPU
     int nr_cpus_allowed;
     cpumask_t cpus_allowed;
     
     struct sched_info sched_info;
     
     #+end_src
**** signal
     #+begin_src c
     /* Signal handlers: */
     struct signal_struct *signal; // signal->shared_pending is process signal set
     struct sighand_struct *sighand;
     sigset_t blocked;
     sigset_t real_blocked;
     sigset_t saved_sigmask;
     struct sigpending pending; // signal set of current thread
     unsigned long sas_ss_sp;
     size_t sas_ss_size;
     unsigned int sas_ss_flags;    
     #+end_src

**** running statistics
     #+begin_src c
     u64        utime;//用户态消耗的CPU时间
     u64        stime;//内核态消耗的CPU时间
     unsigned long      nvcsw;//自愿(voluntary)上下文切换计数
     unsigned long      nivcsw;//非自愿(involuntary)上下文切换计数
     u64        start_time;//进程启动时间，不包含睡眠时间
     u64        real_start_time;//进程启动时间，包含睡眠时间
     #+end_src
**** process affinity
     #+begin_src c     
     struct task_struct __rcu *real_parent; /* real parent process */
     struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */
     struct list_head children;      /* list of my children */
     struct list_head sibling;       /* linkage in my parent's children list */
     #+end_src
**** credential
     #+begin_src c
     /* Objective and real subjective task credentials (COW): */
     const struct cred __rcu         *real_cred;
     /* Effective (overridable) subjective task credentials (COW): */
     const struct cred __rcu         *cred;
     
     struct cred {
	atomic_t	usage;
#ifdef CONFIG_DEBUG_CREDENTIALS
	atomic_t	subscribers;	/* number of processes subscribed */
	void		*put_addr;
	unsigned	magic;
#define CRED_MAGIC	0x43736564
#define CRED_MAGIC_DEAD	0x44656144
#endif
	kuid_t		uid;		/* real UID of the task */
	kgid_t		gid;		/* real GID of the task */
	kuid_t		suid;		/* saved UID of the task */
	kgid_t		sgid;		/* saved GID of the task */
	kuid_t		euid;		/* effective UID of the task */
	kgid_t		egid;		/* effective GID of the task */
	kuid_t		fsuid;		/* UID for VFS ops */
	kgid_t		fsgid;		/* GID for VFS ops */
	unsigned	securebits;	/* SUID-less security management */
	kernel_cap_t	cap_inheritable; /* caps our children can inherit */
	kernel_cap_t	cap_permitted;	/* caps we're permitted */
	kernel_cap_t	cap_effective;	/* caps we can actually use */
	kernel_cap_t	cap_bset;	/* capability bounding set */
	kernel_cap_t	cap_ambient;	/* Ambient capability set */
#ifdef CONFIG_KEYS
	unsigned char	jit_keyring;	/* default keyring to attach requested
					 * keys to */
	struct key	*session_keyring; /* keyring inherited over fork */
	struct key	*process_keyring; /* keyring private to this process */
	struct key	*thread_keyring; /* keyring private to this thread */
	struct key	*request_key_auth; /* assumed request_key authority */
#endif
#ifdef CONFIG_SECURITY
	void		*security;	/* subjective LSM security */
#endif
	struct user_struct *user;	/* real user ID subscription */
	struct user_namespace *user_ns; /* user_ns the caps and keyrings are relative to. */
	struct group_info *group_info;	/* supplementary groups for euid/fsgid */
	/* RCU deletion */
	union {
		int non_rcu;			/* Can we skip RCU deletion? */
		struct rcu_head	rcu;		/* RCU deletion hook */
	};
} __randomize_layout;
     #+end_src
**** memory management
     #+begin_src c
     struct mm_struct                *mm;
     struct mm_struct                *active_mm;
     #+end_src
**** file system and opened files
     #+begin_src c
     /* Filesystem information: */
     struct fs_struct                *fs;
     /* Open file information: */
     struct files_struct             *files;
     #+end_src
*** current pointer in kernel code
**** points to the current task which invokes the system call
**** /home/jocoo/d/linux-5.4/include/asm-generic/current.h
     #+begin_src c
     #define get_current() (current_thread_info()->task)
     #define current get_current()
     #+end_src
**** /home/jocoo/d/linux-5.4/arch/arm/include/asm/thread_info.h
     #+begin_src c
     static inline struct thread_info *current_thread_info(void) __attribute_const__;

     static inline struct thread_info *current_thread_info(void)
     {
        return (struct thread_info *)
           (current_stack_pointer & ~(THREAD_SIZE - 1));
     }
     #+end_src
*** task switch
**** schedule()
**** __switch_to(struct task_srtuct *prev_p, struct task_struct *next_p)
***** read current task pointer from PerCPU variable 
      #+begin_src c
      __visible __notrace_funcgraph struct task_struct *
      __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
      {
      //......
      this_cpu_write(current_task, next_p);
      //......
      return prev_p;
      }

      DECLARE_PER_CPU(struct task_struct *, current_task);
      #define this_cpu_read_stable(var)       percpu_stable_op("mov", var)
      
      static __always_inline struct task_struct *get_current(void){ 
        return this_cpu_read_stable(current_task);
      }
      #+end_src
*** task kernel stack
    #+begin_src c
    // include/linux/sched.h
    union thread_union {
#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK
	struct task_struct task;
#endif
#ifndef CONFIG_THREAD_INFO_IN_TASK
	struct thread_info thread_info;
#endif
	unsigned long stack[THREAD_SIZE/sizeof(long)];
};
    #+end_src
*** mm_struct
**** vm_area_struct
***** vm_start vm_end
**** count
***** count > 0 means lightweight process
** scheduler
*** category
**** stop_sched_class
**** dl_sched_class
**** rt_sched_class
**** fair_sched_class
**** idle_sched_class
*** diagram
   +--------------------+        +---------------------------------+
   |struct rq {//perCPU |        |struct cfs_rq {                  |                         +-------+
   |  struct cfs_rq cfs;+------->+  strcut rb_root tasks_timesline;+------------------------>+rb_node|
   |  struct rt_rq rt;  |        |  struct rb_node *rb_leftmost;   +----------+             X+-------+X
   |  struct dl_rq dl;  |        |}                                |          |            X           X
   |}                   |        +---------------------------------+          |           X             X
   +--------------------+                                                     |      +-------+        +-------+
									      |      |rb_node|        |rb_node|
									      |     X+------X+        +---X--- X
									      |    X        X             X     X
									      v   X         X             X      X
									   +--+----+     +-------+    +-------+   +-------+
									   |rb_node|     |rb_node|    |rb_node|   |rb_node|
									   +-------+     +-------+    +-------+   +-+-----+
														    ^
														    |
														    |
														    |
				   +-----------------------------+               +--------------------------+       |
				   |struct task_struct {         |               |struct sched_entity {     |       |
				   |  struct sched_entity se;    +-------------->+  struct rb_node run_node;+-------+
				   |  struct sched_rt_entity rt; |               |  u64 vruntime;           |
				   |}                            |               |}                         |
				   +-----------------------------+               +--------------------------+
*** schedule()
**** pick_next_task(rq, prev, &rf)
**** context_switch()
***** switch mm_struct
***** switch_to()
****** save %esp to TASK_threadsp(prev), restore %esp from TASK_threadsp(next)
****** __switch_to_asm()->__switch_to()
******* this_cpu_write(current_task, next_p)
******* load_sp0(tss, next) //restore to TSS
***** finish_task_switch(prev)
*** preempt
**** set flag TIF_NEED_RESCHED
***** scheduler_tick
***** try_to_wake_up
**** schedule time
***** exit from syscall to userspace; syscall_return_slowpath -> exit_to_usermode_loop() -> schedule()
***** exit from interrupt to userspace; do_IRQ() -> retint_user -> schedule()
***** exit from interrupt to kernel space; do_IRQ() -> retint_kernel -> schedule()
***** kernel space enable preempt; preempt_enable() -> schedule()
**** code
     #+begin_src c
     static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
     {
	     /*
	      * In order to return to user mode, we need to have IRQs off with
	      * none of EXIT_TO_USERMODE_LOOP_FLAGS set.  Several of these flags
	      * can be set at any time on preemptible kernels if we have IRQs on,
	      * so we need to loop.  Disabling preemption wouldn't help: doing the
	      * work to clear some of the flags can sleep.
	      */
	     while (true) {
		     /* We have work to do. */
		     local_irq_enable();

		     if (cached_flags & _TIF_NEED_RESCHED)
			     schedule();

		     if (cached_flags & _TIF_UPROBE)
			     uprobe_notify_resume(regs);

		     if (cached_flags & _TIF_PATCH_PENDING)
			     klp_update_patch_state(current);

		     /* deal with pending signal delivery */
		     if (cached_flags & _TIF_SIGPENDING)
			     do_signal(regs);

		     if (cached_flags & _TIF_NOTIFY_RESUME) {
			     clear_thread_flag(TIF_NOTIFY_RESUME);
			     tracehook_notify_resume(regs);
			     rseq_handle_notify_resume(NULL, regs);
		     }

		     if (cached_flags & _TIF_USER_RETURN_NOTIFY)
			     fire_user_return_notifiers();

		     /* Disable IRQs and retry */
		     local_irq_disable();

		     cached_flags = READ_ONCE(current_thread_info()->flags);

		     if (!(cached_flags & EXIT_TO_USERMODE_LOOP_FLAGS))
			     break;
	     }
     }
     #+end_src
*** update_curr
**** update current task vruntime
** fork
*** copy_process
**** dup_task_struct
**** copy_files copy_fs copy_sighand copy_signal copy_mm
*** wake_up_new_task
**** set TIF_NEED_RESCHED
**** exit_to_usermode_loop() -> schedule()
** pthread_create
*** create pthread stack in heap(with mmap) of userspace with pthread and tls and guard-space configed
*** clone syscall
*** copy_process
**** dup_task_struct
*** call `start_thread` as a trampoline to call `start_routine` of thread with user-defined arg
** signal
*** [1,31] non-rt signal [32, x] real-time signal
*** kill -> process
**** task_struct -> signal_struct -> struct sigpending `shared_pending` shared by all threads in the process
*** tgkill -> thread
**** task_struct -> `sigpending` stores signal data for thread
*** exit_to_usermode_loop() check and handle signal
*** creating frame in userspace stack for sigaction then return to kernel space with sigreturn syscall which is on sigframe
*** sigaction
**** SA_ONESHOT - called once then restored to default handler
**** rt_sigaction
**** do_sigaction
***** task_struct->sighand_struct->struct k_sigaction action[_NSIG]
***** signum to k_sigaction(associated user sigaction)
** fs
*** open syscall
**** get_unused_fd_flags
***** fdtable->open_fds(bitmap)->find_next_fd->find_next_zero_bit->__ffs->__buildin_ctzl(count trailing zero bits long)
**** do_filp_open
***** path_openat
****** do_last
******* lookup_fast(search in dcache)
******* vfs_open
******** open(inode, file) - assigned as inode->i_fop->open
*** read syscall
**** find_get_page
**** page_cache_sync_readahead
**** copy_page_to_iter
*** struct file - opened file
**** file_operations -> filesystem operations, eg ext4_file_operations
*** strcut mount - mount info
*** struct files_struct - opened files list
*** struct path - mount info and dentry
**** code
     #+begin_src c
     struct path {
       struct vfsmount *mnt;
       struct dentry *dentry;
     } __randomize_layout;
     #+end_src

*** struct dentry - dir name, file name, associated inode
*** struct address_space - contents of mappable or cacheable objects, eg, mmap, memory cache for open read syscall
*** register_filesystem
**** just add a `struct file_system_type` to global `struct file_system_type *file_systems` linked list
*** super_block do management of inode(create, write, destroy)
**** struct super_block->s_op(struct super_operations)
*** block device
**** bdev pseudo filesystem
**** bdev_map
***** dev_t to gendisk
** mm
*** physical mm
**** memory model
***** FLATMEM
***** DISCONTIGMEM
***** SPARSEMEM
**** NUMA node (non uniform memory access) -> typedef struct pglist_data {} pg_data_t
***** store in global node_data[]
***** struct zone node_zones[MAX_NR_ZONES]
***** struct page *node_mem_map - mem_map manage all page frames of the NUMA node
***** zone
****** lowmem - ZONE_DMA, ZONE_NORMAL
****** highmem - ZONE_HIGHMEM
****** struct free_area	free_area[MAX_ORDER]; #define MAX_ORDER 11
******* struct list_head free_list[MIGRATE_TYPES]
**** page
***** struct list_head lru - node in active_list or inactive_list
***** struct address_space mapping - page cache
***** pgoff_t index - offset within mapping
**** buddy -> struct free_area
**** slab -> struct page
*** virtual mem
**** user space
***** TEXT
***** DATA
***** BSS
***** HEAP
***** MMAP
***** STACK
***** args and envs
**** vm_area_struct - fragment of user space virtual space
***** vm_file - mapped file
***** vm_pgoff - offset within vm_file
***** vm_flags - protect flags, eg. VM_READ, VM_WRITE, VM_EXEC
**** kernel space
***** direct mapping region(896MB)
***** vmalloc
***** PKmap
***** FixedMap
**** kmalloc
***** mapping page to DIRECT region
**** vmalloc
***** mapping page to VMALLOC region
***** vm_struct - similar to vm_area_struct
***** vmap
****** mapping page to VMALLOC region
**** alloc_pages
***** alloc phisical pages
***** from freelist of NUMA zone
***** code
#+begin_src c
/*
 * This is the 'heart' of the zoned buddy allocator.
 */
struct page *
__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
							nodemask_t *nodemask)
{
	struct page *page;
	unsigned int alloc_flags = ALLOC_WMARK_LOW;
	gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */
	struct alloc_context ac = { };

	/*
	 * There are several places where we assume that the order value is sane
	 * so bail out early if the request is out of bound.
	 */
	if (unlikely(order >= MAX_ORDER)) {
		WARN_ON_ONCE(!(gfp_mask & __GFP_NOWARN));
		return NULL;
	}

	gfp_mask &= gfp_allowed_mask;
	alloc_mask = gfp_mask;
	if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, &ac, &alloc_mask, &alloc_flags))
		return NULL;

	finalise_ac(gfp_mask, &ac);

	/*
	 * Forbid the first pass from falling back to types that fragment
	 * memory until all local zones are considered.
	 */
	alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp_mask);

	/* First allocation attempt */
	page = get_page_from_freelist(alloc_mask, order, alloc_flags, &ac);
	if (likely(page))
		goto out;

	/*
	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
	 * resp. GFP_NOIO which has to be inherited for all allocation requests
	 * from a particular context which has been marked by
	 * memalloc_no{fs,io}_{save,restore}.
	 */
	alloc_mask = current_gfp_context(gfp_mask);
	ac.spread_dirty_pages = false;

	/*
	 * Restore the original nodemask if it was potentially replaced with
	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
	 */
	if (unlikely(ac.nodemask != nodemask))
		ac.nodemask = nodemask;

	page = __alloc_pages_slowpath(alloc_mask, order, &ac);

out:
	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
	    unlikely(__memcg_kmem_charge(page, gfp_mask, order) != 0)) {
		__free_pages(page, order);
		page = NULL;
	}

	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);

	return page;
}

/*
 * get_page_from_freelist goes through the zonelist trying to allocate
 * a page.
 */
static struct page *
get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
						const struct alloc_context *ac)
{
	struct zoneref *z;
	struct zone *zone;
	struct pglist_data *last_pgdat_dirty_limit = NULL;
	bool no_fallback;

retry:
	/*
	 * Scan zonelist, looking for a zone with enough free.
	 * See also __cpuset_node_allowed() comment in kernel/cpuset.c.
	 */
	no_fallback = alloc_flags & ALLOC_NOFRAGMENT;
	z = ac->preferred_zoneref;
	for_next_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
								ac->nodemask) {
		struct page *page;
		unsigned long mark;

		if (cpusets_enabled() &&
			(alloc_flags & ALLOC_CPUSET) &&
			!__cpuset_zone_allowed(zone, gfp_mask))
				continue;
		/*
		 * When allocating a page cache page for writing, we
		 * want to get it from a node that is within its dirty
		 * limit, such that no single node holds more than its
		 * proportional share of globally allowed dirty pages.
		 * The dirty limits take into account the node's
		 * lowmem reserves and high watermark so that kswapd
		 * should be able to balance it without having to
		 * write pages from its LRU list.
		 *
		 * XXX: For now, allow allocations to potentially
		 * exceed the per-node dirty limit in the slowpath
		 * (spread_dirty_pages unset) before going into reclaim,
		 * which is important when on a NUMA setup the allowed
		 * nodes are together not big enough to reach the
		 * global limit.  The proper fix for these situations
		 * will require awareness of nodes in the
		 * dirty-throttling and the flusher threads.
		 */
		if (ac->spread_dirty_pages) {
			if (last_pgdat_dirty_limit == zone->zone_pgdat)
				continue;

			if (!node_dirty_ok(zone->zone_pgdat)) {
				last_pgdat_dirty_limit = zone->zone_pgdat;
				continue;
			}
		}

		if (no_fallback && nr_online_nodes > 1 &&
		    zone != ac->preferred_zoneref->zone) {
			int local_nid;

			/*
			 * If moving to a remote node, retry but allow
			 * fragmenting fallbacks. Locality is more important
			 * than fragmentation avoidance.
			 */
			local_nid = zone_to_nid(ac->preferred_zoneref->zone);
			if (zone_to_nid(zone) != local_nid) {
				alloc_flags &= ~ALLOC_NOFRAGMENT;
				goto retry;
			}
		}

		mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);
		if (!zone_watermark_fast(zone, order, mark,
				       ac_classzone_idx(ac), alloc_flags)) {
			int ret;

#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
			/*
			 * Watermark failed for this zone, but see if we can
			 * grow this zone if it contains deferred pages.
			 */
			if (static_branch_unlikely(&deferred_pages)) {
				if (_deferred_grow_zone(zone, order))
					goto try_this_zone;
			}
#endif
			/* Checked here to keep the fast path fast */
			BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
			if (alloc_flags & ALLOC_NO_WATERMARKS)
				goto try_this_zone;

			if (node_reclaim_mode == 0 ||
			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
				continue;

			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
			switch (ret) {
			case NODE_RECLAIM_NOSCAN:
				/* did not scan */
				continue;
			case NODE_RECLAIM_FULL:
				/* scanned but unreclaimable */
				continue;
			default:
				/* did we reclaim enough */
				if (zone_watermark_ok(zone, order, mark,
						ac_classzone_idx(ac), alloc_flags))
					goto try_this_zone;

				continue;
			}
		}

try_this_zone:
		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
				gfp_mask, alloc_flags, ac->migratetype);
		if (page) {
			prep_new_page(page, order, gfp_mask, alloc_flags);

			/*
			 * If this is a high-order atomic allocation then check
			 * if the pageblock should be reserved for the future
			 */
			if (unlikely(order && (alloc_flags & ALLOC_HARDER)))
				reserve_highatomic_pageblock(page, zone, order);

			return page;
		} else {
#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
			/* Try again if zone has deferred pages */
			if (static_branch_unlikely(&deferred_pages)) {
				if (_deferred_grow_zone(zone, order))
					goto try_this_zone;
			}
#endif
		}
	}

	/*
	 * It's possible on a UMA machine to get through all zones that are
	 * fragmented. If avoiding fragmentation, reset and try again.
	 */
	if (no_fallback) {
		alloc_flags &= ~ALLOC_NOFRAGMENT;
		goto retry;
	}

	return NULL;
}

/*
 * Go through the free lists for the given migratetype and remove
 * the smallest available page from the freelists
 */
static __always_inline
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
						int migratetype)
{
	unsigned int current_order;
	struct free_area *area;
	struct page *page;

	/* Find a page of the appropriate size in the preferred list */
	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
		area = &(zone->free_area[current_order]);
		page = get_page_from_free_area(area, migratetype);
		if (!page)
			continue;
		del_page_from_free_area(page, area);
		expand(zone, page, order, current_order, area, migratetype);
		set_pcppage_migratetype(page, migratetype);
		return page;
	}

	return NULL;
}

static inline void expand(struct zone *zone, struct page *page,
	int low, int high, struct free_area *area,
	int migratetype)
{
	unsigned long size = 1 << high;

	while (high > low) {
		area--;
		high--;
		size >>= 1;
		VM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);

		/*
		 * Mark as guard pages (or page), that will allow to
		 * merge back to allocator when buddy will be freed.
		 * Corresponding page table entries will not be touched,
		 * pages will stay not present in virtual address space
		 */
		if (set_page_guard(zone, &page[size], high, migratetype))
			continue;

		add_to_free_area(&page[size], area, migratetype);
		set_page_order(&page[size], high);
	}
}
#+end_src
**** kmap
***** map single page
***** might sleep
***** mapping page to PKMap region
***** PKMap region page management
****** hash table
****** code
#+begin_src c
#define LAST_PKMAP 1024
/*
 * Describes one page->virtual association
 */
struct page_address_map {
	struct page *page;
	void *virtual;
	struct list_head list;
};

static struct page_address_map page_address_maps[LAST_PKMAP];

/*
 * Hash table bucket
 */
static struct page_address_slot {
	struct list_head lh;			/* List of page_address_maps */
	spinlock_t lock;			/* Protect this bucket's list */
} ____cacheline_aligned_in_smp page_address_htable[1<<PA_HASH_ORDER];

static struct page_address_slot *page_slot(const struct page *page)
{
	return &page_address_htable[hash_ptr(page, PA_HASH_ORDER)];
}
#+end_src
**** kmap_atomic
***** not sleep
***** map single page
***** mapping page to FixedMap region
**** page level
***** PGD (page global directory)
***** PUD (page upper directory)
***** PMD (page middle directory)
***** PTE (page table entry)
*** page management
**** active list --> inactive list
**** mm/workingset.c
** cdev
*** insmod
**** module_init
***** install pair of dev_t and `struct cdev` into cdev_map
***** struct cdev - {dev_t, struct file_operations}
*** mknod /dev/{name}
**** do_mknodat
***** inode->i_fop = &def_chr_fops
***** def_chr_fops = {.open = chrdev_open}
*** open
**** call chrdev_open
***** replace_fops(filp, fops) - f
** radix tree
*** a compressed trie
*** associated array
*** echo node represents a fragment of key,and points to child fragments
*** linux 4bit per node, nginx 2bit per node
*** shift is high to low from top to bottom
*** graph
               +--------+
               |  0100  |
               +---+----+
                   |
                   |
     +----------------------------+
     |             |              |
     |             |              |
     v             v              v
+----+---+    +----+---+     +----+---+
|  0000  |    |  0001  |     |  0010  |  ....
+--------+    +--------+     +--------+



                 ...



 +--------+       +--------+     +--------+
 |  0010  |       |  0011  |     |  0100  |
 +---+----+       +----+---+     +----+---+
     |                 |              |
     |                 |              |
     v                 v              v
 +---+----+       +----+---+     +----+---+
 |  value |       |  value |     |  value |
 +--------+       +--------+     +--------+
** ipc
*** cmd
**** ipcmk
**** ipcs - inspect
**** ipcrm
*** pipe
**** mkfifo
**** pipe2 syscall
***** do_pipe2 -> __do_pipe_flags -> copy_to_user -> fd_install
***** __do_pipe_flags -> create_pipe_files -> get_pipe_inode -> new_inode_pseudo(pipefs)
****** pipe_mnt->mnt_sb -> alloc_inode
***** struct file -> private_data = pipe_inode_info(has a member: struct pipe_buffer *bufs)
***** f_op = pipefifo_fops
*** msgqueue
**** msgget
**** msgsnd
**** msgrcv
*** shmem
**** shmget
***** create a file on shmem filesystem
**** shmat - attach
**** shmdt - detach
**** shmctl
*** semaphore
**** semget
**** semctl
**** semop
** socket
*** sock_init (in socket.c)
**** initialize the network sysctl infrastructure
**** initialize skbuff SLAB cache
**** initialize `struct socket_alloc` SLAB cache
***** create a socket and an inode at the same time when call `sock_alloc`
***** `struct socket_alloc` - where a new inode and socket object bound together
**** register `sock_fs_type` file system
***** sockfs -> `struct super_operations` sockfs_ops {.alloc_inode = sock_alloc_inode}
*** inet_init (in af_inet.c)
**** proto_register - tcp_prot, udp_prot, raw_prot, ping_prot
***** create SLAB cache for stack sock
***** `struct proto` is "socket layer -> transport layer interface"
**** sock_register(&inet_family_ops)
***** add inet family to `net_families` array
**** inet_add_protocol - icmp_protocol, udp_protocol, tcp_protocol, igmp_protocol
***** handler for receiving data `struct net_protocol tcp_protocol{.handler = tcp_v4_rcv}`
**** insert all the elements in inetsw_array[] into the linked list inetsw
***** inet_register_protosw
*** socket(int domain, int type, int protocol)
**** args
***** domain - AF_UNIX, AF_INET
***** type - SOCK_STREAM, SOCK_DGRAM, SOCK_RAW
***** protocol - IPPROTO_TCP, IPPROTO_UDP, IPPROTO_ICMP(ping), 0(inferred) or specified
**** syscall
***** common calling path: fd -> file -> struct socket(inet_stream_ops) -> struct sock(tcp_prot)
***** synopsis: __sys_socket => sock_create -> sock_map_fd
***** sock_create(aka. __sock_create) => sock_alloc -> pf->create()
****** sock_alloc
******* create `struct socket_alloc` sock where a new inode and socket object bound together
******* new_pseudo_inode(sock_mnt->mnt_sb) -> alloc_inode -> sock_alloc_inode
******* sock_mnt->mnt_sb->s_op = &sockfs_ops {.alloc_inode = sock_alloc_inode} in sockfs mount(socket.c)
****** pf = rcu_dereference(net_families[family])
******* chosing `inet_family_ops` according to `family`
******* initialize the sock above, eg. `inet_create` for AF_INET address family, `unix_create` for AF_UNIX address family
******* initialized in af_inet.c inet_init => proto_register(&tcp_prot, 1) -> sock_register(&inet_family_ops){.create = inet_create} -> inet_register_protosw
******* af_unix.c for AF_UNIX unix domain socket
****** inet_create => lookup inet_protosw in inetsw array with type and protocol -> sk_alloc
******* chosing `struct inet_protosw` according to `type` and `protocol`
******* lookup -> answer{.prot = &tcp_prot, .ops = &inet_stream_ops} for instance
******* sock->ops = &inet_stream_ops
******* sk_alloc
******** sock_common <- sock <- inet_sock <- inet_connection_sock <- tcp_sock (sock class hierachy)
******** sock_common <- sock <- inet_sock <- udp_sock
******** sock_common <- sock <- inet_sock (for ping sock)
******** sock_common <- sock <- raw_sock
****** sock_map_fd => get_unused_fd_flags -> sock_alloc_file
******* sock_alloc_file
******** file = alloc_file_pseudo(..., &socket_file_ops)
******** sock->file = file;
******** file->private_data = sock;
*** bind(int fd, struct sockaddr* addr, int addrlen)
**** sockfd_lookup_light => sock_from_file(via file->private_data)
**** move_addr_to_kernel
**** sock->ops->bind() -> inet_bind(of inet_stream_ops) -> __inet_bind
***** sk->sk_prot->get_port()(of tcp_prot) -> inet_csk_get_port()('csk' stands for connection socket)
*** listen(int fd, int backlog)
**** sockfd_lookup_light => sock_from_file
**** sock->ops->listen() -> inet_listen(of inet_stream_ops) -> inet_csk_listen_start()
**** inet_csk_listen_start
***** init accept queue - reqsk_queue_alloc(&icsk->icsk_accept_queue);
***** set listen state - inet_sk_state_store(sk, TCP_LISTEN);
*** accept(int fd, struct sockaddr *addr, socklen_t *addrlen)
**** sockfd_lookup_light => sock_from_file
**** move_addr_to_kernel()
**** get_unused_fd_flags(flags) sock_alloc_file()
**** sock->ops->accept() => inet_accept(of inet_stream_ops) -> inet_csk_accept(of tcp_prot)
**** inet_csk_accept
***** `struct sk_buff_head` sk_receive_queue and `struct sk_buff_head` sk_write_queue
***** inet_csk_wait_for_connect
***** req = reqsk_queue_remove(queue, sk); newsk = req->sk;
*** connect(int fd, const struct sockaddr *addr, socklen_t addrlen)
**** sockfd_lookup_light => sock_from_file
**** sock->ops->connect() => inet_stream_connect(of inet_stream_ops) -> __inet_stream_connect -> sk->sk_prot->connect -> tcp_v4_connect(of tcp_prot)
**** tcp_v4_connect
***** ip_route_connect
***** tcp_set_state(sk, TCP_SYN_SENT)
***** tcp_connect(sk) - Build a SYN and send it off.
*** write
**** fop = socket_file_ops
**** sock_write_iter -> sock_sendmsg -> sock_sendmsg_nosec
**** sock->ops->sendmsg => inet_sendmsg -> sk->sk_prot->sendmsg => tcp_sendmsg -> tcp_sendmsg_locked
**** tcp_sendmsg_locked
***** tcp_write_queue_tail - get sk_buff
***** tcp_send_mss
****** APP - data, TCP - segment, IP - packet, MAC - frame
****** MTU (Maximum Transmission Unit) 1500 byte
****** TSO (TCP Segment Offload) segmented by network interface
****** MSS (Maximum segmentation size)
****** cwnd - congestion window
***** sk_stream_alloc_skb
***** skb_add_data_nocache or skb_copy_to_page_nocache
***** __tcp_push_pending_frames or tcp_push_one -> tcp_write_xmit
***** ip_queue_xmit -> ip_route_output_ports
***** ip_finish_output -> __neigh_lookup_noref -> neigh_probe
***** dev_queue_xmit
***** raise_softirq_irqoff(NET_TX_SOFTIRQ) -> net_tx_action -> ixgb_xmit_frame
*** read
**** incoming data -> ring buffer
**** ixgb_intr -> __raise_softirq_irqoff(NET_RX_SOFTIRQ)
**** net_rx_action -> napi_poll -> ixgb_clean_rx_irq
**** net_receive_skb -> ip_rcv
**** iptables
**** ip_local_deliver
**** tcp_v4_rcv -> tcp_v4_do_rcv
***** tcp_rcv_established
***** tcp_rcv_state_process
**** tcp_recvmsg
***** sk_receive_queue
***** prequeue
***** backlog
*** Qdisc -> driver queue (ring buffer)
*** iptables
**** NAT
**** netfilters
*** [[https://zhensheng.im/2017/08/11/%e7%bf%bb%e8%af%91linux%e7%bd%91%e7%bb%9c%e6%a0%88%e4%b9%8b%e9%98%9f%e5%88%97.meow.html][ref]]
*** net device init
**** net_dev_init
** net protocol
*** layer2
**** MAC
***** header format
****** dest_mac(6byte)
****** src_mac(6byte)
****** type(2byte) IP or ARP
*** layer3
**** IP
***** header format
****** (4bit)version ipv4 or ipv6
****** (4bit)ip header length
******* unit 4 byte
******* [20, 60]
****** (8bit)TOS
******* type of service
****** (16bit)total length
****** (16bit)identification - packet id
****** (3bit)flag
****** (13bit)fragment offsets
****** (8bit)time to live
****** (8bit)upper-layer protocol - UDP、TCP、ICMP、IGMP、IGP etc
****** (16bit)header checksum
****** (32bit)src ip address
****** (32bit)dest ip address
****** options
****** data
*** layer4
**** UDP
***** header format
****** (16bit)src port
****** (16bit)dst port
****** (16bit)length
****** (16bit)checksum
****** data
**** TCP
***** header format
****** (16bit)src port
****** (16bit)dst port
****** (32bit)sequence num
****** (32bit)ack sequence
****** (4bit)header length
****** (6bit)flags
****** (16bit)window size
****** (16bit)checksum
****** (16bit)urgency pointer
****** options
****** data
***** hand-shake
****** [[./_imgs/tcp_3way_handshake.jpg]]
****** [[./_imgs/tcp_4way_close.jpg]]
****** active-close end -> FIN_WAIT_1, FIN_WAIT_2, TIME_WAIT
****** passive-close end -> CLOSE_WAIT, LAST_ACK
** container
*** namespace
**** uts pid ipc net user mnt(mnemonic: pumnic)
**** nsenter
**** unshare
*** cgroup
**** /sys/fs/cgroup
***** cpu,cpuacct
***** cpuset
***** memory
***** blkio
***** devices
***** net_cls,net_prio
* http protocal
** request
*** METHOD URL HTTP_VERSION
*** Attribute-Name: value
*** empty line
*** body
** response
*** HTTP_VERSION STATUS_CODE RESPONSE-DESC
*** Attribute-Name: value
*** empty line
*** body
** cookie
*** ref [[https://tools.ietf.org/html/rfc6265][RFC6265]]
*** syntax
**** set-cookie-header = Set-Cookie: name=value(; cookie-av)*
**** cookie-av = expires-av / max-age-av / domain-av / path-av / secure-av / httponly-av / extension-av
*** Max-Age prior to Expires
*** Domain
**** The user agent will reject cookies unless the Domain attribute specifies a scope for the cookie that would include the origin server.
**** For example, the user agent will accept a cookie with a Domain attribute of "example.com" or of "foo.example.com" from foo.example.com,
**** but the user agent will not accept a cookie with a Domain attribute of "bar.example.com" or of "baz.foo.example.com".
*** ETag and if-none-match
* java lambda vs inner class
** inner class create an instance if the inner class
** lambda invoke the lambda function through INVOKEDYNAMIC instruction and MethodLookup MethodHandle
* data structure
** HashMap
*** key is nullable
*** hash()
    #+begin_src java
    static final int hash(Object key) {
      int h;
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >> 16);
    }
    #+end_src
*** index of key
    #+begin_src java
    int i = (table.lenght - 1) & hash(key);
    #+end_src
* get generic type in java at runtime
** anonymous inner class
*** code
    #+begin_src kotlin
      open class GenericsToken<T> {
	  var type: Type = Any::class.java

	  init {
	      val superClass = this.javaClass.genericSuperclass
	      type = (superClass as ParameterizedType).actualTypeArguments[0]
	  }
      }
      val gt = object : GenericsToken<List<String>>(){}
      println(gt.type)
    #+end_src
** kotlin reified inline function
*** code
    #+begin_src kotlin
       inline fun <reified T: Any> Gson.fromJson(json: String) : T {
	   return Gson().fromJson<T>(json, T::class.java)
       }
    #+end_src
* java generics
** generic types in Java are invariant
   #+begin_src java
   // Java
   List<String> strs = new ArrayList<String>();
   List<Object> objs = strs; // !!! The cause of the upcoming problem sits here. Java prohibits this!
   objs.add(1); // Here we put an Integer into a list of Strings
   String s = strs.get(0); // !!! ClassCastException: Cannot cast Integer to String
   #+end_src
** covariant
*** Collection<String> is a subtype of Collection<? extends Object>.
*** In "clever words", the wildcard with an extends-bound (upper bound) makes the type covariant.
** contravariance
*** in Java we have List<? super String> a supertype of List<Object>
** mnemonic
*** PECS stands for Producer-Extends, Consumer-Super.
** kotlin Declaration-site variance
*** in
    #+begin_src kotlin
    interface Source<out T> {
      fun nextT(): T
    }

    fun demo(strs: Source<String>) {
      val objects: Source<Any> = strs // This is OK, since T is an out-parameter
      // ...
    }
    #+end_src
*** out
    #+begin_src kotlin
    interface Comparable<in T> {
      operator fun compareTo(other: T): Int
    }

    fun demo(x: Comparable<Number>) {
      x.compareTo(1.0) // 1.0 has type Double, which is a subtype of Number
      // Thus, we can assign x to a variable of type Comparable<Double>
      val y: Comparable<Double> = x // OK!
    }
    #+end_src
** notes
*** if you use a producer-object, say, List<? extends Foo>, you are not allowed to call add() or set() on this object, but this does not mean that this object is immutable: for example, nothing prevents you from calling clear() to remove all items from the list, since clear() does not take any parameters at all. The only thing guaranteed by wildcards (or other types of variance) is type safety. Immutability is a completely different story.

* PriorityQueue
** offer
   #+begin_src java
    public boolean offer(E e) {
        if (e == null)
            throw new NullPointerException();
        modCount++;
        int i = size;
        if (i >= queue.length)
            grow(i + 1);
        size = i + 1;
        if (i == 0)
            queue[0] = e;
        else
            siftUp(i, e);
        return true;
    }

    private void siftUp(int k, E x) {
        if (comparator != null)
            siftUpUsingComparator(k, x);
        else
            siftUpComparable(k, x);
    }

    @SuppressWarnings("unchecked")
    private void siftUpComparable(int k, E x) {
        Comparable<? super E> key = (Comparable<? super E>) x;
        while (k > 0) {
            int parent = (k - 1) >>> 1;
            Object e = queue[parent];
            if (key.compareTo((E) e) >= 0)
                break;
            queue[k] = e;
            k = parent;
        }
        queue[k] = key;
    }
   #+end_src
** poll
   #+begin_src java
    public E poll() {
        if (size == 0)
            return null;
        int s = --size;
        modCount++;
        E result = (E) queue[0];
        E x = (E) queue[s];
        queue[s] = null;
        if (s != 0)
            siftDown(0, x);
        return result;
    }

    private void siftDown(int k, E x) {
        if (comparator != null)
            siftDownUsingComparator(k, x);
        else
            siftDownComparable(k, x);
    }

    @SuppressWarnings("unchecked")
    private void siftDownComparable(int k, E x) {
        Comparable<? super E> key = (Comparable<? super E>)x;
        int half = size >>> 1;        // loop while a non-leaf
        while (k < half) {
            int child = (k << 1) + 1; // assume left child is least
            Object c = queue[child];
            int right = child + 1;
            if (right < size &&
                ((Comparable<? super E>) c).compareTo((E) queue[right]) > 0)
                c = queue[child = right];
            if (key.compareTo((E) c) <= 0)
                break;
            queue[k] = c;
            k = child;
        }
        queue[k] = key;
    }
   #+end_src
* epoll
** epoll_create
*** create `struct eventpoll` ep
#+begin_src c
error = ep_alloc(&ep);
#+end_src
*** alloc a pseudo file, and attach ep to file->private_data
#+begin_src c
fd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));
if (fd < 0) {
	error = fd;
	goto out_free_ep;
}
file = anon_inode_getfile("[eventpoll]", &eventpoll_fops, ep,
			 O_RDWR | (flags & O_CLOEXEC));
ep->file = file;
fd_install(fd, file);
#+end_src
** epoll_ctl
*** create a `struct epitem`
*** add a `struct eppoll_entry` to socket wait_queue and register a `ep_poll_callback` which is triggered when file receive events(POLLIN, POLLOUT)
*** link `struct epitem` to rbtree ep->rbr
#+begin_src c
epi = ep_find(ep, tf.file, fd);

error = -EINVAL;
switch (op) {
case EPOLL_CTL_ADD:
	if (!epi) {
		epds.events |= EPOLLERR | EPOLLHUP;
		error = ep_insert(ep, &epds, tf.file, fd, full_check);
	} else
		error = -EEXIST;
	if (full_check)
		clear_tfile_check_list();
	break;
case EPOLL_CTL_DEL:
	if (epi)
		error = ep_remove(ep, epi);
	else
		error = -ENOENT;
	break;
case EPOLL_CTL_MOD:
	if (epi) {
		if (!(epi->event.events & EPOLLEXCLUSIVE)) {
			epds.events |= EPOLLERR | EPOLLHUP;
			error = ep_modify(ep, epi, &epds);
		}
	} else
		error = -ENOENT;
	break;
}
static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,
		     struct file *tfile, int fd, int full_check)
{
	int error, pwake = 0;
	__poll_t revents;
	long user_watches;
	struct epitem *epi;
	struct ep_pqueue epq;

	lockdep_assert_irqs_enabled();

	user_watches = atomic_long_read(&ep->user->epoll_watches);
	if (unlikely(user_watches >= max_user_watches))
		return -ENOSPC;
	if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
		return -ENOMEM;

	/* Item initialization follow here ... */
	INIT_LIST_HEAD(&epi->rdllink);
	INIT_LIST_HEAD(&epi->fllink);
	INIT_LIST_HEAD(&epi->pwqlist);
	epi->ep = ep;
	ep_set_ffd(&epi->ffd, tfile, fd);
	epi->event = *event;
	epi->nwait = 0;
	epi->next = EP_UNACTIVE_PTR;
	if (epi->event.events & EPOLLWAKEUP) {
		error = ep_create_wakeup_source(epi);
		if (error)
			goto error_create_wakeup_source;
	} else {
		RCU_INIT_POINTER(epi->ws, NULL);
	}

	/* Initialize the poll table using the queue callback */
	epq.epi = epi;
	init_poll_funcptr(&epq.pt, ep_ptable_queue_proc);

	/*
	 * Attach the item to the poll hooks and get current event bits.
	 * We can safely use the file* here because its usage count has
	 * been increased by the caller of this function. Note that after
	 * this operation completes, the poll callback can start hitting
	 * the new item.
	 */
	revents = ep_item_poll(epi, &epq.pt, 1);

	/*
	 * We have to check if something went wrong during the poll wait queue
	 * install process. Namely an allocation for a wait queue failed due
	 * high memory pressure.
	 */
	error = -ENOMEM;
	if (epi->nwait < 0)
		goto error_unregister;

	/* Add the current item to the list of active epoll hook for this file */
	spin_lock(&tfile->f_lock);
	list_add_tail_rcu(&epi->fllink, &tfile->f_ep_links);
	spin_unlock(&tfile->f_lock);

	/*
	 * Add the current item to the RB tree. All RB tree operations are
	 * protected by "mtx", and ep_insert() is called with "mtx" held.
	 */
	ep_rbtree_insert(ep, epi);

	/* now check if we've created too many backpaths */
	error = -EINVAL;
	if (full_check && reverse_path_check())
		goto error_remove_epi;

	/* We have to drop the new item inside our item list to keep track of it */
	write_lock_irq(&ep->lock);

	/* record NAPI ID of new item if present */
	ep_set_busy_poll_napi_id(epi);

	/* If the file is already "ready" we drop it inside the ready list */
	if (revents && !ep_is_linked(epi)) {
		list_add_tail(&epi->rdllink, &ep->rdllist);
		ep_pm_stay_awake(epi);

		/* Notify waiting tasks that events are available */
		if (waitqueue_active(&ep->wq))
			wake_up(&ep->wq);
		if (waitqueue_active(&ep->poll_wait))
			pwake++;
	}

	write_unlock_irq(&ep->lock);

	atomic_long_inc(&ep->user->epoll_watches);

	/* We have to call this outside the lock */
	if (pwake)
		ep_poll_safewake(&ep->poll_wait);

	return 0;

error_remove_epi:
	spin_lock(&tfile->f_lock);
	list_del_rcu(&epi->fllink);
	spin_unlock(&tfile->f_lock);

	rb_erase_cached(&epi->rbn, &ep->rbr);

error_unregister:
	ep_unregister_pollwait(ep, epi);

	/*
	 * We need to do this because an event could have been arrived on some
	 * allocated wait queue. Note that we don't care about the ep->ovflist
	 * list, since that is used/cleaned only inside a section bound by "mtx".
	 * And ep_insert() is called with "mtx" held.
	 */
	write_lock_irq(&ep->lock);
	if (ep_is_linked(epi))
		list_del_init(&epi->rdllink);
	write_unlock_irq(&ep->lock);

	wakeup_source_unregister(ep_wakeup_source(epi));

error_create_wakeup_source:
	kmem_cache_free(epi_cache, epi);

	return error;
}
#+end_src
** epoll_wait
*** check read list `ep->rdllist` or overflow list `ep->ovflist` has events available
*** try to sleep if not have events available
*** try to transfer events to user space
*** new events were added to overflow list `ep->ovflist` when transferring data to user space, which were added to ready list later on
*** LT events were just added to ready list agian for next trigger
#+begin_src c
static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,
		   int maxevents, long timeout)
{
	int res = 0, eavail, timed_out = 0;
	u64 slack = 0;
	bool waiter = false;
	wait_queue_entry_t wait;
	ktime_t expires, *to = NULL;

	lockdep_assert_irqs_enabled();

	if (timeout > 0) {
		struct timespec64 end_time = ep_set_mstimeout(timeout);

		slack = select_estimate_accuracy(&end_time);
		to = &expires;
		*to = timespec64_to_ktime(end_time);
	} else if (timeout == 0) {
		/*
		 * Avoid the unnecessary trip to the wait queue loop, if the
		 * caller specified a non blocking operation. We still need
		 * lock because we could race and not see an epi being added
		 * to the ready list while in irq callback. Thus incorrectly
		 * returning 0 back to userspace.
		 */
		timed_out = 1;

		write_lock_irq(&ep->lock);
		eavail = ep_events_available(ep);
		write_unlock_irq(&ep->lock);

		goto send_events;
	}

fetch_events:

	if (!ep_events_available(ep))
		ep_busy_loop(ep, timed_out);

	eavail = ep_events_available(ep);
	if (eavail)
		goto send_events;

	/*
	 * Busy poll timed out.  Drop NAPI ID for now, we can add
	 * it back in when we have moved a socket with a valid NAPI
	 * ID onto the ready list.
	 */
	ep_reset_busy_poll_napi_id(ep);

	/*
	 * We don't have any available event to return to the caller.  We need
	 * to sleep here, and we will be woken by ep_poll_callback() when events
	 * become available.
	 */
	if (!waiter) {
		waiter = true;
		init_waitqueue_entry(&wait, current);

		spin_lock_irq(&ep->wq.lock);
		__add_wait_queue_exclusive(&ep->wq, &wait);
		spin_unlock_irq(&ep->wq.lock);
	}

	for (;;) {
		/*
		 * We don't want to sleep if the ep_poll_callback() sends us
		 * a wakeup in between. That's why we set the task state
		 * to TASK_INTERRUPTIBLE before doing the checks.
		 */
		set_current_state(TASK_INTERRUPTIBLE);
		/*
		 * Always short-circuit for fatal signals to allow
		 * threads to make a timely exit without the chance of
		 * finding more events available and fetching
		 * repeatedly.
		 */
		if (fatal_signal_pending(current)) {
			res = -EINTR;
			break;
		}

		eavail = ep_events_available(ep);
		if (eavail)
			break;
		if (signal_pending(current)) {
			res = -EINTR;
			break;
		}

		if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) {
			timed_out = 1;
			break;
		}
	}

	__set_current_state(TASK_RUNNING);

send_events:
	/*
	 * Try to transfer events to user space. In case we get 0 events and
	 * there's still timeout left over, we go trying again in search of
	 * more luck.
	 */
	if (!res && eavail &&
	    !(res = ep_send_events(ep, events, maxevents)) && !timed_out)
		goto fetch_events;

	if (waiter) {
		spin_lock_irq(&ep->wq.lock);
		__remove_wait_queue(&ep->wq, &wait);
		spin_unlock_irq(&ep->wq.lock);
	}

	return res;
}
#+end_src
** multiplex io should use nonblocking fd
** ET and LT
| level-triggered   | edge-triggered                                                                    |
|-------------------+-----------------------------------------------------------------------------------|
| notify each state | only notify when changes occurred on monitored file descriptor                    |
|                   | epoll_wait will wait indefinitely if not consumed the whole buffer data last time |
* catlan number
** h(n) = h(0)*h(n-1) + h(1)*h(n-2) + ... + h(n-1)*h(0), h(0)=h(1)=1
** h(n) = C(2n,n)/(n+1)
** h(n) = C(2n,n) - C(2n,n-1)
** parens inserting
** binary search tree enuming
** solution: reflection
*** S - push, X - pop
*** find first position of Count(X) - Count(S) = 1 in sequence (S of n, X of n, C(2n,n) )
*** S: s t (sum n)
*** X: s+1 t-1 (sum n)
*** reflect left partition-> S: s+1+t=n+1 X: s+t-1=n-1
*** find first position of Count(S) - Count(X) = 1 in sequence(S of n+1, X of n-1, C(2n,n-1))
*** S: s t (sum n+1)
*** X: s-1 t-1(sum n-1)
*** reflect left partition -> S: s-1+t=n X: s+t-1=n
* ViewGroup
** isMotionEventSplittingEnabled
*** when true(default), ACTION_POINTER_DOWN will be split
*** case two buttons in a viewgroup, press down two buttons at the same time, two click will invoked
**** viewgroup: ACTION_DOWN -> ACTION_POINTER_DOWN -> ACTION_POINTER_UP -> ACTION_UP
**** both buttons: ACTION_DOWN -> ACTION_UP
* socket
** unix domain socket vs IP socket
*** unix domain socket
**** fast (no need protocol layer encode-decode)
***** send data -> kernel buffer -> TCP -> IP -> LINK -> ... LINK -> IP -> TCP -> kernel buffer -> recv data
**** bind file path while ip socket bind ip address and port
** int socket(int domain, int type, int protocol);
*** domain - address family
**** AF_UNIX
**** AF_INET
*** type
**** SOCK_STREAM
**** SOCK_DGRAM
*** protocol
** int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
*** bind a name to socket
** int listen(int sockfd, int backlog);
*** The backlog argument defines the maximum length to which the queue of pending connections for sockfd may grow.  If a connection request  arrives  when  the queue  is  full, the client may receive an error with an indication of ECONNREFUSED or, if the underlying protocol supports retransmission, the request may be ignored so that a later reattempt at connection succeeds.
** int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
*** may block for incomming connection in runtime
*** The  accept()  system call is used with connection-based socket types (SOCK_STREAM, SOCK_SEQPACKET).  It extracts the first connection request on the queue of pending connections for the listening socket, sockfd, creates a new connected socket, and returns a new file descriptor referring to that  socket. The newly created socket is not in the listening state.  The original socket sockfd is unaffected by this call.
* kotlin coroutine
** kotlin compiler
*** suspend lambda compiled into a class extends `SuspendLambda` class and implementing `Function2` interface
*** `SuspendLambda` super class is BaseContinuationImpl which is `Continuation`
*** user defined code is compiled into `invokeSuspend` member function with a Result object parameter
*** member integer variable `label` is a black magic which acks like `swich-case` thus the `Continuation` can resume with result returned from the suspend point
** kotlinx.coroutines SDK
*** Dispatchers
**** Default
***** DefaultScheduler
**** Main
***** UI operations
**** Unconfined
**** IO
***** share threads with `Default` dispatcher
*** CoroutineContext
**** an indexed set of `Element` instances
**** plus operator
***** contextA + contextB
****** Returns a context containing elements from contextA and elements from contextB. The elements from contextA with the same key as in contextB is dropped.
*** common behavior
**** launch
***** append `Dispatchers.Default` when `CoroutineInterceptor` is empty
***** Create `StandaloneCoroutine`
***** start coroutine
****** create
****** intercepted
******* interceptContinuation
******** Returns a continuation that wraps the provided continuation, thus intercepting all resumptions.
******** create a `DispatchedContinuation` which dispatches a task wrapping continuation resumption to dispatches worker thread.
****** resumeWith
******* can be dispatched to Dispatchers's thread
***** return coroutine
**** delay
***** create `CancellableContinuationImpl`
***** `DefaultDelay` (alias of `DefaultExecutor` which extends `EventLoopImplBase`) `scheduleResumeAfterDelay` continuation created above
****** create `DelayResumeTask` and `schedule`
****** if `schedule` successfully then call `unpark` of `EventLoopImplPlatform` within which a worker thread was created and unparked
***** when delay time is reached the `continuation` is resumed
*** exceptions
**** -> parent coroutine -> context[CoroutineExceptionHandler] -> Global CoroutineExceptionHandler with ServiceLoader -> Thread.uncaughtExceptionHandler
*** a continuation parameter(as callback, which is outer scope coroutine) added to suspending function by compiler
**** receive with `suspendCoroutineUninterceptedOrReturn` function
* android view drawing process
** Measuring layouting drawing
** custom view
** MeasureSpec
** ?attr
** overloads
*** onSizeChanged
*** onFinishInflate
** dispatchTouchEvent
*** TouchTarget aka mFirstTouchTarget
**** TouchTarget is a view with pointers touched to it
**** TouchTargets connected as a linked list
*** when (event is not canceled or intercepted) and (action is ACTION_DOWN or ACTION_POINTER_DOWN), find if a child would to consume the event, if so create a touch target as the head of mFirstTouchTarget(linked list)
**** special case
***** when an event with ACTION_POINTER_DOWN can't find a consumer, it will add the associated pointer id to the TouchTarget  created by the early ACTION_DOWN event
*** other events with pointer_up action_up will delivered to mFirstTouchTarget
*** if no child would to consume the event, the viewgroup it self will try it
** click listener
*** performed when view is clickable or longclickable
*** performed between action_down and action_up event(not the others, eg. action_pointer_down or action_pointer_up)
** window
** nested scrolling
* RxJava
** CacheObservable and NetworkObservable case
*** requires
**** CacheObservable return first then NetworkObservable, take Cache then Network
**** NetworkObservable return first then CacheObservable, take Network only
*** operators
**** publish(optional) + merge + takeUtil
**** publish
***** Returns an observable that emits the results of invoking a specified selector on items emitted by a ConnectableObservable that shares a single subscription to the underlying sequence.
***** upstream -> SourceObserver -> PublishSubject -> applying selector function -> new Observable -> downstream
**** merge
***** delegated to flatMap with non-action(Functions.identity()) mapper function
**** flatMap
***** Returns an Observable that emits items based on applying a function that you supply to each item emitted by the source ObservableSource, where that function returns an ObservableSource, and then merging those resulting ObservableSources and emitting the results of this merger, while limiting the maximum number of concurrent subscriptions to these ObservableSources.
***** upstream -> MergeObserver -> mapper function -> new Observable -> InnerObserver -> dispatch to downstream or cache data in queue of InnerObserver
**** takeUtil
***** Returns an observable that emits the items emitted by the source observable until a second observable emits an item.
**** code
     #+begin_src java
     networkObservable.publish(new Function<Observable<LocalLifeModel>, ObservableSource<LocalLifeModel>>() {
       @Override
       public ObservableSource<LocalLifeModel> apply(@io.reactivex.annotations.NonNull Observable<LocalLifeModel> localLifeModelObservable) throws Exception {
         return Observable.merge(localLifeModelObservable, cacheObservable.takeUntil(localLifeModelObservable));
       }
   })
     #+end_src
** PublishSubject
*** A Subject that emits(multicasts) items to currently subscribed Observers and terminal events to current or late Observers
*** as a Observable and a Observer, thus Connectable Observable
** Disposable$dispose
*** cancelling the connection with parent(upstream) which is passed to `onSubscribe` when calling `subscribe`
** comprehensible diagram
*** Observable S -> operator A -> operator B -> operator C -> Observable D
**** conceptually, D = ObservableWrapperC( ObservableWrapperB( ObservableWrapperA( S ) ) ), which is wrapped in operator function
*** Observable D subscribes Observer O
**** conceptually, FinalObserver FO = ObserverWrapperA( ObserverWrapperB( ObserverWrapperC( O ) ) ), which is wrapped in `subscribeActual` function generally
**** thus the data flow is: Observable S -> ObserverWrapperA -> ObserverWrapperB -> ObserverWrapperC -> Observer O
** fusion
*** BasicFuseableObserver
*** wip(work-in-process) counter/indicator
** error handling
*** after disposing
**** exception -> RxJavaPlugins.errorHandler -> Thread.getUncaughtExceptionHandler()(defaultUncaughtExceptionHandler -> thread group)
*** before disposing
**** onErrorResumeNext
* android input system
** diagram
*** user touch -> driver -> /dev/input/event* (EventHub via epoll(polling new events) and inotify(adding or removing devices)) -> InputReader enqueue EventEntry to mInboundQueue(property of InputDispatcher) -> InputDispatcher dequeue DispatchEntry to outboundQueue(property of Connection), InputManagerService created a pair socket(socketpair) with UI thread of currently focused window, thus InputDispatcher send input data to UI thread via Connection -> Looper.loop -> MessageQueue.next -> MessageQueue.nativePollOnce -> InputEventReceiver.dispatchInputEvent -> call ViewRootImpl$WindowInputEventReceiver.onInputEvent -> mView.dispatchPointerEvent -> windowCallback.dispatchTouchEvent(Activity or Dialog) -> PhoneWindow.dispatchTouchEvent -> view.dispatchTouchEvent
*** ref[https://wanandroid.com/wenda/show/12119]
*** InputManagerService
**** InputChannel client end: ViewRootImpl.mInputChannel, epoll-controlled by Looper of UI thread
**** InputChannel server end: WindowState.mInputChannel, epoll-controlled by Looper of InputDispatcher
**** diagram
***** ViewRootImpl$setView -> mWindowSession.addToDisplay(...mInputChannel) --binder-> Session$addToDisplay(outInputChannel) -> WMS$addWindow() -> win = new WindowState() -> win.openInputChannel() -> InputChannel$openInputChannelPair() --jni-> socketpair() -> mClientChannel.transferTo(outInputChannel) -> mService.mInputManager.registerInputChannel() -> mInputManager->getDispatcher()->registerInputChannel -> new Connection and mLooper.addFd(fd, 0, ALOOPER_EVENT_INPUT, handleReceiveCallback, this)
***** ViewRootImpl$setView -> new WindowInputReceiver(mInputChannel, Looper.myLooper()) -> InputEventReceiver.nativeInit --jni-> receiver = new NativeInputReceiver -> receiver.initialize() -> setFdEvents(ALOOPER_EVENT_INPUT) -> mMessageQueue->getLooper()->addFd(fd, 0, events, this/*callback*/, NULL)
* android smart pointer
** RefBase
*** wrapping the strong reference count and weak reference count, for auto deallocate objects which usually extends from `RefBase` class, when decStrong() encounts 0
** sp template class
*** wrapping RefBase objects, implementing object copy constructing, copy assignment, move constructing, move assignment and deallocating, increasing or decreasing strong references of delegated RefBase objects
* android Watchdog
** monitoring deadlock of targets
** started as a thread in SystemServer
* binder
** binder driver
*** binder_init
**** create debugfs
**** register misc device
***** create `struct binder_device` binder_device which contains `struct binder_context` context
***** associate 'struct file_operations' binder_fops
***** code
 #+begin_src c
 static int __init init_binder_device(const char *name)
 {
	 int ret;
	 struct binder_device *binder_device;

	 binder_device = kzalloc(sizeof(*binder_device), GFP_KERNEL);
	 if (!binder_device)
		 return -ENOMEM;

	 binder_device->miscdev.fops = &binder_fops;
	 binder_device->miscdev.minor = MISC_DYNAMIC_MINOR;
	 binder_device->miscdev.name = name;

	 binder_device->context.binder_context_mgr_uid = INVALID_UID;
	 binder_device->context.name = name;
	 mutex_init(&binder_device->context.context_mgr_node_lock);

	 ret = misc_register(&binder_device->miscdev);
	 if (ret < 0) {
		 kfree(binder_device);
		 return ret;
	 }

	 hlist_add_head(&binder_device->hlist, &binder_devices);

	 return ret;
 }
 #+end_src

**** create binderfs
*** binder_open
**** create `struct binder_proc` proc which is linked to global `binder_procs` list
**** filep.private_data = proc, store binder_proc data to file private_data for later use
**** binder_proc represents the process which opened binder driver
*** binder_mmap
**** init `struct binder_alloc` alloc of `struct binder_proc` proc
***** represents user space vma
**** create a `struct binder_buffer` buffer
***** represents ipc data area, physical page is allocated in `bind_transaction`
***** linked in rbtree `alloc->free_buffers.rb_node`
***** alloc `struct binder_lru_page` pages(control data of `struct page`)
*** binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
**** protocol
***** ioctl data -> BINDER_WRITE_READ + `struct binder_write_read` data
****** binder_write_read is control data to write buffer, write buffer size and consumed size, read buffer, read buffer size and consumed size
***** binder_write_read data -> BC_TRANSACTION + `struct binder_transaction_data` data
***** binder_transaction_data -> parcel data(buffer + offsets)
****** buffer - normal data contains `struct flat_binder_object`
****** offsets - offsets from buffer to flat_binder_object structs
**** handle BINDER_XXX cmd
**** BINDER_WRITE_READ
***** binder_thread_write
****** BC_TRANSACTION -> binder_transaction
****** binder_transaction
******* find target_node and target_proc with `tr->target.handle`
******* alloc `struct binder_transaction` t and `struct binder_work` tcomplete
******* `binder_alloc_new_buf`
******** alloc a new binder_buffer in binder_alloc of target_proc for ipc data
******** alloc physical `struct page` pages with `alloc_page` and `vm_insert_page`, so copy once
******* `binder_alloc_copy_user_to_buffer`
******** copy user binder_transaction_data(buffer + offsets) to new allocated binder_buffer with `kmap`
******* read `struct binder_object` from binder_buffer
******** `binder_alloc_copy_from_buffer`
********* read offsets data(pointed to buffer data) from binder_buffer of target_proc
******** `binder_get_object`
********* read `struct binder_object` of the offsets above
******* translate and replace original binder object
******** `binder_translate_binder`
********* `binder_get_node` - create `struct binder_node` node for strong binder if not exists
********* BINDER_TYPE_BINDER -> BINDER_TYPE_HANDLE
********** `struct binder_node` transfer to `struct binder_ref` when binder server object transferred to another process
********* `fp->handle = rdata.desc;`
********** handle of `struct binder_ref` is the index when added to target_proc's `struct rb_root` refs_by_node rbtree
********* code
#+begin_src c
static int binder_translate_binder(struct flat_binder_object *fp,
				   struct binder_transaction *t,
				   struct binder_thread *thread)
{
	struct binder_node *node;
	struct binder_proc *proc = thread->proc;
	struct binder_proc *target_proc = t->to_proc;
	struct binder_ref_data rdata;
	int ret = 0;

	node = binder_get_node(proc, fp->binder);
	if (!node) {
		node = binder_new_node(proc, fp);
		if (!node)
			return -ENOMEM;
	}
	if (fp->cookie != node->cookie) {
		binder_user_error("%d:%d sending u%016llx node %d, cookie mismatch %016llx != %016llx\n",
				  proc->pid, thread->pid, (u64)fp->binder,
				  node->debug_id, (u64)fp->cookie,
				  (u64)node->cookie);
		ret = -EINVAL;
		goto done;
	}
	if (security_binder_transfer_binder(proc->tsk, target_proc->tsk)) {
		ret = -EPERM;
		goto done;
	}

	ret = binder_inc_ref_for_node(target_proc, node,
			fp->hdr.type == BINDER_TYPE_BINDER,
			&thread->todo, &rdata);
	if (ret)
		goto done;

	if (fp->hdr.type == BINDER_TYPE_BINDER)
		fp->hdr.type = BINDER_TYPE_HANDLE;
	else
		fp->hdr.type = BINDER_TYPE_WEAK_HANDLE;
	fp->binder = 0;
	fp->handle = rdata.desc;
	fp->cookie = 0;

	trace_binder_transaction_node_to_ref(t, node, &rdata);
	binder_debug(BINDER_DEBUG_TRANSACTION,
		     "        node %d u%016llx -> ref %d desc %d\n",
		     node->debug_id, (u64)node->ptr,
		     rdata.debug_id, rdata.desc);
done:
	binder_put_node(node);
	return ret;
}
#+end_src

******** `binder_alloc_copy_to_buffer`
********* repalce the original binder object with the translated one
******* translate fd 
******** `binder_translate_fd`
********* `list_add_tail(&fixup->fixup_entry, &t->fd_fixups);`
********** add fixup data to binder_transaction which is handled in target_proc process
******* enqueue t to target_thread todo list, tcomplete to current thread todo list
***** binder_thread_read
****** `binder_wait_for_work`
****** `w = binder_dequeue_work_head_ilocked(list);`
****** `binder_apply_fd_fixups`
******* `get_unused_fd_flags`
******* `fd_install`
******* `binder_alloc_copy_to_buffer`
****** copy_to_user
* todo
** binder in c implementation on android device
** open device /dev/rtc
* android
** init process
*** add init.${ro.hardware}.rc to init.rc at compile time
**** code
#+begin_src rc
...
import /init.environ.rc
import /init.usb.rc
import /init.${ro.hardware}.rc
import /vendor/etc/init/hw/init.${ro.hardware}.rc // added here
import /init.usb.configfs.rc
import /init.${ro.zygote}.rc
..

// init.${ro.hardware}.rc
...
mount_all /xx/fstab // will load rc files in /{system, vendor, odm]/etc/init directory
...
#+end_src
- 1) /system/etc/init/ is for core system items such as SurfaceFlinger, MediaService, and logcatd.
- 2) /vendor/etc/init/ is for SoC vendor items such as actions or daemons needed for core SoC functionality.
- 3) /odm/etc/init/ is for device manufacturer items such as actions or daemons needed for motion sensor or other peripheral functionality.
*** parse init.rc file
*** start servicemanager
*** start zygote service /system/bin/app_process (app_main.cpp)
**** start zygote (com.android.internal.os.ZygoteInit)
***** start system_server
****** start services
******* AMS
******* PMS
***** runSelectLoop waiting for process-forking connection
** service manager
*** `bs = binder_open(128*1024);`
*** `binder_become_context_manager(bs)`
**** set `struct binder_context` context->binder_context_mgr_node = new_node;
*** `binder_loop(bs, svcmgr_handler);`
**** set binder_thread->looper = `BC_ENTER_LOOPER`
**** `ioctl(bs->fd, BINDER_WRITE_READ, &bwr);` -> binder_thread_read wait for transaction
**** binder_parse
***** `svcmgr_handler`
****** `SVC_MGR_ADD_SERVICE` -> call `do_add_service`
** ViewRootImpl
*** requestLayout
**** checkThread
***** check mThread == Thread.currentThread()
***** mThread is the one ViewRootImpl object created in
****** mThread = Thread.currentThread() in ViewRootImpl constructor
**** scheduleTravversals
***** mHandler.getLooper().getQueue().postSyncBarrier()
****** Message is synchronous by default
******* code
#+begin_src java
    public boolean isAsynchronous() {
        return (flags & FLAG_ASYNCHRONOUS) != 0;
    }

    public static Message obtain() {
        synchronized (sPoolSync) {
            if (sPool != null) {
                Message m = sPool;
                sPool = m.next;
                m.next = null;
                m.flags = 0; // clear in-use flag, no FLAG_ASYNCHRONOUS flag
                sPoolSize--;
                return m;
            }
        }
        return new Message();
    }

    private int postSyncBarrier(long when) {
        // Enqueue a new sync barrier token.
        // We don't need to wake the queue because the purpose of a barrier is to stall it.
        synchronized (this) {
            final int token = mNextBarrierToken++;
            final Message msg = Message.obtain();
            msg.markInUse();
            msg.when = when;
            msg.arg1 = token;

            Message prev = null;
            Message p = mMessages;
            if (when != 0) {
                while (p != null && p.when <= when) {
                    prev = p;
                    p = p.next;
                }
            }
            if (prev != null) { // invariant: p == prev.next
                msg.next = p;
                prev.next = msg;
            } else {
                msg.next = p;
                mMessages = msg;
            }
            return token;
        }
    }
#+end_src
****** Message#next()
******* when head msg is sync barrier, find first asynchronous msg
******** code
#+begin_src java
    Message next() {
        // Return here if the message loop has already quit and been disposed.
        // This can happen if the application tries to restart a looper after quit
        // which is not supported.
        final long ptr = mPtr;
        if (ptr == 0) {
            return null;
        }

        int pendingIdleHandlerCount = -1; // -1 only during first iteration
        int nextPollTimeoutMillis = 0;
        for (;;) {
            if (nextPollTimeoutMillis != 0) {
                Binder.flushPendingCommands();
            }

            nativePollOnce(ptr, nextPollTimeoutMillis);

            synchronized (this) {
                // Try to retrieve the next message.  Return if found.
                final long now = SystemClock.uptimeMillis();
                Message prevMsg = null;
                Message msg = mMessages;
                if (msg != null && msg.target == null) {
                    // Stalled by a barrier.  Find the next asynchronous message in the queue.
                    do {
                        prevMsg = msg;
                        msg = msg.next;
                    } while (msg != null && !msg.isAsynchronous());
                }
                if (msg != null) {
                    if (now < msg.when) {
                        // Next message is not ready.  Set a timeout to wake up when it is ready.
                        nextPollTimeoutMillis = (int) Math.min(msg.when - now, Integer.MAX_VALUE);
                    } else {
                        // Got a message.
                        mBlocked = false;
                        if (prevMsg != null) {
                            prevMsg.next = msg.next;
                        } else {
                            mMessages = msg.next;
                        }
                        msg.next = null;
                        if (DEBUG) Log.v(TAG, "Returning message: " + msg);
                        msg.markInUse();
                        return msg;
                    }
                } else {
                    // No more messages.
                    nextPollTimeoutMillis = -1;
                }

                // Process the quit message now that all pending messages have been handled.
                if (mQuitting) {
                    dispose();
                    return null;
                }

                // If first time idle, then get the number of idlers to run.
                // Idle handles only run if the queue is empty or if the first message
                // in the queue (possibly a barrier) is due to be handled in the future.
                if (pendingIdleHandlerCount < 0
                        && (mMessages == null || now < mMessages.when)) {
                    pendingIdleHandlerCount = mIdleHandlers.size();
                }
                if (pendingIdleHandlerCount <= 0) {
                    // No idle handlers to run.  Loop and wait some more.
                    mBlocked = true;
                    continue;
                }

                if (mPendingIdleHandlers == null) {
                    mPendingIdleHandlers = new IdleHandler[Math.max(pendingIdleHandlerCount, 4)];
                }
                mPendingIdleHandlers = mIdleHandlers.toArray(mPendingIdleHandlers);
            }

            // Run the idle handlers.
            // We only ever reach this code block during the first iteration.
            for (int i = 0; i < pendingIdleHandlerCount; i++) {
                final IdleHandler idler = mPendingIdleHandlers[i];
                mPendingIdleHandlers[i] = null; // release the reference to the handler

                boolean keep = false;
                try {
                    keep = idler.queueIdle();
                } catch (Throwable t) {
                    Log.wtf(TAG, "IdleHandler threw exception", t);
                }

                if (!keep) {
                    synchronized (this) {
                        mIdleHandlers.remove(idler);
                    }
                }
            }

            // Reset the idle handler count to 0 so we do not run them again.
            pendingIdleHandlerCount = 0;

            // While calling an idle handler, a new message could have been delivered
            // so go back and look again for a pending message without waiting.
            nextPollTimeoutMillis = 0;
        }
    }
#+end_src

* flutter
** Widget
*** everything is widget
**** font, style, layout, view
**** declarative
*** State lifecycle
**** constructor
**** initState
**** didChangeDependencies
**** build
**** deactivate
**** dispose
*** App lifecycle
**** WidgetsBindingObserver
**** didChangeAppLifecycleState
**** WidgetsBinding.instance
***** addPostFrameCallback
***** addPersistentFrameCallback
*** Text
**** Text.rich
**** inner implementation
***** RichText
*** Image
**** Image.asset('images/image.png')
**** Image.file(new File('images/image.png'))
**** Image.Network('https://xxxx.png')
**** FadeInImage.assetNetwork
**** _ImageState -> ImageProvider -> ImageCache(memory cache) -> ImageStream
**** CachedNetworkImage
***** cached image file in local storage
*** Button
**** FlatButton
**** RaisedButton
**** FloatingActionButton
**** inner implementation
***** RawMaterialButton
****** extends `StatefulWidget`
****** build() -> `Semantics`
****** extends `SingleChildRenderObjectWidget` which extends `RenderObjectWidget` which extends `Widget`
*** ListView
**** StatelessWidget
**** ListView.builder
**** ListVIew.separate
**** CustomScrollview - behavior
***** SliverAppBar
***** SliverList
**** ScrollController
***** scroll offsets listener
**** NotificationListener<ScrollNotification>
***** scroll event listener(including offset data `scrollNotificatio.metrics.pixels`)
*** Container Widget
**** single child container
***** Container
****** combines a number of simpler widgets together into a convenient package
****** padding - Padding
****** decoration - DecoratedBox
****** In fact, the majority of widgets in Flutter are simply combinations of other simpler widgets.
****** Composition, rather than inheritance, is the primary mechanism for building up widgets.
***** Center
***** Align
***** Padding
**** multiple children container
***** Row - LinearLayout
***** Column
***** Expanded - LinearLayout weight
***** alignment
****** MainAxisAlignment
****** CrossAxisAlignment
******* start, center, end
***** size
****** MainAxisSize.max - match_parent
****** MainAxisSize.min - wrap_content
**** stack container
***** Stack - FrameLayout
***** Positioned - absolute position only in Stack
*** CustomPaint
**** CustomPainter
***** actual custom painting logic
** Element
*** An instantiation of a Widget at a particular position in the tree
*** aids for widgets diff
** RenderObject
*** provide actual rending of the applicatoin
** Theme
*** Theme.of()
**** Global Theme
**** Local widget theme
*** defaultTargetPlatform
** pubspec.yaml
*** dependencies
**** assets
***** single file path declaration
***** directory path declaration
***** 1.0x, 2.0x, 3.0x
**** fonts
*** .packages
**** mappings of lib and local cache path
*** pubspec.lock
**** lock file of specific lib dependencies
** touch event
*** event
**** PointerDownEvent
**** PointerMoveEvent
**** PointerUpEvent
**** PointerCancelEvent
*** Listener
**** raw pointer event listener
**** onPointerDown
**** onPointerMove
**** onPointerUp
*** GestureDetector
**** detecting gesture for widgets
***** onTap
***** onDoubleTap
***** onLongPress
***** onPanUpdate
***** etc
**** returns `RawGestureDetector` in build method
*** RawGestureDetector
**** A widget that detects gestures described by the given gesture factories.
**** GestureRecgnizerFactoryWithHandlers
*** GestureBinding
**** receiving pointer events from window and dispatching to widgets
**** PointerRouter
***** A routing table for `PointerEvent` events
***** dispatch `PointerEvent` to `PointerRoute` entry in two routing table
****** Map<int, Map<PointerRoute, Matrix4>> _routeMap
****** Map<PointerRoute, Matrix4> _globalRoutes
**** GestureArenaManager
***** mapping of pointer id to arena
***** Map<int, _GestureArena> _arenas
***** The first member to accept or the last member to not reject wins
** asynchronous
*** Future
**** then
**** catchError
*** Stream
**** listen
** passing data
*** InheritedWidget
**** parent -> child
**** context.inheritFromWidgetOfExactType
**** read only
*** Notification
**** child -> parent
**** dispatch
**** NotificationListener
*** EventBus
**** custom event bean
**** fire(event)
**** initState
***** subscription = eventBus.on<CustomEvent>().listen((){doSomething();})
**** dispose
***** subscription.cancel()
** navigation
*** basic routes
**** Navigator.push(context, MaterialPageRoute())
*** named routes
**** register routes in MaterialApp
***** onUnknownRoute
**** Navigator.pushNamed(context, 'routeName', arguments: 'name')
*** passing arguments
**** RouteSettings
**** receive args
***** ModalRoute.of(context).settings.arguments
**** return back args
***** Navigator.push().then((value)=>doSomething())
** animation
*** Animation
**** returns animation values
**** separates the state of animation from the rendering logic
*** AnimationController
**** role
***** Play an animation forward or in reverse, or stop an animation.
***** Set the animation to a specific value.
***** Define the upperBound and lowerBound values of an animation.
***** Create a fling animation effect using a physics simulation.
**** good practice: created in State.initState and disposed in State.dispose
*** TickerProvider
**** provide frame callback
*** AnimatedWidget
*** AnimatedBuilder
*** Hero
**** Shared element transition
**** provide same tag
** event loop
*** microtask queue
**** scheduleMicroTask
*** event queue
*** Future(FutureOr<T> computation())
**** put a callback to event queue
*** Future.wait
**** merge Future
*** then
**** `then` share the same event cycle with `Future`
**** register callbacks to be called when this future completes
**** If this future is already completed, the callback will not be called immediately, but will be scheduled in a later microtask
*** practise
#+begin_src dart
Future(() => print('f1'));//声明一个匿名Future
Future fx = Future(() =>  null);//声明Future fx，其执行体为null

//声明一个匿名Future，并注册了两个then。在第一个then回调里启动了一个微任务
Future(() => print('f2')).then((_) {
  print('f3');
  scheduleMicrotask(() => print('f4'));
}).then((_) => print('f5'));

//声明了一个匿名Future，并注册了两个then。第一个then是一个Future
Future(() => print('f6'))
  .then((_) => Future(() => print('f7')))
  .then((_) => print('f8'));

//声明了一个匿名Future
Future(() => print('f9'));

//往执行体为null的fx注册了了一个then
fx.then((_) => print('f10'));

//启动一个微任务
scheduleMicrotask(() => print('f11'));
print('f12');

//result
f12
f11
f1
f10
f2
f3
f5
f4
f6
f9
f7
f8


Future(() => print('f1'))
  .then((_) async => await Future(() => print('f2')))
  .then((_) => print('f3'));
Future(() => print('f4'));

// result
f1
f4
f2
f3

Future<String> fetchContent() =>
    Future<String>.delayed(Duration(seconds: 3), () => "Hello")
        .then((x) => '$x Jocoo');

func() async => print(await fetchContent());

main(List<String> args) async {
  print('func before');
  func();
  print('func after');
}

// result
func before
func after
Hello Jocoo

#+end_src
** asynchronous
*** Future
**** representation of pending work to be done
**** 2 states
***** uncompleted
***** completed
**** When a Future created, a work to be done queues up
**** when a future's operation finishes, the future completes with a value or with an error
*** async-await
**** If the async function has a return type, then update the type to be Future<T>, where T is the type of value the function retures
**** If the function doesn't explicitly return a value, then the return type is Future<void>
**** 
** Isolate
*** an isolated Dart execution context
**** All dart code runs in an isolate, and code can access classes and values only from the same isolate.
**** Diffrent isolates can communicate by sending values through ports.[ReceivePort], [SendPort]
*** controlPort
*** capability
**** pause
**** terminate
*** compute
** dio
*** options
**** headers
**** cookies
*** interceptor
**** InterceptorsWrapper
** Json
*** `json.decode` to map
*** factory to bean using map
** platform specific code
*** platform channel
**** MethodChannel
***** logical identity is a string prefixed with domain in a constructor
***** invokeMethod(String method)
***** asynchronous
***** native code runs in main thread which means callback should run in main thread if method handler is asynchronous
***** StandardMessageCodec
****** serialization
******* int (dart) -> Int (kotlin)
*** platform view
**** PlatformViewFactory
**** AndroidView
**** UIKitView
** dart:ffi
*** foreign function interface
*** native interface
*** language bindings
** localization
*** LocalizationDelegate
*** Flutter Intl plugin
** screen compatible
*** OrientationBuilder
**** SystemChrome.setPreferredOrientations
*** MediaQueryData
** compile mode
*** build type
**** debug
***** JIT
***** assert enabled
**** release
***** AOT
***** assert disabled
**** profile
***** nearly same as release
*** configurations
**** InheritedWidget
**** --target/-t option
***** flutter build apk -t lib/main_dev.dart
**** variant entry point
** crash reporting
*** app exceptions
**** synchronous try-catch
**** asynchronous Future.catchError
**** runZoned((){}, onError:(dynamic e, StackTrace stack){})
*** frame exceptions
**** custom error page
***** ErrorWidget.builder
**** FlutterError.onError
** Travis CI
*** .travis.yml
**** install
**** script
**** deploy
* algorithms
** kmp string pattern-matching
*** main idea
**** S - source string
**** M - pattern string
**** T - partial match table(aka. failure function)
***** the goal of the table is to allow the algorithm not to match any character of S more than once
***** T[i+1] = T[i] + 1 or 0
**** complexity
***** O(len(S) + len(M))
*** [[https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm][ref]]
** dynamic programming
*** child problem with overlapping
**** divide-and-conquer without overlapping
*** top to bottom
**** memorizing result of child problem
**** handling child-problem in depth-first search
*** bottom up
**** handling child problem in reverse topological sort
*** Longest common subsequence
**** minimum edit distance
**** diff program in unix
**** k-candidates optimization
***** https://www.cs.dartmouth.edu/~doug/diff.pdf
